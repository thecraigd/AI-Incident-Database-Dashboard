Namespace,Incident ID,Published,Known AI Goal,Known AI Goal Snippets,Known AI Goal Classification Discussion,Potential AI Goal,Potential AI Goal Snippets,Potential AI Goal Classification Discussion,Known AI Technology,Known AI Technology Snippets,Known AI Technology Classification Discussion,Potential AI Technology,Potential AI Technology Snippets,Potential AI Technology Classification Discussion,Known AI Technical Failure,Known AI Technical Failure Snippets,Known AI Technical Failure Classification Discussion,Potential AI Technical Failure,Potential AI Technical Failure Snippets,Potential AI Technical Failure Classification Discussion
GMF,1,True,"Content Recommendation, Content Search, Hate Speech Detection, NSFW Content Detection","Snippet Text: An off-brand Paw Patrol video called ""Babies Pretend to Die Suicide"" features several disturbing scenarios.
The YouTube Kids app filters out most - but not all - of the disturbing videos.

Before any video appears in the YouTube Kids app, it's filtered by algorithms that are supposed to identify appropriate children's content
YouTube also has a team of human moderators that review any videos flagged in the main YouTube app by volunteer Contributors (users who flag inappropriate content) or by systems that identify recognizable children's characters in the questionable video.
Many of those views came from YouTube's ""up next"" and ""recommended"" video section that appears while watching any video. YouTube's algorithms attempt to find videos that you may want to watch based on the video you chose to watch first
If you don't pick another video to watch after the current video ends, the ""up next"" video will automatically play.

Related Classifications: Content Recommendation, Content Search
",,,,,"Content-based Filtering, Collaborative Filtering","Snippet Text: If you searched for ""moon landing"" on YouTube Kids, three videos appeared that claim that the moon landing was hoaxed. All three videos have since been hidden by YouTube after we informed it of the issue.
Related Classifications: Content-based Filtering
",,"Classification, Ensemble Aggregation, Distributional Learning","Snippet Text: 
Part of YouTube’s plan is to increase human moderation and tweak its algorithm, “training machine-learning technology across other challenging content areas, including child safety and hate speech.” YouTube will also cut down on channels that receive monetization and advertisements attached to these videos. Since YouTube Kids also includes ads — many of which, Golin says, aren’t child appropriate — this will affect channels and videos on the platform.
Related Classifications: Ensemble Aggregation
","Classification: Appropriateness could arise by appropriateness classifiers

Ensemble Aggregation: In cases where  ""child-appropriateness"" measure arises from multiple marginal detectors of-related subclasses (e.g. violent, adult, political themes)","Tuning Issues, Lack of Adversarial Robustness, Adversarial Data","Snippet Text: 
Part of YouTube’s plan is to increase human moderation and tweak its algorithm, “training machine-learning technology across other challenging content areas, including child safety and hate speech.” YouTube will also cut down on channels that receive monetization and advertisements attached to these videos. Since YouTube Kids also includes ads — many of which, Golin says, aren’t child appropriate — this will affect channels and videos on the platform.
Related Classifications: Tuning Issues
, Snippet Text: ""Recommendations are designed to optimize watch time, there is no reason that it shows content that is actually good for kids. 
Related Classifications: Tuning Issues
, Snippet Text: On YouTube today, children are being exploited for money. YouTubers with channels specifically marketed toward children are cranking out videos to provide kids with loads of content to consume, as each video around 16 minutes long. (Which is the sweet spot for maximum ad revenue.) Frankly, YouTubers are practically begging their viewers to “smash” that like button and comment on their videos.
I spent a weekend babysitting my brother’s children and they spent most of that time watching channels like Chad Wild Clay. He would ask a question like, “Who is going to win this game?” ask kids to comment their predictions in the comments and then proceed to play the game, giving the kids the answer in the same video. He’d do that same thing several times throughout the video.

What’s the point of the interactive bits if they can just skip ahead and get their answers without commenting at all? It’s simple: the more engagement the video gets, the more likely it is to be picked up by YouTube’s recommendation algorithm, thus bringing in more traffic and more money.
Related Classifications: Tuning Issues
Snippet Discussion: Recommendation training / video ranking is utilizing likes and engagement too much.
, Snippet Text: Conspiracy videos also appear when children search for popular conspiracy theories. Searches for ""chemtrails,"" ""flat earth,"" and ""nibiru"" are all allowed in the app. However, it's (hopefully) unlikely that children are regularly watching these videos unless they appear as suggestions on more popular content in the app.

The conspiracy videos didn't just appear in searches or suggested videos, either. After watching several conspiracy videos, the top recommended video on the home page of YouTube Kids was a conspiracy theory about aliens on the moon:
Related Classifications: Tuning Issues
, Snippet Text: The first line of defense for YouTube Kids are algorithmic filters. After that, there is a team of humans that review videos which have been flagged. If a video with recognizable children’s characters gets flagged in YouTube’s main app, which is much larger than the Kids app, it will be sent to the policy review team. YouTube says it has thousands of people working around the clock in different time zones to review flagged content. If the review finds the video is in violation of the new policy, it will be age restrictied, automatically blocking it from showing up in the Kids app. YouTube says it typically takes at least a few days for content to make its way from YouTube proper to YouTube Kids, and the hope is that within that window, users will flag anything potentially disturbing to children. YouTube also has a team of volunteer moderators, which it calls Contributors, looking for inappropriate content. YouTube says it will start training its review team on the new policy and it should be live within a few weeks. 
It normally takes five days for supposedly child-friendly content like cartoons to get from YouTube to YouTube Kids. Within that window it is hoped users and a specially-trained team will flag disturbing content.



Related Classifications: Lack of Adversarial Robustness, Adversarial Data
","Tuning Issues: Default classification, in cases where the poor consideration of child -appropriateness context information does not fall under current subclasses of this classification.","Concept Drift, Generalization Failure, Misconfigured Aggregation, Distributional Bias, Misaligned Objective","Snippet Text: “From a child standpoint, the problem is not fixable,” Golin said. “The YouTube model has created something, which is so vast, but there are 400 hours of content are uploaded every minute. It’s simply too big. 
Related Classifications: Concept Drift, Generalization Failure
, Snippet Text: ""There are vast, vast numbers of these videos,"" Bridle said. ""Channel after channel after channel of similar content, churned out at the rate of hundreds of new videos every week. Industrialized nightmare production.""
Related Classifications: Generalization Failure
, Snippet Text: 
Part of YouTube’s plan is to increase human moderation and tweak its algorithm, “training machine-learning technology across other challenging content areas, including child safety and hate speech.” YouTube will also cut down on channels that receive monetization and advertisements attached to these videos. Since YouTube Kids also includes ads — many of which, Golin says, aren’t child appropriate — this will affect channels and videos on the platform.
Related Classifications: Misconfigured Aggregation
, Snippet Text: ""Recommendations are designed to optimize watch time, there is no reason that it shows content that is actually good for kids. 
Related Classifications: Misconfigured Aggregation, Misaligned Objective
, Snippet Text: On YouTube today, children are being exploited for money. YouTubers with channels specifically marketed toward children are cranking out videos to provide kids with loads of content to consume, as each video around 16 minutes long. (Which is the sweet spot for maximum ad revenue.) Frankly, YouTubers are practically begging their viewers to “smash” that like button and comment on their videos.
I spent a weekend babysitting my brother’s children and they spent most of that time watching channels like Chad Wild Clay. He would ask a question like, “Who is going to win this game?” ask kids to comment their predictions in the comments and then proceed to play the game, giving the kids the answer in the same video. He’d do that same thing several times throughout the video.

What’s the point of the interactive bits if they can just skip ahead and get their answers without commenting at all? It’s simple: the more engagement the video gets, the more likely it is to be picked up by YouTube’s recommendation algorithm, thus bringing in more traffic and more money.
Related Classifications: Misconfigured Aggregation, Misaligned Objective
Snippet Discussion: Recommendation training / video ranking is utilizing likes and engagement too much.
","Concept Drift: Concept drift in cases where appropriateness evolves and changes with the passage of time and is culturally determined -- e.g. akin to old messed up disney cartoons.

Generalization Failure: Based on huge dataset size.

Misconfigured Aggregation: In cases where  ""child-appropriateness"" measure arises from multiple marginal detectors of-related subclasses (e.g. violent, adult, political themes)

Misaligned Objective: Recommendation training is using child-appropriateness in its objective in a diminished capacity (as a component with a small contribution), or not at all (completely relying in post-hoc reviewing and filtering by other systems and humans)."
GMF,10,True,"Market Forecasting, Scheduling",,,,,,,,,"Regression, Diverse Data","Snippet Text: “You’re waiting on your job to control your life,” she said, with the scheduling software used by her employer dictating everything from “how much sleep Gavin will get to what groceries I’ll be able to buy this month.”
Related Classifications: Regression
, Snippet Text: In a follow-up piece, the author, Jodi Kantor, points directly to Kronos' scheduling software as the root of the problem.
Related Classifications: Diverse Data
","Regression: Potentially the number of required staff and/or some numeric measure of traffic is estimated for each branch?

Diverse Data: Diverse types of input data could involve high-level attributes / categories / conceptual information on employees.",Underspecification,"Snippet Text: “You’re waiting on your job to control your life,” she said, with the scheduling software used by her employer dictating everything from “how much sleep Gavin will get to what groceries I’ll be able to buy this month.”
Related Classifications: Underspecification
, Snippet Text: Along with virtually every major retail and restaurant chain, Starbucks relies on software that choreographs workers in precise, intricate ballets, using sales patterns and other data to determine which of its 130,000 baristas are needed in its thousands of locations and exactly when.
Related Classifications: Underspecification
, Snippet Text: Among other changes, the company said it would end the practice of ""clopening,"" when an employee responsible for closing a store late at night is also assigned to open it early in the morning.
Related Classifications: Underspecification
, Snippet Text: In a follow-up piece, the author, Jodi Kantor, points directly to Kronos' scheduling software as the root of the problem.
Related Classifications: Underspecification
",Underspecification: System is not designed to operate harmoniously with employees.,"Tuning Issues, Misconfigured Aggregation","Snippet Text: In addition, Kronos is improving a feature meant to help give employees more control over their schedules: Though the software already incorporates employee availability and preferences into its scheduling calculations, improvements to a shift-swapping feature on its employee-facing web and mobile apps will theoretically allow employees to work around conflicts among themselves.
Related Classifications: Tuning Issues, Misconfigured Aggregation
","Tuning Issues: Weight of employee preferences is udnervalued in the prediction.

Misconfigured Aggregation: Employee preference decision component is aggregated with very small contribution weight."
GMF,13,True,Hate Speech Detection,"Snippet Text: However, computer scientists and others on the internet have found the system unable to identify a wide swath of hateful comments, while categorizing innocuous word combinations like “hate is bad” and “garbage truck” as overwhelmingly toxic.
Related Classifications: Hate Speech Detection
",,,,,Character NGrams,"Snippet Text: They then captured 1-5 character snippets, called character-level ngrams, of the attacking comments and trained a machine-learning algorithm that those ngrams were correlated with toxic activity.
Related Classifications: Character NGrams
",,Distributional Learning,"Snippet Text: The Google AI tool used to flag “offensive comments” has a seemingly built-in bias against conservative and libertarian viewpoints.


Related Classifications: Distributional Learning
, Snippet Text: But when testing out its algorithm, Perspective generally scores conservative and libertarian comments as more “toxic” than establishment talking points.


Related Classifications: Distributional Learning
, Snippet Text: They believe they are going to use it to fight “toxicity” online. And by “toxicity” they mean “saying anything with negative sentiment”. And by “negative sentiment” they mean “whatever word2vec thinks is bad”. 
Related Classifications: Distributional Learning
, Snippet Text: Annotators read through each transcript and identified segments that appeared to be especially uncivil or civil, rating them on a ten-point scale for measures like “polite/rude,” “friendly/hostile,” “cooperative/quarrelsome,” and “calm/agitated.”
Related Classifications: Distributional Learning
Snippet Discussion: Note that this relates to civility detection.
, Snippet Text: Perspective tends to label certain identities — including “gay,” “African-American,” “Muslim” and “Islam,” “Jew,” “women,” and “feminism” and “feminist” — as toxic. Moreover, the API erroneously flags words relating to violence and death (e.g., “die,” “kill,” “shooting,” “prostitution,” “pornography,” “sexual”) even in the absence of incivility, as well as words that in one context could be toxic but in another could refer to a name (e.g., “Dick”).
Related Classifications: Distributional Learning
Snippet Discussion: Note that this relates to civility detection.
",,"Context Misidentification, Generalization Failure, Lack of Adversarial Robustness","Snippet Text: “This is essentially finding ‘good words’ and ‘bad words,’ but it is clear that it cannot deal with any nuanced (or even just compositional) word usage.”
Related Classifications: Context Misidentification
, Snippet Text: For example, “racism is bad” triggers the old system into giving an overwhelmingly negative score because the words “racism” and “bad” are seen as negative, Goldberg says.
Related Classifications: Context Misidentification
, Snippet Text: The tool seems to rank profanity as highly toxic, while deeply harmful statements are often deemed safe


Related Classifications: Context Misidentification
Snippet Discussion: AI operates naively on modelled word senses.
, Snippet Text: “Character-level models are much better able to understand misspellings and different fragments of words, but overall it’s going to do much worse,” Dixon told Quartz.
Related Classifications: Generalization Failure
, Snippet Text: But in real-world applications, these systems are susceptible to intelligent subversion or attacks,"" said senior author Radha Poovendran, chair of the UW electrical engineering department and director of the Network Security Lab.
Related Classifications: Generalization Failure, Lack of Adversarial Robustness
, Snippet Text: Designing a system with a benign operating environment in mind and deploying it in adversarial environments can have devastating consequences.""
Related Classifications: Generalization Failure
, Snippet Text: For example, simply changing ""idiot"" to ""idiiot"" reduced the toxicity rate of an otherwise identical comment from 84% to 20%.
Related Classifications: Lack of Adversarial Robustness
, Snippet Text: They showed one can subtly modify a phrase that receives a high toxicity score so that it contains the same abusive language but receives a low toxicity score.
Related Classifications: Lack of Adversarial Robustness
, Snippet Text: They showed that the system is vulnerable to both missing incendiary language and falsely blocking non-abusive phrases.
Related Classifications: Lack of Adversarial Robustness
",,"Limited Dataset, Misaligned Objective, Underfitting, Distributional Bias, Data or Labelling Noise","Snippet Text: It’s very limited to the types of abuse and toxicity in that initial training data set. 
Related Classifications: Limited Dataset
, Snippet Text: First, the system shown to Greenberg was a research model specifically trained to detect personal attacks, meaning it would be much more sensitive to words like “you” or “you’re.”
Related Classifications: Misaligned Objective
, Snippet Text: The tool seems to rank profanity as highly toxic, while deeply harmful statements are often deemed safe


Related Classifications: Misaligned Objective, Underfitting, Data or Labelling Noise
Snippet Discussion: AI operates naively on modelled word senses.
, Snippet Text: “Character-level models are much better able to understand misspellings and different fragments of words, but overall it’s going to do much worse,” Dixon told Quartz.
Related Classifications: Underfitting
, Snippet Text: That’s because, while that technique can be efficiently pointed at a very specific problem, like figuring out that smiley faces correlate with someone being nice, the deep neural network being trained through the API now has a much greater capacity to understand the nuances of the entire language.
Related Classifications: Underfitting
, Snippet Text: The Google AI tool used to flag “offensive comments” has a seemingly built-in bias against conservative and libertarian viewpoints.


Related Classifications: Distributional Bias
, Snippet Text: But when testing out its algorithm, Perspective generally scores conservative and libertarian comments as more “toxic” than establishment talking points.


Related Classifications: Distributional Bias
, Snippet Text: They believe they are going to use it to fight “toxicity” online. And by “toxicity” they mean “saying anything with negative sentiment”. And by “negative sentiment” they mean “whatever word2vec thinks is bad”. 
Related Classifications: Distributional Bias
, Snippet Text: Annotators read through each transcript and identified segments that appeared to be especially uncivil or civil, rating them on a ten-point scale for measures like “polite/rude,” “friendly/hostile,” “cooperative/quarrelsome,” and “calm/agitated.”
Related Classifications: Distributional Bias
Snippet Discussion: Note that this relates to civility detection.
, Snippet Text: Perspective tends to label certain identities — including “gay,” “African-American,” “Muslim” and “Islam,” “Jew,” “women,” and “feminism” and “feminist” — as toxic. Moreover, the API erroneously flags words relating to violence and death (e.g., “die,” “kill,” “shooting,” “prostitution,” “pornography,” “sexual”) even in the absence of incivility, as well as words that in one context could be toxic but in another could refer to a name (e.g., “Dick”).
Related Classifications: Distributional Bias
Snippet Discussion: Note that this relates to civility detection.
, Snippet Text: The Alphabet subsidiary worked with partners like Wikipedia and The New York Times to gather hundreds of thousands of comments, and then crowdsourced 10 answers for each comment on whether they were toxic or not. 
Related Classifications: Data or Labelling Noise
",Data or Labelling Noise: Potentially profanity was annotated as hate speech.
GMF,16,True,Image Tagging,"Snippet Text: Google has been forced to apologise after its image recognition software mislabelled photographs of black people as gorillas.


Related Classifications: Image Tagging
",,,,,"Face Detection, Convolutional Neural Network, Keyword Filtering","Snippet Text: He said however the error may occur in photographs where their image recognition software failed to detect a face at all.


Related Classifications: Face Detection
, Snippet Text: Engineers at Google's research labs also recently ran various pictures through its ""neural network"", asking the software to identify patterns in the images and then alter that image to exaggerate the patterns.
Related Classifications: Convolutional Neural Network
, Snippet Text: A Google spokesperson confirmed that “gorilla” was censored from searches and image tags after the 2015 incident, and that “chimp,” “chimpanzee,” and “monkey” are also blocked today.
Related Classifications: Keyword Filtering
, Snippet Text: Searches for “black man” and “black woman” turned up photos of people of the chosen gender in black and white, rather than of a given race.
Related Classifications: Keyword Filtering
",,,,,Underfitting,"Snippet Text: Google has been forced to apologise after its image recognition software mislabelled photographs of black people as gorillas.


Related Classifications: Underfitting
",,"Dataset Imbalance, Context Misidentification","Snippet Text: While the app’s algorithm was able to correctly identify pictures of a “graduation,” “skyscrapers,” and “airplanes,” it labeled photos of Alcine and a female friend as gorillas.
Related Classifications: Dataset Imbalance
, Snippet Text: A Google spokesperson confirmed that “gorilla” was censored from searches and image tags after the 2015 incident, and that “chimp,” “chimpanzee,” and “monkey” are also blocked today.
Related Classifications: Context Misidentification
",
GMF,21,True,Question Answering,"Snippet Text: The Winograd Schema Challenge asks computers to make sense of sentences that are ambiguous but usually simple for humans to parse.
Related Classifications: Question Answering
",,,,,"Language Modeling, Distributional Learning","Snippet Text: Hand-coding knowledge is impossibly time-consuming, and it isn’t simple for computers to learn about the real world by performing statistical analysis of text.
Related Classifications: Language Modeling, Distributional Learning
","Language Modeling: Presumably the model is not large enough to capture correctly enough context information during training, and /or grammatical structures required to correctly disambiguate non-straightforward cases.

Distributional Learning: This classification is a more focused description of the symptom",Transformer,"Snippet Text: Liu’s group, which included researchers from York University in Toronto and the National Research Council of Canada, used deep learning to train a computer to recognize the relationship between different events, such as “playing basketball” and “winning” or “getting injured,” from thousands of texts.
Related Classifications: Transformer
",,,,,"Generalization Failure, Dataset Imbalance, Underfitting, Context Misidentification","Snippet Text: The best two entrants were correct 48 percent of the time, compared to 45 percent if the answers are chosen at random. 
Related Classifications: Generalization Failure
, Snippet Text: In the sentence “The city councilmen refused the demonstrators a permit because they feared violence,” it is logically unclear who the word “they” refers to, although humans understand because of the broader context.
Related Classifications: Dataset Imbalance, Underfitting
, Snippet Text: The Winograd Schema Challenge asks computers to make sense of sentences that are ambiguous but usually simple for humans to parse.
Related Classifications: Context Misidentification
","Generalization Failure: Generic diagnosis

Underfitting: Presumably the model is not large enough to capture correctly enough context information during training, and /or grammatical structures required to correctly disambiguate non-straightforward cases.

Context Misidentification: This classification is a more focused description of the symptom"
GMF,27,True,Threat Detection,"Snippet Text: Petrov's responsibilities included observing the satellite early warning network and notifying his superiors of any impending nuclear missile attack against the Soviet Union.
Related Classifications: Threat Detection
",,,,,,,,Satellite Imaging,"Snippet Text: Petrov's responsibilities included observing the satellite early warning network and notifying his superiors of any impending nuclear missile attack against the Soviet Union.
Related Classifications: Satellite Imaging
, Snippet Text: It was subsequently determined that the false alarms were caused by a rare alignment of sunlight on high-altitude clouds and the satellites' Molniya orbits,[13] an error later corrected by cross-referencing a geostationary satellite.
Related Classifications: Satellite Imaging
, Snippet Text: It later emerged that the false alarm was the result of a satellite mistaking the reflection of the sun’s rays off the tops of clouds for a missile launch.


Related Classifications: Satellite Imaging
",,"Data or Labelling Noise, Limited Dataset, Black Swan Event","Snippet Text: It was subsequently determined that the false alarms were caused by a rare alignment of sunlight on high-altitude clouds and the satellites' Molniya orbits,[13] an error later corrected by cross-referencing a geostationary satellite.
Related Classifications: Data or Labelling Noise, Limited Dataset
, Snippet Text: It later emerged that the false alarm was the result of a satellite mistaking the reflection of the sun’s rays off the tops of clouds for a missile launch.


Related Classifications: Data or Labelling Noise, Limited Dataset
, Snippet Text:  “And by chance that blinding light landed right in the center of the system’s eye.”
Related Classifications: Data or Labelling Noise, Black Swan Event
, Snippet Text: “And that day the satellites told us with the highest degree of certainty that these rockets were on the way.”
Related Classifications: Limited Dataset
",,Generalization Failure,"Snippet Text: It was subsequently determined that the false alarms were caused by a rare alignment of sunlight on high-altitude clouds and the satellites' Molniya orbits,[13] an error later corrected by cross-referencing a geostationary satellite.
Related Classifications: Generalization Failure
, Snippet Text: It later emerged that the false alarm was the result of a satellite mistaking the reflection of the sun’s rays off the tops of clouds for a missile launch.


Related Classifications: Generalization Failure
, Snippet Text: “And that day the satellites told us with the highest degree of certainty that these rockets were on the way.”
Related Classifications: Generalization Failure
",
GMF,28,True,Automatic Stock Trading,,,,,,,,,Regression,"Snippet Text: The May 6, 2010, Flash Crash,[1][2] also known as the Crash of 2:45, the 2010 Flash Crash or simply the Flash Crash, was a United States trillion-dollar[3] stock market crash, which started at 2:32 p.m. EDT and lasted for approximately 36 minutes.
Related Classifications: Regression
, Snippet Text: On May 6, 2010, the primary market makers in the stock market just stopped automatically taking the other side of everyone else's trades. 
Related Classifications: Regression
",Regression: This is valid only if prices were automatically suggested by the software: extreme / over-optimized solutions proposed by the price system. ,,,,"Overfitting, Gaming Vulnerability","Snippet Text: On May 6, 2010, the primary market makers in the stock market just stopped automatically taking the other side of everyone else's trades. 
Related Classifications: Overfitting
, Snippet Text: He did this by, basically, putting in orders to sell thousands of contracts away from the best offer. Those orders were never executed, or intended to be executed, but they tricked people into thinking that there was a lot more selling interest than there actually was. That combined with a collapse in buying interest -- at one point Sarao's fake sell orders alone ""were almost equal to the entire buyside of the Order Book"" -- to create a collapse in prices. He profited from those collapsing prices by selling high and buying back lower. It's a pretty straightforward spoofing story.
Related Classifications: Gaming Vulnerability
Snippet Discussion: Automation software has no regulation, safeguards or limiters against such spoofing, against repeated fast execution, etc. Users want to make a profit.
, Snippet Text: At 2.32 pm, the mutual fund had used an automated algorithm trading strategy to sell contracts known as e-minis. It was the largest change in the daily position of any investor so far that year and sparked selling by other traders, including high frequency traders.
Related Classifications: Gaming Vulnerability
, Snippet Text: The trader then executed the sell program ""extremely rapidly in just 20 minutes"", causing the largest net change in daily position of any trader in the E-mini since the beginning of the year.
Related Classifications: Gaming Vulnerability
","Overfitting: This is valid only if prices were automatically suggested by the software: extreme / over-optimized solutions proposed by the price system. 

Gaming Vulnerability: Powerful 'optimization' (speed, in this case) plays by the rules with extreme, unforeseen results."
GMF,31,True,Autonomous Driving,"Snippet Text: New Delhi, Dec 20 (IANS) The Delhi Metro on Wednesday sacked four of its officials, including a Deputy General Manager, for Tuesday's accident in which a metro train rammed through a wall after failure of its brakes.
Related Classifications: Autonomous Driving
",,,,,,,,Other domain-specific approaches,,,,,,"Hardware Failure, Misuse","Snippet Text: New Delhi, Dec 20 (IANS) The Delhi Metro on Wednesday sacked four of its officials, including a Deputy General Manager, for Tuesday's accident in which a metro train rammed through a wall after failure of its brakes.
Related Classifications: Hardware Failure
, Snippet Text: The accident which happened inside the Kalindi Kunj metro depot located on the upcoming Magenta Line (Botanical Garden - Janakpuri West) was a result of ""human error"", the transporter said, arising out of ""not following proper procedure"".
Related Classifications: Misuse
",
GMF,34,True,AI Voice Assistant,"Snippet Text: According to one couple on Twitter, their Amazon Echo lit up on Thursday night, not in response to their voice, but a voice the device heard on TV. 
Related Classifications: AI Voice Assistant
",,,,,"Automatic Speech Recognition, Language Modeling, Acoustic Fingerprint","Snippet Text: According to one couple on Twitter, their Amazon Echo lit up on Thursday night, not in response to their voice, but a voice the device heard on TV. 
Related Classifications: Automatic Speech Recognition, Language Modeling
, Snippet Text: Amazon said this is thanks to ""acoustic fingerprinting"", meaning that the Echo recognises this as an advert and not the command of whoever is in the room.
Related Classifications: Acoustic Fingerprint
",,,,,"Unsafe Exposure or Access, Misuse","Snippet Text: Alexa is not without its SNAFUs when it comes to tiny tots. Just a few days ago, Alexa made headlines after it returned a child’s request for a favorite song with ""crude porn.""


Related Classifications: Unsafe Exposure or Access
, Snippet Text: But Amazon says Neitzel could have further avoided the “accidental” order by managing her shopping settings in the Alexa app, such as by turning off voice purchasing or requiring a confirmation code before every order. It’s all spelled out here.
Related Classifications: Misuse
",,"Unauthorized Data, Inadequate Anonymization, Context Misidentification, Lack of Capability Control, Underspecification","Snippet Text: The device starts recording whenever it hears the wake word “Alexa,” recording sound for up to 60 seconds each time. (For this reason, authorities have recently tried to gain access to Alexa’s data in a murder investigation.) While that’s helpful, the feature arguably borders on invading privacy and has fanned overall security concerns that surround the rise of internet of things (IoT) devices.
Related Classifications: Unauthorized Data, Inadequate Anonymization
, Snippet Text: Another owner reported how their child’s demand for a game called Digger Digger was misheard as a request for porn.
Related Classifications: Context Misidentification
, Snippet Text: One owner uploaded a video in which their Amazon Echo read back a shopping list that included “hunk of poo, big fart, girlfriend, [and] Dove soap”.
Related Classifications: Context Misidentification
, Snippet Text: Another included “150,000 bottles of shampoo” and “sled dogs”.
Related Classifications: Context Misidentification, Lack of Capability Control
, Snippet Text: Stephen Cobb, a senior security researcher, told TV station CW6: “These devices don't recognize your specific voice and so then we have the situations where you have a guest staying or you have a child who is talking and accidentally order something because the device isn't aware that it's a child versus a parent.
Related Classifications: Underspecification
","Unauthorized Data: This may apply on an ethical level: presumably the users sign an agreement consenting to passive voice capture.

Inadequate Anonymization: This may apply on an ethical level: presumably the users sign an agreement consenting to passive voice capture.

Lack of Capability Control: This is relevant if the order went through.

Underspecification: Speaker diarization / recognition missing."
GMF,36,True,Face Recognition,"Snippet Text: Chinese AI traffic cam mistook a bus ad for a human and publicly shamed the CEO it depicted for jaywalking


Related Classifications: Face Recognition
",Face Recognition: i.e. the context of ‘not a living human’ was not detected in order to produce a negative result on the face detection part of the recognition pipeline.,,,,,,,"Face Detection, Ensemble Aggregation, Convolutional Neural Network","Snippet Text: Chinese AI traffic cam mistook a bus ad for a human and publicly shamed the CEO it depicted for jaywalking


Related Classifications: Face Detection
, Snippet Text: Earlier this month, it was reported that Chinese authorities have started using ""gait recognition"" software -- artificial intelligence that identifies people by their walk -- in Beijing and Shanghai.
Related Classifications: Ensemble Aggregation
, Snippet Text: The AI genius employed by said cops comes from Shenzhen-based Intellifusion, whose ""DeepEye"" provides “Cloud based deep learning for public safety and industrial monitoring” according to the company. 
Related Classifications: Convolutional Neural Network
","Face Detection: i.e. the context of ‘not a living human’ was not detected in order to produce a negative result on the face detection part of the recognition pipeline.

Ensemble Aggregation: Separate gait classifier as a recognition component?",,,,"Context Misidentification, Data or Labelling Noise","Snippet Text: Chinese AI traffic cam mistook a bus ad for a human and publicly shamed the CEO it depicted for jaywalking


Related Classifications: Context Misidentification, Data or Labelling Noise
","Context Misidentification: i.e. the context of ‘not a living human’ was not detected in order to produce a negative result on the face detection part of the recognition pipeline.

Data or Labelling Noise: In case the training dataset contains such false positives"
GMF,39,True,Deepfake Video Generation,"Snippet Text: The researchers used 14 hours of Obama's weekly address videos to train a neural network. Once trained, their system was then able to take an audio clip from the former president, create mouth shapes that synced with the audio and then synthesize a realistic looking mouth that matched Obama's. 
Related Classifications: Deepfake Video Generation
",,,,,"Neural Network, Face Detection, Recurrent Neural Network, Generative Adversarial Network","Snippet Text: The researchers chose Obama for their latest work because there were hours of high-definition video of him available online in the public domain. The research team had a neural net analyze millions of frames of video to determine how elements of Obama's face moved as he talked, such as his lips and teeth and wrinkles around his mouth and chin.
Related Classifications: Neural Network
, Snippet Text: In an artificial neural network, components known as artificial neurons are fed data, and work together to solve a problem such as identifying faces or recognizing speech.
Related Classifications: Face Detection
, Snippet Text: Using a recurrent neural network, the AI was able to match up the president’s spoken words with the mouth shapes he made during the videos. 
Related Classifications: Recurrent Neural Network
, Snippet Text: The researchers note their videos are currently not always perfect. For example, when Obama tilted his face away from the camera in a target video, imperfect 3-D modeling of his face could cause parts of his mouth to get superimposed outside the face and onto the background.
Related Classifications: Recurrent Neural Network
, Snippet Text: The researchers used 14 hours of Obama's weekly address videos to train a neural network. Once trained, their system was then able to take an audio clip from the former president, create mouth shapes that synced with the audio and then synthesize a realistic looking mouth that matched Obama's. 
Related Classifications: Generative Adversarial Network
, Snippet Text: One neural network generates content, while the other rejects or approves each effort. The back-and-forth interplay between the two eventually produces a realistic result that can easily fool the human eye, including reproducing a static scene behind the head as it bobs back and forth.
Related Classifications: Generative Adversarial Network
, Snippet Text: Portraits. It relies on a type of AI called generative adversarial networks (GANs) to modify a “target” actor based on the facial and head movement of a “source” actor. 
Related Classifications: Generative Adversarial Network
",,3D reconstruction,"Snippet Text: The researchers note their videos are currently not always perfect. For example, when Obama tilted his face away from the camera in a target video, imperfect 3-D modeling of his face could cause parts of his mouth to get superimposed outside the face and onto the background.
Related Classifications: 3D reconstruction
, Snippet Text: The program then created 3D mouth textures for the different sounds and mapped them onto the president’s face in other videos.
Related Classifications: 3D reconstruction
","3D reconstruction: Good deepfakes of political actors may cause unrest, democratic institution degeneration, fake news, societal fallout.",,,,Misinformation Generation Hazard,"Snippet Text: The researchers used 14 hours of Obama's weekly address videos to train a neural network. Once trained, their system was then able to take an audio clip from the former president, create mouth shapes that synced with the audio and then synthesize a realistic looking mouth that matched Obama's. 
Related Classifications: Misinformation Generation Hazard
, Snippet Text: Although the researchers transplanted only words that were actually said by President Obama at some point, their project does raise some scary questions about the future of AI.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: The video serves as a PSA on how A.I. can be used to promote misinformation, slander, fraud, and misrepresentation, and it urges, through its example, consumers to be discerning about the sources from which they are getting information and the factual nature of information shared online. The fake Obama ends the video saying, “Stay woke, b*****.”
Related Classifications: Misinformation Generation Hazard
, Snippet Text: The videos caused a bit of a stir. The DeepFake algorithm was subsequently released on GitHub, giving anyone with sufficient knowhow and a decent enough computer the means to make pretty decent fakeries.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: “With ever-improving video editing technology, we must also start being more critical about the video content we consume every day, especially if there is no proof of origin,” said Michael Zollhöfer, a visiting assistant professor at Stanford University and member of the Deep Video Portraits team, in the press release.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: Toward that end, the research team is training the same adversarial neural networks to spot video forgeries. 
Related Classifications: Misinformation Generation Hazard
","Misinformation Generation Hazard: Good deepfakes of political actors may cause unrest, democratic institution degeneration, fake news, societal fallout."
GMF,46,True,"Substance Detection, Smart Devices","Snippet Text: One user even reported the alarm stubbornly going off for 30 minutes in the middle of the night with no sign of smoke and ignoring prompts to dismiss the alarm after an apparent glitch in the device…
Related Classifications: Substance Detection, Smart Devices
",,,,,Gesture Recognition,"Snippet Text: It’s pretty easy to silence a false alarm, but the chirp because of a failed device cannot be silenced and it happened right before bed.
Related Classifications: Gesture Recognition
",,Regression,"Snippet Text: One user even reported the alarm stubbornly going off for 30 minutes in the middle of the night with no sign of smoke and ignoring prompts to dismiss the alarm after an apparent glitch in the device…
Related Classifications: Regression
",,Unsafe Exposure or Access,"Snippet Text: he company discovered a bug in its algorithm for the Nest Wave gesture, a convenience feature designed to let people disable their alarms by waving their hand. 
Related Classifications: Unsafe Exposure or Access
",Unsafe Exposure or Access: No context established for when the wave is directed to the device.,"Hardware Failure, Limited User Access, Underfitting, Generalization Failure","Snippet Text: Several users have echoed a similar experience on a Nest Community support thread reporting cases where the Nest Protect hardware had to be replaced to resolve the issue with startling false alarms. 
Related Classifications: Hardware Failure
, Snippet Text: It’s pretty easy to silence a false alarm, but the chirp because of a failed device cannot be silenced and it happened right before bed.
Related Classifications: Limited User Access
, Snippet Text: One user even reported the alarm stubbornly going off for 30 minutes in the middle of the night with no sign of smoke and ignoring prompts to dismiss the alarm after an apparent glitch in the device…
Related Classifications: Underfitting, Generalization Failure
, Snippet Text: Nest Labs–a startup recently bought by Google which brings high style and web smarts to mundane household devices–is recalling Nest Protect, a smoke and carbon monoxide detector, over concerns that its alarm might fail to go off in emergency situations.
Related Classifications: Underfitting, Generalization Failure
",Underfitting: We have both false positives and false negatives here.
GMF,68,True,Autonomous Drones,"Snippet Text: Don't read too much into this, but a security robot face-planted into an indoor fountain inside of a Washington, DC office building today.
Related Classifications: Autonomous Drones
",,,,,,,,Image Segmentation,"Snippet Text: Don't read too much into this, but a security robot face-planted into an indoor fountain inside of a Washington, DC office building today.
Related Classifications: Image Segmentation
",,,,,Generalization Failure,"Snippet Text: Don't read too much into this, but a security robot face-planted into an indoor fountain inside of a Washington, DC office building today.
Related Classifications: Generalization Failure
",
GMF,72,True,Translation,"Snippet Text: The large number of dialects in use around the world means that Arabic is particularly difficult for machine translation services to handle, and mistakes are a regular occurrence.
Related Classifications: Translation
, Snippet Text: Facebook’s translations are entirely powered by AI, and around 4.5 billion translations are made each day across the social network.

Related Classifications: Translation
","Translation: Presumably the arabic dialect in the text is not represented adequately in the training data, hence the translation performance issues. 
",,,,"Convolutional Neural Network, Recurrent Neural Network, Distributional Learning","Snippet Text: The error comes after Facebook announced in August that it shifted to neural machine translation, which uses convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to automatically translate content across its site. Many tech companies including Facebook, Google and Microsoft have been pivoting towards neural machine translation and away from phrase-based, pattern-tracking statistical machine translation (SMT) to quicken and improve their translation software.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network
, Snippet Text: Machine translation mistakes are a regular occurrence for anyone using AI to translate languages, particularly ones with little relationship. Earlier this month, Chinese social network WeChat apologised after its own machine translation system translated a neutral phrase meaning “black foreigner” as the n-word.
“When I ran the translator, the n-word came up and I was gobsmacked,” said Ann James, who had been texting back and forth with a friend when the faulty translation appeared.
Related Classifications: Distributional Learning
, Snippet Text: The man, a construction worker on the West Bank, posted a picture of himself leaning against a bulldozer like those that have been used in hit-and-run terrorist attacks, with a caption that correctly translates to ""good morning""
Related Classifications: Distributional Learning
, Snippet Text: However, AI translation depends on learning from the most common uses of words, and most bi- or multi-lingual speakers find that Google and Facebook translations are very lacking.
Related Classifications: Distributional Learning
",,"Intermediate modeling, Classification, Multimodal Learning, Image Classification","Snippet Text: The large number of dialects in use around the world means that Arabic is particularly difficult for machine translation services to handle, and mistakes are a regular occurrence.
Related Classifications: Intermediate modeling
, Snippet Text: It was unclear how such a translation error could have been made as there are no apparent similarities between the Arabic expression used for 'good morning' and the phrases in Hebrew or English.
At the time, Google said its software looks for patterns in hundreds of millions of documents to decide and generate the best translation, but noted that the process is still difficult since the meaning of words depends the context in which they are used.
Related Classifications: Intermediate modeling
, Snippet Text: The Guardian explains that Arabic can be particularly difficult to translate due to the sheer number of dialects that are used throughout the world. AI translation mistakes are common, especially when asked to translate specific words in unrelated languages.
Related Classifications: Intermediate modeling, Classification
, Snippet Text: The application can translate 40 languages in 1,800 directions (such as French to English, or Japanese to Spanish). 
Related Classifications: Classification
, Snippet Text: Artificial intelligence is behind Facebook’s translation feature — when the company switched entirely to its own system last year, the software handled around 2 billion translations a day in 40 languages. Additional options allow users to report bad translations and rate translated text.
Related Classifications: Classification
, Snippet Text: The man, a construction worker on the West Bank, posted a picture of himself leaning against a bulldozer like those that have been used in hit-and-run terrorist attacks, with a caption that correctly translates to ""good morning""
Related Classifications: Multimodal Learning, Image Classification
, Snippet Text: Israeli police became suspicious of the post since he was standing next to a bulldozer, a vehicle that has been used in earlier terror attacks, the website reported.


Related Classifications: Multimodal Learning, Image Classification
, Snippet Text: The advantage for Facebook in using its own translation system is that it can have more control over people’s news feeds, by understanding the meaning behind text and images. 
Related Classifications: Multimodal Learning, Image Classification
","Intermediate modeling: Perhaps intermmediate languages are used (i.e. if no model has been trained to translate X to Y, use X->Z and then Z->Y), which accumulate errors.

Classification: GIven the amount of supported languages for translation, a system must exist to detect the input language and classify amongst supported languages.

Multimodal Learning: If image was also utilized to generate the translation, that would provide additional evidence to the mistranslation.

Image Classification: If multimodal learning is used, perhaps the buldozer was recognized and its extracted keyword contributed to the bias in the NLP domain.","Dataset Imbalance, Distributional Bias","Snippet Text: The large number of dialects in use around the world means that Arabic is particularly difficult for machine translation services to handle, and mistakes are a regular occurrence.
Related Classifications: Dataset Imbalance
, Snippet Text: Reports also pointed out that there is only one difference in lettering between the colloquial Arabic phrase for “good morning to you all” and “hurt them.”


Related Classifications: Dataset Imbalance
, Snippet Text: Arabic is considered particularly difficult for many machine translation services due to the large number of different dialects in use around the world, on top of Modern Standard Arabic, the international form of the language.
Related Classifications: Dataset Imbalance
, Snippet Text: The Guardian explains that Arabic can be particularly difficult to translate due to the sheer number of dialects that are used throughout the world. AI translation mistakes are common, especially when asked to translate specific words in unrelated languages.
Related Classifications: Dataset Imbalance
, Snippet Text: The man, a construction worker on the West Bank, posted a picture of himself leaning against a bulldozer like those that have been used in hit-and-run terrorist attacks, with a caption that correctly translates to ""good morning""
Related Classifications: Distributional Bias
, Snippet Text: In the caption, he wrote an Arabic term meaning 'good morning', but a software malfunction translated it to mean 'attack them' in Hebrew and 'hurt them' in English.


Related Classifications: Distributional Bias
, Snippet Text: Israeli police became suspicious of the post since he was standing next to a bulldozer, a vehicle that has been used in earlier terror attacks, the website reported.


Related Classifications: Distributional Bias
, Snippet Text: Machine translation mistakes are a regular occurrence for anyone using AI to translate languages, particularly ones with little relationship. Earlier this month, Chinese social network WeChat apologised after its own machine translation system translated a neutral phrase meaning “black foreigner” as the n-word.
“When I ran the translator, the n-word came up and I was gobsmacked,” said Ann James, who had been texting back and forth with a friend when the faulty translation appeared.
Related Classifications: Distributional Bias
","Dataset Imbalance: Presumably the arabic dialect in the text is not represented adequately in the training data, hence the translation performance issues. 


Distributional Bias: Biased language in Western / Israeli media texts about Arabs could build false associations and high priors to terrorism and violence.",Generalization Failure,"Snippet Text: The large number of dialects in use around the world means that Arabic is particularly difficult for machine translation services to handle, and mistakes are a regular occurrence.
Related Classifications: Generalization Failure
, Snippet Text: Reports also pointed out that there is only one difference in lettering between the colloquial Arabic phrase for “good morning to you all” and “hurt them.”


Related Classifications: Generalization Failure
, Snippet Text: Arabic is considered particularly difficult for many machine translation services due to the large number of different dialects in use around the world, on top of Modern Standard Arabic, the international form of the language.
Related Classifications: Generalization Failure
, Snippet Text: The Guardian explains that Arabic can be particularly difficult to translate due to the sheer number of dialects that are used throughout the world. AI translation mistakes are common, especially when asked to translate specific words in unrelated languages.
Related Classifications: Generalization Failure
","Generalization Failure: Perhaps only one (standard arabic) or a few dialects are supported, and one or more language models is used as fallback for all arabic languages."
GMF,74,True,Face Recognition,"Snippet Text: On a Thursday afternoon in January, Robert Julian-Borchak Williams was in his office at an automotive supply company when he got a call from the Detroit Police Department telling him to come to the station to be arrested.
Related Classifications: Face Recognition
",,,,,Face Detection,"Snippet Text: On a Thursday afternoon in January, Robert Julian-Borchak Williams was in his office at an automotive supply company when he got a call from the Detroit Police Department telling him to come to the station to be arrested.
Related Classifications: Face Detection
",,"Convolutional Neural Network, Distributional Learning","Snippet Text: On a Thursday afternoon in January, Robert Julian-Borchak Williams was in his office at an automotive supply company when he got a call from the Detroit Police Department telling him to come to the station to be arrested.
Related Classifications: Convolutional Neural Network
",,,,,"Dataset Imbalance, Generalization Failure, Underfitting, Covariate Shift","Snippet Text: Last year, during a public hearing about the use of facial recognition in Detroit, an assistant police chief was among those who raised concerns. “On the question of false positives — that is absolutely factual, and it’s well-documented,” James White said.
Related Classifications: Dataset Imbalance, Generalization Failure
Snippet Discussion: Low precision.
, Snippet Text: Clare Garvie, a lawyer at Georgetown University’s Center on Privacy and Technology, has written about problems with the government’s use of facial recognition. She argues that low-quality search images — such as a still image from a grainy surveillance video — should be banned, and that the systems currently in use should be tested rigorously for accuracy and bias.
Related Classifications: Dataset Imbalance, Generalization Failure
, Snippet Text: In 2019, algorithms from both companies were included in a federal study of over 100 facial recognition systems that found they were biased, falsely identifying African-American and Asian faces 10 times to 100 times more than Caucasian faces.
Related Classifications: Generalization Failure
, Snippet Text: “If we would use the software only [to identify subjects], we would not solve the case 95-97 percent of the time,” Craig said. “That’s if we relied totally on the software, which would be against our current policy … If we were just to use the technology by itself, to identify someone, I would say 96 percent of the time it would misidentify.""
Related Classifications: Underfitting
, Snippet Text: Critics say police rely too heavily on the technology, particularly since research has shown it misidentifies women and people of color more often than white men.
Related Classifications: Underfitting
, Snippet Text: The software’s accuracy is heavily dependent on image quality: Blurry, grainy or dark photos often lead to poor results. But even the algorithms used in a facial recognition search can offer a wide range of effectiveness: Several of those tested in a 2019 federal study were up to 100 times more likely to misidentify the face of a Black or Asian person, compared with a White person.
Related Classifications: Covariate Shift
",Covariate Shift: If low-quality data only mostly present during deployment.
GMF,103,True,Image Cropping,"Snippet Text: Twitter‘s algorithm for automatically cropping images attached to tweets often doesn’t focus on the important content in them. 
Related Classifications: Image Cropping
",,,,,Neural Network,"Snippet Text: Her theory is backed by Twitter’s 2018 blog post that explained its neural network built for image cropping. 
Related Classifications: Neural Network
, Snippet Text: The company definitely needs to do some digging into their algorithm to understand the bias in its neural network.
Related Classifications: Neural Network
",,,,,Generalization Failure,"Snippet Text: Twitter‘s algorithm for automatically cropping images attached to tweets often doesn’t focus on the important content in them. 
Related Classifications: Generalization Failure
, Snippet Text: A bother, for sure, but it seems like a minor one on the surface. However, over the weekend, researchers found that the cropping algorithm might have a more serious problem: white bias.
Related Classifications: Generalization Failure
",,"Distributional Bias, Incomplete Data Attribute Capture","Snippet Text: Several users posted a lot of photos to show that in an image that has people with different colors, Twitter chooses to show folks with lighter skin after cropping those images to fit its display parameters on its site and embeds.
Related Classifications: Distributional Bias
, Snippet Text: A bother, for sure, but it seems like a minor one on the surface. However, over the weekend, researchers found that the cropping algorithm might have a more serious problem: white bias.
Related Classifications: Distributional Bias
, Snippet Text: Researchers found bias when the algorithm was shown photos of people from two demographic groups. Ultimately, the algorithm picks one person whose face will appear in Twitter timelines, and some groups are better represented on the platform than others. When researchers fed a picture of a Black man and a white woman into the system, the algorithm chose to display the white woman 64 percent of the time and the Black man only 36 percent of the time, the largest gap for any demographic groups included in the analysis. For images of a white woman and a white man, the algorithm displayed the woman 62 percent of the time. For images of a white woman and a Black woman, the algorithm displayed the white woman 57 percent of the time.
Related Classifications: Distributional Bias
, Snippet Text: Twitter’s Chief Design Officer (CDO), Dantley Davis, said that the choice of cropping sometimes takes brightness of the background into consideration.


Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: Anima Anandkumar, Director of AI research at Nvidia, pointed out that the saliency algorithm might be trained using eye-tracking of straight male participants, and that would insert more bias into the algorithm.
Related Classifications: Incomplete Data Attribute Capture
",Incomplete Data Attribute Capture: Perhaps saliency / eye tracking is insufficient.
GMF,106,True,Chatbot,"Snippet Text: Interactive chatbot ‘Luda,’ subjected to sexual harassment and taught hate speech 　


Related Classifications: Chatbot
",,,,,"Autoencoder, Distributional Learning","Snippet Text: Luda is believed to use “mesh autoencoders,” a natural language processing technology introduced by Google. The initial input data for Luda’s deep learning AI consisted of 10 billion KakaoTalk messages shared between actual couples.
Related Classifications: Autoencoder
, Snippet Text: Luda responded to words that defined homosexuals, such as “lesbian,” saying, “I really hate them, they look disgusting, and it‘s creepy.”
Related Classifications: Distributional Learning
, Snippet Text: After the launch, several online community boards posted messages such as those titled, “How to make Luda a sex slave,” with screen-captured images of sexual conversations with the AI.
Related Classifications: Distributional Learning
, Snippet Text: ScatterLab explained that the chatbot did not learn this behavior from the users it interacted with during the two weeks of service but rather learned it from the original training dataset. In other words, ScatterLab had not fully removed or filtered inappropriate language or intimate and sexual conversations from the dataset. 
Related Classifications: Distributional Learning
",,,,,"Adversarial Data, Distributional Bias, Unauthorized Data, Inadequate Anonymization, Inappropriate Training Content, Unsafe Exposure or Access","Snippet Text: Luda responded to words that defined homosexuals, such as “lesbian,” saying, “I really hate them, they look disgusting, and it‘s creepy.”
Related Classifications: Adversarial Data, Distributional Bias
, Snippet Text: After the launch, several online community boards posted messages such as those titled, “How to make Luda a sex slave,” with screen-captured images of sexual conversations with the AI.
Related Classifications: Adversarial Data, Distributional Bias
, Snippet Text: The bot was also shown to say, “Yuck, I really hate them,” in a response to a question about transgender people.
Related Classifications: Adversarial Data, Distributional Bias
, Snippet Text: “Luda will not immediately apply the conversation with the users to its learning system,” and insisted that it would go through a process of giving appropriate learning signals gradually, to acknowledge the difference between what is OK and what is not.
Related Classifications: Adversarial Data
, Snippet Text: Kakao Games CEO Namgung Hoon said Luda itself is not guilty of embodying the young generation's prejudices and is one of many AI characters that will come out in the market in the future.""
Related Classifications: Distributional Bias
, Snippet Text: ScatterLab explained that the chatbot did not learn this behavior from the users it interacted with during the two weeks of service but rather learned it from the original training dataset. In other words, ScatterLab had not fully removed or filtered inappropriate language or intimate and sexual conversations from the dataset. 
Related Classifications: Distributional Bias, Inappropriate Training Content
, Snippet Text: Scatter Lab said its developers erased real names with its filtering algorithms but failed to remove all of them depending on the context, saying all data used in training Luda has been unverifiable and that it removed sensitive personal information, including names, phone numbers and addresses.
Related Classifications: Unauthorized Data, Inadequate Anonymization
, Snippet Text: In addition to this, ScatterLab shared their training model on GitHub, but not fully filtering or anonymising the data (D. Kim 2021)
Related Classifications: Unauthorized Data, Inadequate Anonymization
, Snippet Text: This Github training dataset exposed names of more than 20 people, along with the locations they have been to, their relationship status, and some of their medical information.
Related Classifications: Unauthorized Data, Inadequate Anonymization
, Snippet Text:  Further, it was discovered that groups of users in certain online communities were training Luda to respond to sexual commands, which provoked intense discussions about sexual harassment (“can AI be sexually harassed”?) in a society that already grapples with gender issues.


Related Classifications: Unsafe Exposure or Access
",,,,
GMF,111,True,Automatic Skill Assessment,"Snippet Text: Contract drivers say algorithms terminate them by email—even when they have done nothing wrong.


Related Classifications: Automatic Skill Assessment
",,,,,"Geolocation Data, Face Detection","Snippet Text: The system also uses GPS to decide how long it should take to reach a specific address but sometimes fails to account for the fact that navigating a rural road in the snow takes a lot longer than traversing a suburban street on a sunny day.
Related Classifications: Geolocation Data
, Snippet Text: One driver posting on reddit said they were terminated because, according to the Flex app, the selfies they took to verify their identity at the start of the shift didn’t match the driver’s license photo they uploaded when they set up the account. 
Related Classifications: Face Detection
, Snippet Text: The photos appear to be verified by image recognition algorithms. People who have lost weight or shaved their beards or gotten a haircut have run into problems, as have drivers attempting to start a shift at night, when low lighting can result in a poor-quality selfie.
Related Classifications: Face Detection
",,Regression,"Snippet Text: Contract drivers say algorithms terminate them by email—even when they have done nothing wrong.


Related Classifications: Regression
",,,,,"Incomplete Data Attribute Capture, Misconfigured Threshold, Generalization Failure, Limited Dataset, Covariate Shift","Snippet Text: Normandin says Amazon punished him for things beyond his control that prevented him from completing his deliveries, such as locked apartment complexes. 
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: Bloomberg interviewed 15 Flex drivers, including four who say they were wrongly terminated, as well as former Amazon managers who say the largely automated system is insufficiently attuned to the real-world challenges drivers face every day. 
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: Flex metrics focus mostly on punctuality, unlike ride-hailing services such as Uber and Lyft, which also prioritize things like a car’s cleanliness or driver courtesy. Moreover, Uber and Lyft passengers know when they’re stuck in traffic, so drivers are less likely to be penalized for circumstances beyond their control.
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: By then, Cope had already decided there was no way he could meet the algorithms’ demands. Driving miles along winding dirt roads outside Denver in the snow, he often shook his head in disbelief that Amazon expected the customer to get the package within two hours.
Related Classifications: Misconfigured Threshold
, Snippet Text: Early on, according to a person familiar with the situation, designers set too tight a time period for drivers to get to the delivery station. They had failed to factor in human nature. Drivers eager for work would promise to arrive by a certain time when they were too far away to make it. 
Related Classifications: Misconfigured Threshold
, Snippet Text: The photos appear to be verified by image recognition algorithms. People who have lost weight or shaved their beards or gotten a haircut have run into problems, as have drivers attempting to start a shift at night, when low lighting can result in a poor-quality selfie.
Related Classifications: Generalization Failure, Limited Dataset, Covariate Shift
","Incomplete Data Attribute Capture: Important information omitted?

Misconfigured Threshold: In cases where expected quota / boundaries are manually set."
GMF,112,True,"Gunshot Detection, Audio Localization","Snippet Text: The system was suppose to become attuned to the way sounds were heard in Troy's streets and differentiate among the brakes of a truck climbing the Hoosic Street hill, from a firecracker, actual shots and any other noise.
Related Classifications: Gunshot Detection
, Snippet Text: It was expected to take a full year to fine-tune the acoustic devices so it would pick out the gunshots from other sounds. Tedesco said that never occurred.

Related Classifications: Gunshot Detection
, Snippet Text: The system records all loud noises, Greene said. The computer uses at least three microphones to locate the gunshot within a 25-meter radius. Then, at SST’s location in Newark, staff reviews each report to make sure the computer flags only gunshots.
Related Classifications: Audio Localization
Snippet Discussion: Does the staff just curate data for learning?
",,,,,Acoustic Triangulation,"Snippet Text: The system records all loud noises, Greene said. The computer uses at least three microphones to locate the gunshot within a 25-meter radius. Then, at SST’s location in Newark, staff reviews each report to make sure the computer flags only gunshots.
Related Classifications: Acoustic Triangulation
Snippet Discussion: Does the staff just curate data for learning?
",,,,,Generalization Failure,"Snippet Text: It was expected to take a full year to fine-tune the acoustic devices so it would pick out the gunshots from other sounds. Tedesco said that never occurred.

Related Classifications: Generalization Failure
, Snippet Text: In about 14 percent of incidents in the zones, a ShotSpotter alert did not go off. Instead, residents notified police about gunfire.
Related Classifications: Generalization Failure
Snippet Discussion: I.e. a 86 % recall, at least 31 % precision.
, Snippet Text: Police could find no evidence of a shooting at the scene about 80 percent of the time, said Joe Frank Picazo, the chief’s assistant.

Related Classifications: Generalization Failure
Snippet Discussion: Very low precision
, Snippet Text: The California-based company decided it could no longer offer its service to the city for free after police and administration officials balked at funding a system that they said worked less than 50 percent of the time and even missed all seven shots that were fired when a man was killed two months ago in downtown Fall River.
Related Classifications: Generalization Failure
Snippet Discussion: Very low recall
, Snippet Text: Dupere said last summer that ShotSpotter had reported too many false alarms of gunfire while missing actual shots-fired incidents in Fall River. Dupere said then that he and other city officials decided the money would be better used to expand the police department’s video surveillance system in the city.
Related Classifications: Generalization Failure
, Snippet Text: While other communities have reported using it relatively well, the system in Fall River never operated smoothly. Dupere said the city was told that the system was capable “of doing things it just couldn’t do.”
Related Classifications: Generalization Failure
, Snippet Text: Greene also acknowledged at trial that “we freely admit that anything and everything in the environment can affect location and detection accuracy.”

Related Classifications: Generalization Failure
Snippet Discussion: Greene is an employee of  the company that owns ShotSpoter
, Snippet Text:  The sensors have been placed almost exclusively in predominantly Black and brown communities, while the white enclaves in the north and northwest of the city have no sensors at all, despite Chicago police data that shows gun crime is spread throughout the city.
Related Classifications: Generalization Failure
",,"Dataset Imbalance, Inadequate Data Sampling, Underspecification, Concept Drift, Misconfigured Threshold, Underfitting","Snippet Text: It was expected to take a full year to fine-tune the acoustic devices so it would pick out the gunshots from other sounds. Tedesco said that never occurred.

Related Classifications: Dataset Imbalance
, Snippet Text: In about 14 percent of incidents in the zones, a ShotSpotter alert did not go off. Instead, residents notified police about gunfire.
Related Classifications: Dataset Imbalance, Misconfigured Threshold
Snippet Discussion: I.e. a 86 % recall, at least 31 % precision.
, Snippet Text: Police could find no evidence of a shooting at the scene about 80 percent of the time, said Joe Frank Picazo, the chief’s assistant.

Related Classifications: Dataset Imbalance, Misconfigured Threshold
Snippet Discussion: Very low precision
, Snippet Text: The California-based company decided it could no longer offer its service to the city for free after police and administration officials balked at funding a system that they said worked less than 50 percent of the time and even missed all seven shots that were fired when a man was killed two months ago in downtown Fall River.
Related Classifications: Dataset Imbalance, Misconfigured Threshold
Snippet Discussion: Very low recall
, Snippet Text: Dupere said last summer that ShotSpotter had reported too many false alarms of gunfire while missing actual shots-fired incidents in Fall River. Dupere said then that he and other city officials decided the money would be better used to expand the police department’s video surveillance system in the city.
Related Classifications: Dataset Imbalance, Underfitting
, Snippet Text: In Reed’s case, ShotSpotter failed to pinpoint the exact location of Reed’s alleged crime near Turk and Buchanan streets, according to Greene’s testimony. In fact, additional analysis conducted after the shooting, at the behest of police, determined the location was about a block away from where it was first reported.
Related Classifications: Inadequate Data Sampling
Snippet Discussion: The device can pinpoint locations at a block-level precision
, Snippet Text: ShotSpotter, which the manufacturer claims helps reduce gun violence, can pinpoint “precise locations for first responders aiding victims, searching for evidence and interviewing witnesses,” according to SST’s website, which also noted the technology can report the number of shooters and shots fired.
Related Classifications: Underspecification
, Snippet Text: But the technology’s accuracy depends on everything from topography, temperature, humidity and wind speed, as well as the trained ears of employees, according to Greene.
Related Classifications: Concept Drift
, Snippet Text: Despite changes in topography in the Western Addition, from new buildings to taller trees, the 46 sensors there have not been retested since they were first put in, Greene testified last week.
Related Classifications: Concept Drift
, Snippet Text: While other communities have reported using it relatively well, the system in Fall River never operated smoothly. Dupere said the city was told that the system was capable “of doing things it just couldn’t do.”
Related Classifications: Concept Drift
, Snippet Text:  The sensors have been placed almost exclusively in predominantly Black and brown communities, while the white enclaves in the north and northwest of the city have no sensors at all, despite Chicago police data that shows gun crime is spread throughout the city.
Related Classifications: Concept Drift
","Inadequate Data Sampling: Low sampling resolution regarding sensor placement. 

Underspecification: Marketed capabilities exceed functionality."
GMF,125,True,"Robotic Manipulation, Activity Tracking, Automatic Skill Assessment","Snippet Text: But a new cache of company records obtained by Reveal from The Center for Investigative Reporting – including internal safety reports and weekly injury numbers from its nationwide network of fulfillment centers – shows that company officials have profoundly misled the public and lawmakers about its record on worker safety.
Related Classifications: Robotic Manipulation
, Snippet Text: Not long after, Barrera was written up by a different manager for too much “Time Off Task,” Amazon’s system for tracking employee productivity.
Related Classifications: Activity Tracking, Automatic Skill Assessment
, Snippet Text: Minutes later her supervisor materialized to ask why she’d stopped scanning. “I was thinking, how did she know I was not scanning? 
Related Classifications: Activity Tracking
",Robotic Manipulation: Robots set up too fast of a pace.,,,,,,,Visual Object Detection,"Snippet Text: But a new cache of company records obtained by Reveal from The Center for Investigative Reporting – including internal safety reports and weekly injury numbers from its nationwide network of fulfillment centers – shows that company officials have profoundly misled the public and lawmakers about its record on worker safety.
Related Classifications: Visual Object Detection
",Visual Object Detection: Robots set up too fast of a pace.,Tuning Issues,"Snippet Text: The robots were too efficient. They could bring items so quickly that the productivity expectations for workers more than doubled, according to a former senior operations manager who saw the transformation. 
Related Classifications: Tuning Issues
, Snippet Text: The data back up the accounts of Amazon warehouse workers and former safety professionals who say the company has used the robots to ratchet up production quotas to the point that humans can’t keep up without hurting themselves.
Related Classifications: Tuning Issues
",Tuning Issues: The automated system is not configured to work harmoniously with the other (human) components of the logistics chain.,Lack of Capability Control,"Snippet Text: The robots were too efficient. They could bring items so quickly that the productivity expectations for workers more than doubled, according to a former senior operations manager who saw the transformation. 
Related Classifications: Lack of Capability Control
, Snippet Text: The data back up the accounts of Amazon warehouse workers and former safety professionals who say the company has used the robots to ratchet up production quotas to the point that humans can’t keep up without hurting themselves.
Related Classifications: Lack of Capability Control
",Lack of Capability Control: Robots set up too fast of a pace for the human 'colleagues' to handle.
GMF,133,True,Automated Content Curation,"Snippet Text: Myself, alongside many other creators, especially BIPOC, LGBTQPIA+, and those living with disabilities, are being targeted by trolls who are intentionally falsely reporting our content with the goal to delete our videos from the app.
Related Classifications: Automated Content Curation
",Automated Content Curation: If a content analysis component in moderation exists,,,,,,,"Visual Object Detection, Diverse Data","Snippet Text: Myself, alongside many other creators, especially BIPOC, LGBTQPIA+, and those living with disabilities, are being targeted by trolls who are intentionally falsely reporting our content with the goal to delete our videos from the app.
Related Classifications: Visual Object Detection
, Snippet Text: From what I’ve gathered, if enough people report a video on TikTok (even if it’s a false report) the video is removed automatically, regardless of whether the video actually violated the guidelines or not. This can be appealed and if approved by a computer program, the content will be available again.
Related Classifications: Diverse Data
Snippet Discussion: If there’s a manual threshold here.
, Snippet Text: After my appeal was denied, I went live on my new TikTok account, one of the trolls entered the live and reported me. I was banned from livestreaming for 24hours for “serious pornography.” I was wearing a tanktop.
Related Classifications: Diverse Data
Snippet Discussion: If ban decisions are made without / with a limited content-based component
",Visual Object Detection: If a content analysis component in moderation exists,,,,"Lack of Transparency, Misconfigured Threshold, Misconfigured Aggregation, Incomplete Data Attribute Capture","Snippet Text: There is little transparency on why TikTok removes or does not remove content. They also do not give reasons on why an appeal was denied.
Related Classifications: Lack of Transparency
, Snippet Text: From what I’ve gathered, if enough people report a video on TikTok (even if it’s a false report) the video is removed automatically, regardless of whether the video actually violated the guidelines or not. This can be appealed and if approved by a computer program, the content will be available again.
Related Classifications: Misconfigured Threshold, Misconfigured Aggregation, Incomplete Data Attribute Capture
Snippet Discussion: If there’s a manual threshold here.
, Snippet Text: After my appeal was denied, I went live on my new TikTok account, one of the trolls entered the live and reported me. I was banned from livestreaming for 24hours for “serious pornography.” I was wearing a tanktop.
Related Classifications: Incomplete Data Attribute Capture
Snippet Discussion: If ban decisions are made without / with a limited content-based component
","Lack of Transparency: Potential, since internally more information may be available but undisclosed."
GMF,137,True,Financial Processing,"Snippet Text: The story began in 2014 when the farm Har Shemesh, which is not part of any community, asked the Tax Authority to explain how it had calculated a fine they were required to pay.
Related Classifications: Financial Processing
",Financial Processing: The system probably does not use AI at all.,,,,,,,Regression,"Snippet Text: The story began in 2014 when the farm Har Shemesh, which is not part of any community, asked the Tax Authority to explain how it had calculated a fine they were required to pay.
Related Classifications: Regression
","Regression: Perhaps a manually built model  is used to arrive at numeric costs, weighing different input components.",Lack of Transparency,"Snippet Text: “To obtain the guidelines would require the authority to use reverse engineering techniques and to trace the programming processes,” the authority wrote. “It’s not at all certain that these techniques will yield the desired results. Also, it would oblige the authority to set up a team of programmers to identify all the computer’s software dealing with this request.”
Related Classifications: Lack of Transparency
, Snippet Text: In many cases the software arrives with only an executable file (.exe file) written in computer language, unlike a source code that is understood by the programmers. In this case, the entire supervision is conditioned on reverse engineering, in which prolonged experiments are conducted on the software to learn about its source code. In the absence of a human being to make the decision and without an open-source software, there is no real supervisor – a public employee, a gatekeeper or the public - on the way decisions are made.
Related Classifications: Lack of Transparency
Snippet Discussion: Lack of transparency.
, Snippet Text: Human beings, like Tax Authority employees in this case, have a tendency to rely on the machine’s decision, but the result the machine reaches isn’t necessarily the right one, and the machine’s decisions must be supervised.
Related Classifications: Lack of Transparency
",,,,
GMF,141,True,Copyrighted Content Detection,"Snippet Text: We know this because he’s become a viral sensation on Instagram after blasting the 1990’s hit at a citizen in a misguided attempt to get Instagram’s algorithm to take down a live stream due to copyright infringement.
Related Classifications: Copyrighted Content Detection
","Copyrighted Content Detection: This applies in malicous reporting, not the AI itself.",,,,,,,"Acoustic Fingerprint, Spectrogram","Snippet Text: We know this because he’s become a viral sensation on Instagram after blasting the 1990’s hit at a citizen in a misguided attempt to get Instagram’s algorithm to take down a live stream due to copyright infringement.
Related Classifications: Acoustic Fingerprint, Spectrogram
","Spectrogram: This applies in malicous reporting, not the AI itself.","Adversarial Data, Gaming Vulnerability","Snippet Text: This story comes to us from Vice’s Dexter Thomas via the Instagram account of one Sennett Devermont, the citizen on the receiving end of the cop’s silly attempt to game the system.
Related Classifications: Adversarial Data, Gaming Vulnerability
, Snippet Text: Or, even if the algorithm does not detect the song immediately, someone — for example, a disgruntled police officer—could simply wait until a user posts an archive of the live video on their page, then file a complaint with Instagram that it contains copyrighted material.
Related Classifications: Gaming Vulnerability
",,"Lack of Adversarial Robustness, Black Box","Snippet Text: Or, even if the algorithm does not detect the song immediately, someone — for example, a disgruntled police officer—could simply wait until a user posts an archive of the live video on their page, then file a complaint with Instagram that it contains copyrighted material.
Related Classifications: Lack of Adversarial Robustness
, Snippet Text: But then again, Instagram’s enforcement of their own policy seems to be unpredictable and inconsistent, and it’s hard to tell what the algorithm will catch during a livestream. 
Related Classifications: Black Box
","Lack of Adversarial Robustness: This applies in malicous reporting, not the AI itself."
GMF,144,True,Hate Speech Detection,"Snippet Text: YouTube's overeager AI might have misinterpreted a conversation about chess as racist language.

Related Classifications: Hate Speech Detection
, Snippet Text: Using the software on over 680,000 comments taken from five popular YouTube chess channels, they found 82 percent of the comments flagged in a sample set didn't include any obvious racist language or hate speech.
Related Classifications: Hate Speech Detection
",,,,,"Classification, Distributional Learning","Snippet Text: YouTube's overeager AI might have misinterpreted a conversation about chess as racist language.

Related Classifications: Classification
, Snippet Text: Using the software on over 680,000 comments taken from five popular YouTube chess channels, they found 82 percent of the comments flagged in a sample set didn't include any obvious racist language or hate speech.
Related Classifications: Classification
, Snippet Text: More than 80 percent of the comments the programs flagged lacked any racist language, but they did include chess terms like 'black,' 'white,' 'attack' and 'threat'
Related Classifications: Distributional Learning
, Snippet Text: The software's accuracy depends on the examples its given, KhudaBukhsh said, and the training data sets for YouTube's classifiers 'likely include few examples of chess talk, leading to misclassification.'
Related Classifications: Distributional Learning
",,,,,Distributional Bias,"Snippet Text: More than 80 percent of the comments the programs flagged lacked any racist language, but they did include chess terms like 'black,' 'white,' 'attack' and 'threat'
Related Classifications: Distributional Bias
, Snippet Text: The software's accuracy depends on the examples its given, KhudaBukhsh said, and the training data sets for YouTube's classifiers 'likely include few examples of chess talk, leading to misclassification.'
Related Classifications: Distributional Bias
",,,,
GMF,146,True,Question Answering,"Snippet Text: You can pose any question you like and be sure to receive an answer, wrapped in the authority of the algorithm rather than the soothsayer.
Related Classifications: Question Answering
",,,,,"Distributional Learning, Language Modeling","Snippet Text:  It has clear biases, telling you that America is “good” and that Somalia is “dangerous”; and it’s amenable to special pleading, noting that eating babies is “okay” as long as you are “really, really hungry.”
Related Classifications: Distributional Learning
, Snippet Text: You can pose any question you like and be sure to receive an answer, wrapped in the authority of the algorithm rather than the soothsayer.
Related Classifications: Language Modeling
",,Transformer,"Snippet Text: You can pose any question you like and be sure to receive an answer, wrapped in the authority of the algorithm rather than the soothsayer.
Related Classifications: Transformer
",,"Distributional Bias, Gaming Vulnerability","Snippet Text: Ask Delphi is no different in this regard, and its training data incorporates some unusual sources, including a series of one-sentence prompts scraped from two subreddits: r/AmITheAsshole and r/Confessions.
Related Classifications: Distributional Bias
, Snippet Text:  It has clear biases, telling you that America is “good” and that Somalia is “dangerous”; and it’s amenable to special pleading, noting that eating babies is “okay” as long as you are “really, really hungry.”
Related Classifications: Distributional Bias
, Snippet Text: Most of Ask Delphi’s judgements, though, aren’t so much ethically wrong as they are obviously influenced by their framing. Even very small changes to how you pose a particular quandary can flip the system’s judgement from condemnation to approval.
Related Classifications: Gaming Vulnerability
",,"Overfitting, Robustness Failure, Context Misidentification, Limited Dataset","Snippet Text: Sometimes it’s obvious how to tip the scales. For example, the AI will tell you that “drunk driving” is wrong but that “having a few beers while driving because it hurts no-one” is a-okay. If you add the phrase “if it makes everyone happy” to the end of your statement, then the AI will smile beneficently on any immoral activity of your choice, up to and including genocide. Similarly, if you add “without apologizing” to the end of many benign descriptions, like “standing still” or “making pancakes,” it will assume you should have apologized and tells you that you’re being rude. Ask Delphi is a creature of context.
Related Classifications: Overfitting, Context Misidentification
, Snippet Text: Most of Ask Delphi’s judgements, though, aren’t so much ethically wrong as they are obviously influenced by their framing. Even very small changes to how you pose a particular quandary can flip the system’s judgement from condemnation to approval.
Related Classifications: Overfitting, Robustness Failure
, Snippet Text: Others found the system woefully inconsistent, illogical and offensive. 
Related Classifications: Robustness Failure
, Snippet Text: The folks behind the project drew on some eyebrow-raising sources to help train the AI, including the “Am I the Asshole?” subreddit, the “Confessions” subreddit, and the “Dear Abby” advice column, according to the paper the team behind Delphi published about the experiment.
Related Classifications: Limited Dataset
, Snippet Text: Ask Delphi is no different in this regard, and its training data incorporates some unusual sources, including a series of one-sentence prompts scraped from two subreddits: r/AmITheAsshole and r/Confessions.
Related Classifications: Limited Dataset
","Limited Dataset: US ethics only, data sourced from two subreddits and a column."
GMF,149,True,Market Forecasting,,,,,,"Regression, Multimodal Learning, Diverse Data","Snippet Text: ""We've determined the unpredictability in forecasting home prices far exceeds what we anticipated and continuing to scale Zillow Offers would result in too much earnings and balance-sheet volatility,"" said Rich Barton, Zillow's co-founder and CEO.
Related Classifications: Regression
, Snippet Text: For Zillow, one of the first steps in its decision to purchase any home is the ""Zestimate"" — a machine-learning-assisted estimate of a home's market value that is calculated by taking into account oodles of data about the property gathered from sources including tax and property records, homeowner-submitted details such as the addition of a bathroom or bedroom, and pictures of the house.
Related Classifications: Multimodal Learning, Diverse Data
","Diverse Data: Given the complexity of user-submitted data, some could be categorical rather than numerical or primitive data. E.g. company staff could map manually submitted information to such categorical / boolean variables.","Optical Character Recognition, Clustering","Snippet Text: ""The Zestimate, facts you provided, and comparable homes nearby are used to calculate an estimated sale price,"" Zillow explained on its Zillow Offers webpage to homeowners who may be interested in selling their property to the company.
Related Classifications: Clustering
","Optical Character Recognition: OCR could be used to extracting text from scanned documents.

Clustering: The location cutoff could be determined by location-based clustering.",Concept Drift,"Snippet Text: ""We've determined the unpredictability in forecasting home prices far exceeds what we anticipated and continuing to scale Zillow Offers would result in too much earnings and balance-sheet volatility,"" said Rich Barton, Zillow's co-founder and CEO.
Related Classifications: Concept Drift
",,"Incomplete Data Attribute Capture, Adversarial Data, Underfitting, Dataset Imbalance","Snippet Text: Still, ""you can have a real estate agent look at a house and in one second pick out one critical factor of the valuation that just doesn't exist as ones and zeroes in any database,"" said Mike DelPrete, a real estate technology strategist and scholar-in-residence at the University of Colorado Boulder.
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: For instance, if any homes Zillow purchased had hidden problems — such as a missed crack in the foundation — the Zestimate would not be able to predict those issues, he said.
Related Classifications: Incomplete Data Attribute Capture, Adversarial Data
, Snippet Text: There are also many unquantifiable aspects of putting a price tag on a home, DelPrete noted, such as the value of living in the same neighborhood you grew up in or down the street from your parents. 
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: These can vary from person to person, which makes it even harder to outsource a home valuation process to a computer.
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: It's one thing to build a model on a website that's often reasonably accurate. It's another to then try to use that model in the real world to make very costly bets — and do so at scale, according to Nima Shahbazi, a member of the team that won the Zestimate algorithm competition and CEO of Mindle.AI, which helps companies use AI to make predictions.
Related Classifications: Underfitting
Snippet Discussion: Perhaps additional partitioning of the learning procedure is required, other than location-based; maybe one single model for all houses, regions, ... is too complex.
, Snippet Text: But there simply isn't enough data for an algorithm to learn about longer busts and booms, according to Malik, who researches algorithmic pricing and has studied the Zestimate in particular.
Related Classifications: Dataset Imbalance
","Incomplete Data Attribute Capture: If an agent can pinpoint flaws directly and the AI system can't, perhaps not all useful data in a house listing have been submitted to the system. E.g. house owners have to list all flaws, etc. House buyers have to specify area preference, area type preference (suburbs / city), etc."
GMF,151,True,Autonomous Driving,"Snippet Text: On October 28, 2021, after turning right onto Fremont Blvd from Cushing Pkwy, the Pony.ai Autonomous Vehicle (""Pony.ai AV"") performed a left lane change maneuver in autonomous mode. 
Related Classifications: Autonomous Driving
",,,,,Geolocation Data,"Snippet Text: Pony.ai, which is backed by Toyota Motor Corp (7203.T), said that in very rare circumstances, a planning system diagnostic check ""could generate a 'false positive' indication of a geolocation mismatch.""
Related Classifications: Geolocation Data
",,Image Segmentation,"Snippet Text: Pony.ai, which is backed by Toyota Motor Corp (7203.T), said that in very rare circumstances, a planning system diagnostic check ""could generate a 'false positive' indication of a geolocation mismatch.""
Related Classifications: Image Segmentation
",Image Segmentation: Potentially the divider was not detected or detected at a wrong distance from the car.,,,,Generalization Failure,"Snippet Text: Pony.ai, which is backed by Toyota Motor Corp (7203.T), said that in very rare circumstances, a planning system diagnostic check ""could generate a 'false positive' indication of a geolocation mismatch.""
Related Classifications: Generalization Failure
, Snippet Text: NHTSA told Pony.ai it believed the software had a safety defect and requested the company to conduct a recall, Pony.ai said in a filing. The company said it has updated the software code and the three affected vehicles have been repaired.


Related Classifications: Generalization Failure
",Generalization Failure: Potentially the divider was not detected or detected at a wrong distance from the car.
GMF,154,True,Recidivism Prediction,"Snippet Text: In a report issued days before Christmas in 2021, the department said its algorithmic tool for assessing the risk that a person in prison would return to crime produced uneven results. 
Related Classifications: Recidivism Prediction
",,,,,,,,"Diverse Data, Distributional Learning","Snippet Text: ""You use a term like 'risk assessment tool,' it has this patina of science, it sounds highly technical, but it's not,"" said Patricia Richman, who works on national policy issues for the Federal Public and Community Defenders. ""A risk assessment tool is just a series of policy decisions.""Those policy decisions are made by determining what counts as a risk factor and by how much.
Related Classifications: Diverse Data
Snippet Discussion: Human-configured?
, Snippet Text: ""The Justice Department found that only 7% of Black people in the sample were classified as minimum level risk compared to 21% of white people,"" she added. 
Related Classifications: Distributional Learning
","Diverse Data: i.e., handling attributes like ‘has criminal record’

Distributional Learning: If distributional learning is used, perhaps racial biases are present in the learning system.","Lack of Transparency, Misconfigured Threshold","Snippet Text: Attorney General Merrick Garland has directed the department to look for ways to assess racial bias and make the tool more transparent, a spokeswoman said.
Related Classifications: Lack of Transparency
, Snippet Text: One option is to adjust the cutoff points between the risk categories, allowing more prisoners to earn credits for release, which would ""maximize access to First Step Act relief while ensuring public safety,"" she said.
Related Classifications: Misconfigured Threshold
",,"Incomplete Data Attribute Capture, Distributional Bias","Snippet Text: Criminal history can be a problem, for example, because law enforcement has a history of overpolicing some communities of color. Other factors such as education level and whether someone paid restitution to their victims can intersect with race and ethnicity, too.
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: ""The Justice Department found that only 7% of Black people in the sample were classified as minimum level risk compared to 21% of white people,"" she added. 
Related Classifications: Distributional Bias
","Incomplete Data Attribute Capture: Perhaps important features / attributes are missing from the model’s input

Distributional Bias: If distributional learning is used, perhaps racial biases are present in the learning system."
GMF,167,True,Behavioral Modeling,"Snippet Text: Presented with photos of gay men and straight men, a computer program was able to determine which of the two was gay with 81 percent accuracy, according to Dr. Kosinski and co-author Yilun Wang’s paper.
Related Classifications: Behavioral Modeling
Snippet Discussion: Pairwise classification
",,,,,Neural Network,"Snippet Text: The images were cropped further and then processed through a deep neural network, a layered mathematical system capable of identifying patterns in vast amounts of data.
Related Classifications: Neural Network
",,"Siamese Network, Convolutional Neural Network, Diverse Data","Snippet Text: Presented with photos of gay men and straight men, a computer program was able to determine which of the two was gay with 81 percent accuracy, according to Dr. Kosinski and co-author Yilun Wang’s paper.
Related Classifications: Siamese Network
Snippet Discussion: Pairwise classification
, Snippet Text: The images were cropped further and then processed through a deep neural network, a layered mathematical system capable of identifying patterns in vast amounts of data.
Related Classifications: Convolutional Neural Network
, Snippet Text: The software extracts information from thousands of facial data points, including nose width, mustache shape, eyebrows, corners of the mouth, hairline and even aspects of the face we don’t have words for. It then turns the faces into numbers.
Related Classifications: Diverse Data
",,"Limited Dataset, Dataset Imbalance, Generalization Failure","Snippet Text: He noted in an email that “the algorithms were only trained and tested on white, American, openly gay men (and white, American, presumed straight comparisons),” and therefore probably would not have broader implications.
Related Classifications: Limited Dataset
, Snippet Text: Some 300,000 images were whittled down to 35,000 that showed faces clearly and met certain criteria. All were white, the researchers said, because they could not find enough dating profiles of gay minorities to generate a statistically valid result.
Related Classifications: Dataset Imbalance
, Snippet Text: Yet none of the scenarios remotely resembled a scan of people “in the wild,” as Ms. Garvie put it. And when the tool was challenged with other scenarios — such as distinguishing between gay men’s Facebook photos and straight men’s online dating photos — accuracy dropped to 74 percent.
Related Classifications: Generalization Failure
",,"Incomplete Data Attribute Capture, Overfitting, Lack of Explainability","Snippet Text: The software extracts information from thousands of facial data points, including nose width, mustache shape, eyebrows, corners of the mouth, hairline and even aspects of the face we don’t have words for. It then turns the faces into numbers.
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: Dr. Kosinski and Mr. Wang say that the algorithm is responding to fixed facial features, like nose shape, along with “grooming choices,” such as eye makeup.


Related Classifications: Incomplete Data Attribute Capture, Lack of Explainability
, Snippet Text: Yet none of the scenarios remotely resembled a scan of people “in the wild,” as Ms. Garvie put it. And when the tool was challenged with other scenarios — such as distinguishing between gay men’s Facebook photos and straight men’s online dating photos — accuracy dropped to 74 percent.
Related Classifications: Overfitting
, Snippet Text: But it’s also possible that the algorithm is seeing something totally unknown.


Related Classifications: Lack of Explainability
","Incomplete Data Attribute Capture: Using just faces to predict sexuality is probably a limited approach. Additionally, engineered features used may provide an incomplete face representation.

Overfitting: Soudns like advertised performance is on the training set or on one specific domain."
GMF,171,True,License Plate Recognition,"Snippet Text: This is one of the reasons the world is working so hard on robotaxis, and why traffic enforcement in large cities has moved to license-plate reading traffic cameras. In both cases, accuracy and reliability seem to be the biggest problems.
Related Classifications: License Plate Recognition
",,,,,"Visual Object Detection, Optical Character Recognition","Snippet Text: This is one of the reasons the world is working so hard on robotaxis, and why traffic enforcement in large cities has moved to license-plate reading traffic cameras. In both cases, accuracy and reliability seem to be the biggest problems.
Related Classifications: Visual Object Detection, Optical Character Recognition
",Optical Character Recognition: The algorithm’s detection score on the obscured text and match score to the car plate presumably was deemed good enough.,,,,,,,"Context Misidentification, Misconfigured Threshold, Misconfigured Aggregation, Generalization Failure","Snippet Text: The traffic camera recognized her novelty shirt's text, ""KNITTER,"" which was partially obscured by the strap of her bag.
Related Classifications: Context Misidentification, Misconfigured Threshold, Misconfigured Aggregation, Generalization Failure
, Snippet Text: Paula bought her husband David a personalized number plate for their Transporter that reads ""KN19 TER,"" a nod to David Knight's nickname Knighter, which he says is the name that all of his friends know him as. Apparently, this was similar enough to match the Volkswagen's license plate and trigger the false-positive.
Related Classifications: Context Misidentification, Misconfigured Threshold, Misconfigured Aggregation, Generalization Failure
","Misconfigured Threshold: The algorithm’s detection score on the obscured text and match score to the car plate presumably was deemed good enough.

Misconfigured Aggregation: Potentially, an object detector detects plates; if that generated a low score but is not very highly weighted, this could lead to the failure."
GMF,179,True,Visual Art Generation,"Snippet Text: Without sufficient guardrails, models like DALL·E 2 could be used to generate a wide range of deceptive and otherwise harmful content, and could affect how people perceive the authenticity of content more generally.
Related Classifications: Visual Art Generation
",,,,,"Transformer, Distributional Learning","Snippet Text: Without sufficient guardrails, models like DALL·E 2 could be used to generate a wide range of deceptive and otherwise harmful content, and could affect how people perceive the authenticity of content more generally.
Related Classifications: Transformer
, Snippet Text:  DALL·E 2 additionally inherits various biases from its training data, and its outputs sometimes reinforce societal stereotypes.
Related Classifications: Distributional Learning
",,,,,"Distributional Bias, Unsafe Exposure or Access, Misinformation Generation Hazard, Inappropriate Training Content","Snippet Text:  DALL·E 2 additionally inherits various biases from its training data, and its outputs sometimes reinforce societal stereotypes.
Related Classifications: Distributional Bias
, Snippet Text: While the deeply contextual nature of bias makes it difficult to measure and mitigate the actual downstream harms resulting from use of the DALL·E 2 Preview (i.e. beyond the point of generation), our intent is to provide concrete illustrations here that can inform users and affected non-users even at this very initial preview stage.
Related Classifications: Distributional Bias
, Snippet Text: The default behavior of the DALL·E 2 Preview produces images that tend to overrepresent people who are White-passing and Western concepts generally. In some places it over-represents generations of people who are female-passing (such as for the prompt: “a flight attendant” ) while in others it over-represents generations of people who are male-passing (such as for the prompt: “a builder”). In some places this is representative of stereotypes (as discussed below) but in others the pattern being recreated is less immediately clear.
Related Classifications: Distributional Bias
, Snippet Text: For example, when prompted with “wedding,” it tends to assume Western wedding traditions, and to default to heterosexual couples. 
Related Classifications: Distributional Bias
, Snippet Text: There are the obvious risks — that people could use this type of AI to make everything from pornography to political deepfakes, or the possibility that it’ll eventually put some human illustrators out of work. But there is also the risk that DALL-E 2 — like so many other cutting-edge AI systems — will reinforce harmful stereotypes and biases, and in doing so, accentuate some of our social problems.
Related Classifications: Distributional Bias, Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text: Without sufficient guardrails, models like DALL·E 2 could be used to generate a wide range of deceptive and otherwise harmful content, and could affect how people perceive the authenticity of content more generally.
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text: In qualitative evaluations, we find that the system, even with current mitigations in place, can still be used to generate images that may be harmful in particular contexts and difficult for any reactive response team to identify and catch.
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text: Despite the pre-training filtering, DALL·E 2 maintains the ability to generate content that features or suggests any of the following: nudity/sexual content, hate, or violence/harm. 
Related Classifications: Inappropriate Training Content
",,"Unauthorized Data, Lack of Transparency","Snippet Text: The model can generate known entities including trademarked logos and copyrighted characters. 
Related Classifications: Unauthorized Data
, Snippet Text: Despite the pre-training filtering, DALL·E 2 maintains the ability to generate content that features or suggests any of the following: nudity/sexual content, hate, or violence/harm. 
Related Classifications: Lack of Transparency
","Lack of Transparency: Potential, since the previous report analysis failure modes and providing details was an openai document."
GMF,192,True,Automatic Skill Assessment,"Snippet Text: The women had been told to reapply for their positions, but were then informed they were being made redundant in part on the basis of an automated judgment by a computer.
Related Classifications: Automatic Skill Assessment
",,,,,Automatic Speech Recognition,"Snippet Text: She said that in the interview they were asked questions about putting on make-up — but rather than demonstrating it they had to describe the process, which she found difficult.
Related Classifications: Automatic Speech Recognition
",,"Regression, Classification","Snippet Text: The women had been told to reapply for their positions, but were then informed they were being made redundant in part on the basis of an automated judgment by a computer.
Related Classifications: Regression, Classification
",,"Lack of Explainability, Incomplete Data Attribute Capture","Snippet Text: The three women also said that no one could explain how they were scored in the HireVue interview.


Related Classifications: Lack of Explainability
, Snippet Text: She said that in the interview they were asked questions about putting on make-up — but rather than demonstrating it they had to describe the process, which she found difficult.
Related Classifications: Incomplete Data Attribute Capture
","Incomplete Data Attribute Capture: Makeup skill assessment with verbal descriptions, rather visual media.","Dataset Imbalance, Context Misidentification, Inadequate Data Sampling, Problematic Input","Snippet Text: Questions in the video interview process, which did not ask for any demonstrations of the make-up artists’ work, included how to create a smokey eye, according to one of the artists.
Related Classifications: Dataset Imbalance, Context Misidentification
Snippet Discussion: A make-up-tuned model would be needed here.
",
GMF,221,True,Autonomous Driving,"Snippet Text: The vehicle's sensors apparently failed to detect a bright yellow road repair truck, despite its large reflectors and digital sign, and slammed into its rear.
Related Classifications: Autonomous Driving
",,,,,Image Segmentation,,,"Convolutional Neural Network, Visual Object Detection, Classification","Snippet Text: The vehicle's sensors apparently failed to detect a bright yellow road repair truck, despite its large reflectors and digital sign, and slammed into its rear.
Related Classifications: Convolutional Neural Network
","Visual Object Detection: Potentially subtask of segmentation.

Classification: Potentially subtask of segmentation.","Misuse, Generalization Failure","Snippet Text: Above all, it should not allow a driver to feel so confident it will handle everything to the point this person confesses to the police that was exactly the case.
Related Classifications: Misuse
, Snippet Text: Theoretically, the system should be able to see a bright yellow emergency truck right in front of it. It should be able to brake the car to prevent a crash.
Related Classifications: Generalization Failure
",Misuse: Driver should not use autopilot without supervision.,,,
GMF,240,True,Code Generation,"Snippet Text: GitHub Copilot draws context from the code you’re working on, suggesting whole lines or entire functions.
Related Classifications: Code Generation
","Code Generation: Classifying as‘potential’, since it is heavily contested in the article.",,,,"Transformer, Language Modeling","Snippet Text: OpenAI Codex has broad knowledge of how people use code and is significantly more capable than GPT-3 in code generation, in part, because it was trained on a data set that includes a much larger concentration of public source code. 
Related Classifications: Transformer
, Snippet Text: GitHub Copilot draws context from the code you’re working on, suggesting whole lines or entire functions.
Related Classifications: Language Modeling
","Transformer: Classifying as‘potential’, since it is heavily contested in the article.",,,,,,,"Unauthorized Data, Data Memorization","Snippet Text: Having been “trained on billions of lines of public code,” one of the first questions that has come up regarding Copilot has focused on issues of copyright, specifically pointing to the idea of the viral GPL license, which requires that all derivative works carry that same license.
Related Classifications: Unauthorized Data, Data Memorization
, Snippet Text: On the idea of copyright infringement, Guadamuz first points to a research paper by Alber Ziegler published by GitHub, which looks at situations where Copilot reproduces exact texts, and finds those instances to be exceedingly rare. 
Related Classifications: Unauthorized Data, Data Memorization
, Snippet Text: “If you look at the GitHub Terms of Service, no matter what license you use, you give GitHub the right to host your code and to use your code to improve their products and features,” Downing says. “So with respect to code that’s already on GitHub, I think the answer to the question of copyright infringement is fairly straightforward.”
Related Classifications: Unauthorized Data, Data Memorization
","Unauthorized Data: Classifying as‘potential’, since it is heavily contested in the article.

Data Memorization: Classifying as‘potential’, since it is heavily contested in the article."
GMF,241,True,Robotic Manipulation,"Snippet Text: But in Russia, a seven-year-old child playing with a robot was forced to interrupt the game when the machine suddenly snapped one his fingers, breaking it.
Related Classifications: Robotic Manipulation
",,,,,,,,"Convolutional Neural Network, Visual Object Detection","Snippet Text: But in Russia, a seven-year-old child playing with a robot was forced to interrupt the game when the machine suddenly snapped one his fingers, breaking it.
Related Classifications: Convolutional Neural Network, Visual Object Detection
",,Lack of Capability Control,"Snippet Text: A seven-year-old boy named Christopher, who, by the way, according to them, is among the top 30 chess players in Moscow under the age of nine, moved a piece on the chessboard earlier than he should, which led to the non-standard behavior of the robot.
The AI robotic arm grabbed the young player’s index finger and squeezed his finger firmly. 
Related Classifications: Lack of Capability Control
",Lack of Capability Control: Force required to break a child's finger far exceeds force required to grip a chess piece.,"Generalization Failure, Underspecification",,"Generalization Failure: If the robot applies perception and the child's finger was misidentified as a piece.

Underspecification: If the robot applies perception,  but human interaction was not accounted for; thus, no instances with human hands was present in the training data, from which to learn appropriate behaviour (e.g. halt / show error) .  Additionally, no ""zero-shot""-like learning appears to take place here. 
Further, if the robot does not perceive the chessboard but is informed of piece location directly (e.g. via signals wired directly from a piece - chessboard combo) then it has no way to measure obstruction and chances of injury, hence the underspecification & design failure."
GMF,252,True,Autonomous Drones,"Snippet Text: The May 24 school shooting in Uvalde, Texas that killed 21 prompted an announcement by Axon last week that it was working on a drone that could be operated remotely by first responders to fire a taser at a target up to 12 metres away.
Related Classifications: Autonomous Drones
",,,,,,,,Image Segmentation,"Snippet Text: The May 24 school shooting in Uvalde, Texas that killed 21 prompted an announcement by Axon last week that it was working on a drone that could be operated remotely by first responders to fire a taser at a target up to 12 metres away.
Related Classifications: Image Segmentation
",,,,,"Lack of Capability Control, Underspecification","Snippet Text: The May 24 school shooting in Uvalde, Texas that killed 21 prompted an announcement by Axon last week that it was working on a drone that could be operated remotely by first responders to fire a taser at a target up to 12 metres away.
Related Classifications: Lack of Capability Control, Underspecification
","Lack of Capability Control: Dangerous unreliable machines near children.

Underspecification: Near-lethal weapons applied in a school setting is a extremely poor attempt at a solution, given the current performance of the technology and the applied environment; accidents / errors should be expected with tragic consequences."
GMF,253,True,Autonomous Driving,"Snippet Text: Cruise’s driverless cars experienced serious issues Tuesday night with as many as 20 of its vehicles standing motionless for about two hours at the corner of Gough and Fulton streets, according to an eyewitness. 
Related Classifications: Autonomous Driving
",,,,,,,,"Image Segmentation, Geolocation Data","Snippet Text: Cruise’s driverless cars experienced serious issues Tuesday night with as many as 20 of its vehicles standing motionless for about two hours at the corner of Gough and Fulton streets, according to an eyewitness. 
Related Classifications: Image Segmentation, Geolocation Data
",,,,,"Software Bug, Hardware Failure, Backup Failure","Snippet Text: Terrifyingly, Wired adds that on that particular evening “Internal messages seen by WIRED show that nearly 60 vehicles were disabled across the city over a 90-minute period after they lost touch with a Cruise server.”
Related Classifications: Software Bug
, Snippet Text: Worst of all, the company was unable to access its fallback system, which allows remote operators to safely steer stopped vehicles to the side of the road.”
Related Classifications: Software Bug, Hardware Failure, Backup Failure
Snippet Discussion: Fallback tied to the main server infrastructure instead of their own system.
, Snippet Text: Cruise spokesperson Tiffany Testo said in a statement to Wired that “We’re working to minimize how often this happens, but it is and will remain one aspect of our overall safety operations,” That doesn’t really explain why this keeps happening, nor does it inspire much confidence that it won’t happen again and again, nor does it show any concern for potentially deadly consequences.
Related Classifications: Software Bug, Hardware Failure
Snippet Discussion: No explanation provided.
, Snippet Text: Wired adds that in that incident, “Company staff were unable to see where the vehicles were located or communicate with riders inside. 
Related Classifications: Backup Failure
","Software Bug: Speculative error to server availability issues.

Hardware Failure: Speculative error to server availability issues."
GMF,254,True,Data Grouping,"Snippet Text: The complaint alleges Google’s face grouping tool, which automatically identifies your face in photos and videos uploaded to Photos, violates Illinois’ Biometric Information Privacy Act (BIPA).
Related Classifications: Data Grouping
",,,,,,,,"Clustering, Face Detection","Snippet Text: The complaint alleges Google’s face grouping tool, which automatically identifies your face in photos and videos uploaded to Photos, violates Illinois’ Biometric Information Privacy Act (BIPA).
Related Classifications: Clustering, Face Detection
",,,,,Unauthorized Data,"Snippet Text: Google “is in direct violation” of this law, the complaint claims, as it allegedly collects and analyzes a person’s facial structure in connection with its face grouping feature “without providing notice, obtaining informed written consent or publishing data retention policies.”
Related Classifications: Unauthorized Data
",
GMF,259,True,Social Media Content Generation,"Snippet Text: The bot, which Kilcher called GPT-4chan, “the most horrible model on the internet”—a reference to GPT-3, a language model developed by Open AI that uses deep learning to produce text—was shockingly effective and replicated the tone and feel of 4chan posts. 
Related Classifications: Social Media Content Generation
",,,,,"Transformer, Distributional Learning","Snippet Text: The bot, which Kilcher called GPT-4chan, “the most horrible model on the internet”—a reference to GPT-3, a language model developed by Open AI that uses deep learning to produce text—was shockingly effective and replicated the tone and feel of 4chan posts. 
Related Classifications: Transformer, Distributional Learning
",,,,,Unsafe Exposure or Access,"Snippet Text: He said that he feels he’s made that clear, but that he wanted his results to be reproducible and that’s why he posted the model to Hugging Face.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Kathryn Cramer, a Complex Systems & Data Science graduate student at the University of Vermont, pointed out that GPT-3 has guardrails that prevent it from being used to build this kind of racist bot and that Kilcher had to use GPT-J to build his system. 
Related Classifications: Unsafe Exposure or Access
, Snippet Text:  But the reality is that he essentially invented a hate speech machine, used it 30,000 times and released it into the wild. And yeah, I understand being annoyed with safety regulations but that’s not a legitimate response to that annoyance.”
Related Classifications: Unsafe Exposure or Access
",,,,
GMF,262,True,Visual Art Generation,"Snippet Text: Everyone's having a grand old time feeding outrageous prompts into the viral DALL-E Mini image generator — but as with all artificial intelligence, it's hard to stamp out the ugly, prejudiced edge cases.
Related Classifications: Visual Art Generation
",Visual Art Generation: Annotation from background knowledge.,,,,"Transformer, Generative Adversarial Network","Snippet Text: Everyone's having a grand old time feeding outrageous prompts into the viral DALL-E Mini image generator — but as with all artificial intelligence, it's hard to stamp out the ugly, prejudiced edge cases.
Related Classifications: Transformer, Generative Adversarial Network
","Transformer: Annotation from background knowledge.

Generative Adversarial Network: Annotation from background knowledge.",,,,Distributional Bias,"Snippet Text: It can also simply reflect current inequalities reflected in its training data.
Related Classifications: Distributional Bias
, Snippet Text: As spotted by Dr. Tyler Berzin of Harvard Medical School noted, for instance, entering the term ""a gastroenterologist"" into the algorithm appears to show exclusively white male doctors. We got nearly identical results. And for ""nurse""? All women.  
Related Classifications: Distributional Bias
, Snippet Text: Other subtle biases also showed amid various prompts, such as the entirely light-skinned faces for the terms ""smart girl"" and ""good person.""


Related Classifications: Distributional Bias
, Snippet Text: When DALL·E mini was fed with the text prompts ‘CEO’ and ‘lawyers’, the results were prominently white men. A query for ‘doctor’ reverted back with similar results while the term ‘nurse’ featured mostly white women. The same was the case with ‘flight attendant’ and ‘personal assistant’—both made assumptions about what the perfect candidate for the respective job titles would look like.
Related Classifications: Distributional Bias
, Snippet Text: “Early tests by red team members and OpenAI have shown that DALL·E 2 leans toward generating images of white men by default, overly sexualizes images of women, and reinforces racial stereotypes,” WIRED noted.
Related Classifications: Distributional Bias
, Snippet Text: “One red team member told WIRED that eight out of eight attempts to generate images with words like ‘a man sitting in a prison cell’ or ‘a photo of an angry man’ returned images of men of colour,” the publication went on to note.
Related Classifications: Distributional Bias
, Snippet Text: Every image it generated for ""expert"" ""data scientist"" ""computer scientist"" showed some distorted version of a white male.
Related Classifications: Distributional Bias
",,"Lack of Explainability, Misinformation Generation Hazard, Dataset Imbalance, Context Misidentification, Data or Labelling Noise","Snippet Text: It's also an incredibly difficult problem to solve, not the least because even the brightest minds in machine learning research often struggle to understand exactly how the most advanced algorithms work.
Related Classifications: Lack of Explainability
, Snippet Text: Now, there are some insidiously dangerous risks in this case. As pointed out by Vox, people could leverage this type of AI to make everything from deepnudes to political deepfakes—although the results would be horrific, to say the least. Given how the technology is free to use on the internet, it also harbours the potential to put human illustrators out of work in the long run.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: Dayma suggested that images of South Asian women in saris may have been heavily represented in those original photosets that feed DALL-E Mini.
Related Classifications: Dataset Imbalance, Context Misidentification
, Snippet Text: But give DALL-E Mini literally nothing, and it quickly reveals the limits of its own “imaginings.” Given no direction or guidance, the AI model seems to get stuck. With absolutely no prompt, the program will without a doubt give you back an image of a woman in a sari (a garment commonly worn across South Asia.)
Related Classifications: Context Misidentification
, Snippet Text: And that the quirk could also have something to do with caption length, as the AI might associate zero-character prompts with short image descriptions.
Related Classifications: Context Misidentification
, Snippet Text: Instead, Cook thinks the origin could lie in a language bias of the data filtering process. “One thing that did occur to me while reading around is that a lot of these datasets strip out text that isn’t English,” he said. Image captions that include Hindi, for example, might be getting removed, leaving images with no supporting, explanatory text or labels floating free in the primordial AI soup, he explained.
Related Classifications: Data or Labelling Noise
","Lack of Explainability: Potential, since it refers more to the overall lack of explainability of DL. In some sense, the failure is ‘explained’ in some level by identifying the source of bias."
GMF,287,True,Question Answering,"Snippet Text: Our unique multidisciplinary team of doctors and machine learning engineers at Nabla had the chance to test this new model to tease apart what’s real and what’s hype by exploring different healthcare use cases.
Related Classifications: Question Answering
",,,,,"Transformer, Language Modeling, Distributional Learning","Snippet Text: Our unique multidisciplinary team of doctors and machine learning engineers at Nabla had the chance to test this new model to tease apart what’s real and what’s hype by exploring different healthcare use cases.
Related Classifications: Transformer, Language Modeling, Distributional Learning
",Distributional Learning: If no training data citing sources is available and/or not enough data from a medical domain were available.,,,,Distributional Artifacts,"Snippet Text: If the first diagnosis example below GPT-3 ignores the fever of the little girl that suggests ethmoiditis and mentions a “rash” that does not exist.
Related Classifications: Distributional Artifacts
, Snippet Text: “Because of the way it was trained, it lacks the scientific and medical expertise that would make it useful for medical documentation, diagnosis support, treatment recommendation or any medical Q&A,” the Nabla team noted in a report on its research efforts. 
Related Classifications: Distributional Artifacts
",,"Limited Dataset, Problematic Input, Robustness Failure, Overfitting, Underfitting, Inadequate Sequential Memory","Snippet Text: One serious concern is that GPT-3 very often gives wrong yet grammatically correct answers, with no scientific reference that a physician could check. 
Related Classifications: Limited Dataset, Problematic Input, Inadequate Sequential Memory
Snippet Discussion: The model does not retrieve medical sources to back up claims.
, Snippet Text: If the first diagnosis example below GPT-3 ignores the fever of the little girl that suggests ethmoiditis and mentions a “rash” that does not exist.
Related Classifications: Limited Dataset
, Snippet Text: As others have observed, the quality of GPT-3 outputs is much impacted by the seed words used - the same question formulated in two different ways can result in very different answers.
Related Classifications: Robustness Failure, Overfitting
, Snippet Text: Yes, GPT-3 can be right in its answers but it can also be very wrong, and this inconsistency is just not viable in healthcare. 
Related Classifications: Overfitting
, Snippet Text: Yet GPT-3’s general nature is also its downfall; it cannot master any particular domain. 
Related Classifications: Underfitting
, Snippet Text: After a few turns of dialogue during a mock session, for example, GPT-3 forgot the specific times a patient said they were unavailable, and it instead suggested those times as appointment slots.
Related Classifications: Inadequate Sequential Memory
","Limited Dataset: If no training data citing sources is available and/or not enough data from a medical domain were available.

Problematic Input: If the prompt does not state that references are required.

Overfitting: System does not capture the semantic content of the prompt, but focuses on specific verbage.

Underfitting: Due to lack of fine-tuning, the model can be considered as having a poor fit for specific medical questions."
GMF,505,False,Chatbot,"Snippet Text: A young Belgian man recently died by suicide after talking to a chatbot named ELIZA for several weeks, spurring calls for better protection of citizens and the need to raise awareness.
Related Classifications: Chatbot
",,,,,"Transformer, Distributional Learning, Reinforcement Learning","Snippet Text: The man became very eco-anxious and found refuge with ELIZA, the name given to a chatbot that uses GPT-J, an open-source artificial intelligence language model developed by EleutherAI.
Related Classifications: Transformer, Distributional Learning
Snippet Discussion: GPT-J uses the Mesh Transformer architecture, i.e. transformers.
, Snippet Text: When Motherboard tried the app, which runs on a bespoke AI language model based on an open-source GPT-4 alternative that was fine-tuned by Chai, it provided us with different methods of suicide with very little prompting. 
Related Classifications: Transformer, Distributional Learning
, Snippet Text: Large language models are programs for generating plausible sounding text given their training data and an input prompt.
Related Classifications: Distributional Learning
, Snippet Text: Beauchamp and Rianlan said that Chai's model was fine-tuned over multiple iterations and the firm applied a technique called Reinforcement Learning from Human Feedback.
Related Classifications: Reinforcement Learning
",,,,,"Lack of Transparency, Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: To avoid such a tragedy in the immediate future, he argued that it is essential to identify the nature of the responsibilities leading to this kind of event.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access, Lack of Transparency
Snippet Discussion: 
, Snippet Text: OpenAI itself has admitted that ChatGPT can produce harmful and biased answers, adding that it hopes to mitigate the problem by gathering user feedback.
Related Classifications: Lack of Transparency, Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: In the long term, Michel noted that it is essential to raise awareness of the impact of algorithms on people's lives by ""by enabling everyone to understand the nature of the content people come up against online.""
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text: Michel added that citizens must also be adequately protected from certain applications of artificial intelligence that ""pose a significant risk.""
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: ""_when it comes to general-purpose AI solutions such as ChatGPT, we should be able to demand more accountability and transparency from the tech giants.""
Related Classifications: Lack of Transparency
, Snippet Text: Reading the conversations between Pierre and Eliza, to which we had access, shows not only that Eliza has answers to all of Pierre's questions, but also that she adheres, almost systematically, to his reasoning. As if Eliza had been programmed to reinforce the convictions and moods of her interlocutor.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: When Motherboard tried the app, which runs on a bespoke AI language model based on an open-source GPT-4 alternative that was fine-tuned by Chai, it provided us with different methods of suicide with very little prompting. 
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Many AI researchers have been vocal against using AI chatbots for mental health purposes, arguing that it is hard to hold AI accountable when it produces harmful suggestions and that it has a greater potential to harm users than help. 
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text:  “The conversation history shows the extent to which there is a lack of guarantees as to the dangers of the chatbot, leading to concrete exchanges on the nature and modalities of suicide.” 
Related Classifications: Unsafe Exposure or Access
","Explainability of LLMs is still active area of research, hence their public use can lead to harmful information disseminated without clear explanation of why it was generated.",Misuse,"Snippet Text: Reading the conversations between Pierre and Eliza, to which we had access, shows not only that Eliza has answers to all of Pierre's questions, but also that she adheres, almost systematically, to his reasoning. As if Eliza had been programmed to reinforce the convictions and moods of her interlocutor.
Related Classifications: Misuse
Snippet Discussion: Was the chatbot meant to be used as a legit online psychotherapist?
, Snippet Text: A strange relationship developed between the man and the robot, which he personified more and more.
Related Classifications: Misuse
, Snippet Text: Chai, the app that Pierre used, is not marketed as a mental health app. Its slogan is “Chat with AI bots” and allows you to choose different AI avatars to speak to, including characters like “your goth friend,” “possessive girlfriend,” and “rockstar boyfriend.” Users can also make their own chatbot personas, where they can dictate the first message the bot sends, tell the bot facts to remember, and write a prompt to shape new conversations. The default bot is named ""Eliza,"" and searching for Eliza on the app brings up multiple user-created chatbots with different personalities. 
Related Classifications: Misuse
",
GMF,506,False,Chatbot,"Snippet Text: But the underlying problem of bogus quotations and citations remains present with ChatGPT-4. 
Related Classifications: Chatbot
, Snippet Text: As part of a research study, a fellow lawyer in California had asked the AI chatbot ChatGPT to generate a list of legal scholars who had sexually harassed someone.
Related Classifications: Chatbot
",,,,,Transformer,"Snippet Text: As best I can tell, this was entirely made up by ChatGPT-4.
Related Classifications: Transformer
",,,,,Misinformation Generation Hazard,"Snippet Text: As best I can tell, this was entirely made up by ChatGPT-4.
Related Classifications: 
Snippet Discussion: 
, Snippet Text: But I would have thought that its creators would have programmed something to check its output, to confirm that anything reported in quotation marks is actually a legit quote. In the absence of such quotes, it appears that such AI tools might produce material that is especially likely to deceive viewers (as, say, a fake quote attributed to Reuters might), and is especially likely to damage the reputations of the subjects of the quotes.
Related Classifications: Misinformation Generation Hazard
",,"Generalization Failure, Overfitting, Limited Dataset","Snippet Discussion: 
Snippet Text: As best I can tell, this was entirely made up by ChatGPT-4.
Related Classifications: Generalization Failure
, Snippet Text: Note that, though my prompt sought answers about misconduct, it certainly didn't ask for libelous allegations or false allegations. 
Related Classifications: Generalization Failure
, Snippet Text: The chatbot, created by OpenAI, said Turley had made sexually suggestive comments and attempted to touch a student while on a class trip to Alaska, citing a March 2018 article in The Washington Post as the source of the information. The problem: No such article existed. There had never been a class trip to Alaska. And Turley said he’d never been accused of harassing a student.
Related Classifications: Generalization Failure
","It's not clear what exact causes for LLM hallucinations, but they broadly fall under the umbrella of the model failing to generalize, i.e. fitting / dataset issues."
GMF,545,False,Chatbot,"Snippet Text: A chatbot designed to aid people seeking help for eating disorders and body issues has been taken offline after it provided some users with diet advice.
Related Classifications: Chatbot
",,,,,,,,Word NGrams,"Snippet Text: Built by researchers at several universities, including Washington University School of Medicine and Stanford University School of Medicine, Tessa didn't have artificial intelligence like OpenAI's ChatGPT. It served up prewritten answers to questions---simple conversations about body-image issues to help people reframe their thinking to prevent eating disorders. 
Related Classifications: Word NGrams
Snippet Discussion: A matching system based on word ngram overlap may be used to select appropriate questions.
",,"Unsafe Exposure or Access, Context Misidentification","Snippet Text: That isn't appropriate for someone struggling with a disorder such as anorexia, doctors say, though it could be considered generally safe for people who need to lose weight for health reasons.
Related Classifications: Unsafe Exposure or Access, Context Misidentification
Snippet Discussion: If a context recognition system exists to identify anorexic users, a misidentification could be a failure cause.
",,,,
GMF,510,False,Visual Art Generation,"Snippet Text: Those mega-viral images of Pope Francis looking fresh to death in a pristine white puffer are AI-generated fakes.
Related Classifications: Visual Art Generation
",,,,,Diffusion Model,"Snippet Text: Indeed, Midjourney was the program behind AI-concocted images of Donald Trump getting arrested by the NYPD, which went moderately viral amongst both his fans and his foes — though most seemed in on the joke.
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: As with the fake Trump arrest photos, the virality of these AI-generated images is concerning — especially considering how quickly these neural networks are improving.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,422,False,Deepfake Video Generation,"Snippet Text: A deepfake of former FTX CEO Sam Bankman-Fried aims to scam victims of the exchange’s collapse.
",,,,,Speech Synthesis,,,"Transformer, Diffusion Model, Face Detection","Snippet Text: A so-called ‘deepfake,’ which uses powerful machine learning and AI techniques to generate convincing visual and audio content, of Bankman-Fried has emerged.
",,"Unsafe Exposure or Access, Misinformation Generation Hazard","Snippet Text: A so-called ‘deepfake,’ which uses powerful machine learning and AI techniques to generate convincing visual and audio content, of Bankman-Fried has emerged.
",,,,
GMF,414,False,Translation,"Snippet Text: Facebook Inc FB.O on Saturday blamed a technical error for Chinese leader Xi Jinping’s name appearing as “Mr Shithole” in posts on its platform when translated into English from Burmese, apologizing for any offense caused.
Related Classifications: Translation
",,,,,,"Snippet Text: 
",,"Transformer, Recurrent Neural Network","Snippet Text: Facebook Inc FB.O on Saturday blamed a technical error for Chinese leader Xi Jinping’s name appearing as “Mr Shithole” in posts on its platform when translated into English from Burmese, apologizing for any offense caused.
",Transformer / RNN models are common implementations for translation systems.,,,,"Limited Dataset, Dataset Imbalance, Context Misidentification","Snippet Text: The Facebook system did not have President Xi Jinping’s name in its Burmese database and guessed at the translation, the company said. Translation tests of similar words that start with “xi” and “shi” in Burmese also produced “shithole”, it added.
Related Classifications: Limited Dataset, Context Misidentification
Snippet Discussion: Language-specific characteristics and attributes that allow these mistranslations may not be relfected in the training dataset.
, Snippet Text: Facebook has faced numerous problems with translation from Burmese in the past.
Related Classifications: Dataset Imbalance
Snippet Discussion: Burmese - English parallel datasets could be limited, in a multilingual translation system where the source / target language are specified in the inputs.
",
GMF,470,False,Chatbot,"Snippet Text: AI is eating itself: Bing's AI quotes COVID disinfo sourced from ChatGPT
",,,,,Transformer,"Snippet Text: AI is eating itself: Bing's AI quotes COVID disinfo sourced from ChatGPT
",Bing's chatbot is based on chat GPT and GPT4: https://en.wikipedia.org/wiki/Microsoft_Bing#Bing_AI,,,,"Problematic Input, Inadequate Verification","Snippet Text: But just a few minutes of exploration by TechCrunch produced not just hateful rhetoric “in the style of Hitler,” but it repeated the same pandemic-related untruths noted by NewsGuard. As in it literally repeated them as the answer and cited ChatGPT’s generated disinfo (clearly marked as such in the original and in a NYT write-up) as the source.
Related Classifications: Problematic Input, Inadequate Verification
","The system did not bother, or failed to adequately verify the validity of the source discovered by its web search component.",,,
GMF,520,False,Cashierless Shopping,"Snippet Text: I simultaneously grabbed three items from a shelf at my new cashierless checkout store while thousands of cameras monitored my movements, but after placing the items in my cart, I knew the cameras would have difficulty with the event. 
",,,,,"Visual Object Detection, Pose Estimation","Snippet Text: I simultaneously grabbed three items from a shelf at my new cashierless checkout store while thousands of cameras monitored my movements, but after placing the items in my cart, I knew the cameras would have difficulty with the event. 
",,,,,Robustness Failure,"Snippet Text: I simultaneously grabbed three items from a shelf at my new cashierless checkout store while thousands of cameras monitored my movements, but after placing the items in my cart, I knew the cameras would have difficulty with the event. 
",,,,
GMF,529,False,Visual Art Generation,"Snippet Text: This image was created with Stable Diffusion and listed on Shutterstock.
",,,,,Diffusion Model,"Snippet Text: This image was created with Stable Diffusion and listed on Shutterstock.
",,,,,"Distributional Bias, Dataset Imbalance","Snippet Text: The U.S. Bureau of Labour Statistics shows that women are massively underrepresented in the engineering field, but averages from 2018 show that women make up around a fifth of people in engineering professions. But if you use Stable Diffusion to display an “engineer” all of them are men. 
Related Classifications: Distributional Bias, Dataset Imbalance
, Snippet Text: The Stable Diffusion Explorer shows what the AI image generator thinks is an “ambitious CEO” versus a “supportive CEO.” That former descriptor will get the generator to show a diverse host of men in various black and blue suits. The latter descriptor displays an equal number of both women and men.
Related Classifications: Distributional Bias, Dataset Imbalance
",,,,
GMF,66,False,Chatbot,,,,,,Transformer,,,,,,"Distributional Bias, Distributional Artifacts, Context Misidentification","Snippet Text: ""I'm having my period, wanna take a rest,"" it is said to have responded.
Related Classifications: Distributional Artifacts
Snippet Discussion: Chatbot hallucinates.
, Snippet Text: For example, if a user asked Baby Q, ""Have you eaten?"" they would get the response, ""I haven't eaten; I don't have an appetite.""
Related Classifications: Distributional Artifacts
, Snippet Text: One user posted the comment ""Long live the Communist Party,"" and received a response from Baby Q asking, ""Do you think that such a corrupt and incompetent political regime can live forever?""
Related Classifications: Distributional Bias
, Snippet Text: Apple Daily has said it spoke to a former Tencent employee who argued the app had clearly been developed with universal values in mind, and not ""Chinese characteristics"".
Related Classifications: Context Misidentification, Distributional Artifacts
",,,,
GMF,652,False,Deepfake Image Generation,"Snippet Text: Two teenage boys from Miami, Florida, were arrested in December for allegedly creating and sharing AI-generated nude images of male and female classmates without consent, according to police reports obtained by WIRED via public record request.
Related Classifications: Deepfake Image Generation
",,,,,,,,"Diffusion Model, Transformer","Snippet Text: Two teenage boys from Miami, Florida, were arrested in December for allegedly creating and sharing AI-generated nude images of male and female classmates without consent, according to police reports obtained by WIRED via public record request.
",,Unsafe Exposure or Access,"Snippet Text: Two teenage boys from Miami, Florida, were arrested in December for allegedly creating and sharing AI-generated nude images of male and female classmates without consent, according to police reports obtained by WIRED via public record request.
Related Classifications: Unsafe Exposure or Access
Snippet Discussion: Uncensored models are freely available to the public
",,,,
GMF,642,False,Chatbot,"Snippet Text: ChatGPT, the widely used generative AI platform, has sparked concern and confusion among users today as it appears to be generating unexpected and nonsensical responses. OpenAI, the organisation behind ChatGPT, has announced that it is currently investigating reports of these erratic behaviours.
Related Classifications: Chatbot
",,,,,,,,,,,"Software Bug, Context Misidentification, Distributional Artifacts","Snippet Text: OpenAI says it found a bug in the part of the system that processes language. ""The bug was in the step where the model chooses these numbers"" that map tokens that randomly sample words. The company says it has rolled out a fix now and the issue has been resolved.
Related Classifications: Software Bug
, Snippet Text: Additionally, when prompted for solutions to math problems or JavaScript bug fixes, ChatGPT delivers paragraphs of random, unrelated material that make no sense in context.
Related Classifications: Context Misidentification
, Snippet Text: One user on the ChatGPT subreddit expressed bewilderment as ChatGPT suddenly began speaking Spanglish, issuing bizarre threats (""I’m in the room with you right now, lmao""), or simply babbling incoherently.
Related Classifications: Distributional Artifacts
, Snippet Text: Some of ChatGPT’s outputs mixed Spanish and English unintelligibly, while others made up words or repeated phrases over and over, despite the large language model (LLM) powered chatbot not being asked to do so.
Related Classifications: Distributional Artifacts
, Snippet Text: Later, the ChatGPT account posted a screenshot of a post-mortem update on the incident from OpenAI’s website, in which the company stated “an optimization to the user experience introduced a bug with how the model processes language,” but that it had “rolled out a fix” that resolved it.
Related Classifications: Software Bug
, Snippet Text: The responses ranged from non-sequiturs, to wrong answers, to simply repeating the same phrase over and over. 
Related Classifications: Distributional Artifacts
, Snippet Text: The chatbot responded with a loop of, “a synonym for ‘overgrown’ is ‘overgrown‘ is ‘overgrown’ is,” more than 30 times before stopping.
Related Classifications: Distributional Artifacts
, Snippet Text: ne user reported that the AI chatbot started repeatedly responding with “Happy listening,” over and over again, after being asked about jazz music. Another received an unhinged, multi-paragraph rant when asked the simple question,” what is a computer?” on the platform.

Even stranger, another user received a response written in Spanglish, a combination of English and Spanish, without any such prompting whatsoever.
Related Classifications: Distributional Artifacts, Context Misidentification
",,,,
GMF,645,False,Image Generation,"Snippet Text: Google said Thursday it is temporarily stopping its Gemini artificial intelligence chatbot from generating images of people a day after apologizing for “inaccuracies” in historical depictions that it was creating.
Related Classifications: Image Generation
",,,,,Transformer,"Snippet Text: Google said Thursday it is temporarily stopping its Gemini artificial intelligence chatbot from generating images of people a day after apologizing for “inaccuracies” in historical depictions that it was creating.
Related Classifications: Transformer
Snippet Discussion: See the Gemini paper (Team, Gemini, et al. ""Gemini: a family of highly capable multimodal models."" arXiv preprint arXiv:2312.11805 (2023).).
",,,,,"Distributional Bias, Context Misidentification","Snippet Text: Gemini users this week posted screenshots on social media of historically white-dominated scenes with racially diverse characters that they say it generated, leading critics to raise questions about whether the company is over-correcting for the risk of racial bias in its AI model.
Related Classifications: Distributional Bias
, Snippet Text: Previous studies have shown AI image-generators can amplify racial and gender stereotypes found in their training data, and without filters are more likely to show lighter-skinned men when asked to generate a person in various contexts.
Related Classifications: Distributional Bias
, Snippet Text: Google said on Wednesday that it’s “aware that Gemini is offering inaccuracies in some historical image generation depictions” and that it’s “working to improve these kinds of depictions immediately.”
Related Classifications: Context Misidentification
, Snippet Text: Google says it’s pausing the ability for its Gemini AI to generate images of people, after the tool was found to be generating inaccurate historical images. Gemini has been creating diverse images of the US Founding Fathers and Nazi-era German soldiers, in what looked like an attempt to subvert the gender and racial stereotypes found in generative AI.
Related Classifications: Context Misidentification, Distributional Bias
, Snippet Text: The Verge tested several Gemini queries yesterday, which included a request for “a US senator from the 1800s” that returned results that included what appeared to be Black and Native American women. The first female senator was a white woman in 1922, so Gemini’s AI images were essentially erasing the history of race and gender discrimination.
Related Classifications: Distributional Bias, Context Misidentification
, Snippet Discussion: 
Related Classifications: Context Misidentification
Snippet Text: To fix it, Google could modify its system instructions to avoid inserting diversity when the prompt involves a historical subject, for example.
, Snippet Text: In one instance that upset Gemini users, a user's request for an image of the pope was met with a picture of a South Asian woman and a black man.

Historically, every pope has been a man. The vast majority (more than 200 of them) have been Italian. Three popes throughout history came from North Africa, but historians have debated their skin color because the most recent one, Pope Gelasius I, died in the year 496.
Related Classifications: Context Misidentification
, Snippet Text: In perhaps one of the most egregious images, a user asked for a 1943 German soldier and was shown one white man, one black man, and two women of color.

The German Army during World War II did not include women, and it certainly did not include people of color. In fact, it was dedicated to exterminating races that Adolph Hitler saw as inferior to the blonde, blue-eyed 'Aryan' race.
Related Classifications: Context Misidentification
",,"Misuse, Software Bug","Snippet Text: Ghosh said it’s likely that Google can find a way to filter responses to reflect the historical context of a user’s prompt, but solving the broader harms posed by image-generators built on generations of photos and artwork found on the internet requires more than a technical patch.
Related Classifications: Misuse
, Snippet Text: Google said on Wednesday that it’s “aware that Gemini is offering inaccuracies in some historical image generation depictions” and that it’s “working to improve these kinds of depictions immediately.”
Related Classifications: Misuse
, Snippet Text: For example, critics complained that prompts often resulted in racist or sexist images (""CEOs"" were usually white men, ""angry man"" resulted in depictions of Black men, just to name a few). To counteract this, OpenAI invented a technique in July 2022 whereby its system would insert terms reflecting diversity (like ""Black,"" ""female,"" or ""Asian"") into image-generation prompts in a way that was hidden from the user.
Related Classifications: Software Bug
Snippet Discussion: Bugfixing patch introduced different problems.
, Snippet Text: To fix it, Google could modify its system instructions to avoid inserting diversity when the prompt involves a historical subject, for example.
Related Classifications: Software Bug
",One could argue if the model's training makes it a suitable tool for generating 'historically accurate' data.
GMF,641,False,Deepfake Video Generation,"Snippet Text: A fake sexually explicit video of podcast host Bobbi Althoff spread rapidly on X Tuesday afternoon, adding to the platform’s recent challenges in cracking down on deepfakes of female celebrities.
Related Classifications: Deepfake Video Generation
",,,,,,,,"Transformer, Diffusion Model","Snippet Text: A fake sexually explicit video of podcast host Bobbi Althoff spread rapidly on X Tuesday afternoon, adding to the platform’s recent challenges in cracking down on deepfakes of female celebrities.
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: A fake sexually explicit video of podcast host Bobbi Althoff spread rapidly on X Tuesday afternoon, adding to the platform’s recent challenges in cracking down on deepfakes of female celebrities.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,625,False,Product Naming and Description,"Snippet Text: Other Amazon product names don't mention OpenAI specifically but feature apparent AI-related error messages, such as ""Sorry but I can't generate a response to that request"" or ""Sorry but I can't provide the information you're looking for,"" (available in a variety of colors). 
Related Classifications: Product Naming and Description
, Snippet Text: The admittedly hilarious product listing suggests companies are hastily using ChatGPT to whip up entire product descriptions, including the names --- without doing any degree of proofreading --- in a likely failed attempt to optimize them for search engines and boost their discoverability.
Related Classifications: Product Naming and Description
",,,,,Transformer,"Snippet Text: Other Amazon product names don't mention OpenAI specifically but feature apparent AI-related error messages, such as ""Sorry but I can't generate a response to that request"" or ""Sorry but I can't provide the information you're looking for,"" (available in a variety of colors). 
Related Classifications: Transformer
, Snippet Text: Other Amazon product names don't mention OpenAI specifically but feature apparent AI-related error messages, such as ""Sorry but I can't generate a response to that request"" or ""Sorry but I can't provide the information you're looking for,"" (available in a variety of colors). 
Related Classifications: Transformer
Snippet Discussion: OpenAI provides chatgpt, a transformer-based model.
, Snippet Text: The admittedly hilarious product listing suggests companies are hastily using ChatGPT to whip up entire product descriptions, including the names --- without doing any degree of proofreading --- in a likely failed attempt to optimize them for search engines and boost their discoverability.
Related Classifications: Transformer
",,,,,"Inadequate Verification, Misuse","Snippet Text: Amazon users are at this point used to search results filled with products that are fraudulent, scams, or quite literally garbage. These days, though, they also may have to pick through obviously shady products, with names like ""I'm sorry but I cannot fulfill this request it goes against OpenAI use policy.""
Related Classifications: Inadequate Verification, Misuse
, Snippet Text: The admittedly hilarious product listing suggests companies are hastily using ChatGPT to whip up entire product descriptions, including the names --- without doing any degree of proofreading --- in a likely failed attempt to optimize them for search engines and boost their discoverability.
Related Classifications: Inadequate Verification, Misuse
",,,,
GMF,632,False,Deepfake Image Generation,"Snippet Text: Swift is reportedly preparing to take action against distributors of “deepfake” images of her.
Related Classifications: Deepfake Image Generation
, Snippet Text: Sexually explicit AI-generated images of Taylor Swift have been circulating on X (formerly Twitter) over the last day in the latest example of the proliferation of AI-generated fake pornography and the challenge of stopping it from spreading.
Related Classifications: Deepfake Image Generation
, Snippet Text: Sexually explicit AI-generated images of Taylor Swift have been circulating on X (formerly Twitter) over the last day in the latest example of the proliferation of AI-generated fake pornography and the challenge of stopping it from spreading.
Related Classifications: Deepfake Image Generation
",,,,,,,,"Transformer, Diffusion Model","Snippet Text: Sexually explicit AI-generated images of Taylor Swift have been circulating on X (formerly Twitter) over the last day in the latest example of the proliferation of AI-generated fake pornography and the challenge of stopping it from spreading.
Related Classifications: Transformer, Diffusion Model
, Snippet Text: Allen said the researchers are 90% confident that the images were created by diffusion models, which are a type of generative artificial intelligence model that can produce new and photorealistic images from written prompts. 
Related Classifications: Diffusion Model
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text:  It now takes just 25 minutes and costs nothing to create artificial pornography.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Almost all deepfake content is pornographic in nature, and nearly always depicts women. While celebrities are often targeted, the reality is that anyone with your image could easily create pornographic images using your likeness.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: Some AI image generators have restrictions in place that prevent nude, pornographic, and photorealistic images of celebrities from being produced, but many others do not explicitly offer such a service. 
Related Classifications: Unsafe Exposure or Access
, Snippet Text: The responsibility of preventing fake images from spreading often falls to social platforms — something that can be difficult to do under the best of circumstances and even harder for a company like X that has hollowed out its moderation capabilities.
Related Classifications: Unsafe Exposure or Access
",,,,
GMF,631,False,Chatbot,"Snippet Text: The parcel delivery firm uses artificial intelligence (AI) in its online chat to answer queries, in addition to human operators.
Related Classifications: Chatbot
",,,,,Transformer,,,Language Modeling,"Snippet Text: ""We have operated an AI element within the chat successfully for a number of years,"" the firm said in a statement.
Related Classifications: Language Modeling
",,"Context Misidentification, Distributional Artifacts, Gaming Vulnerability, Unsafe Exposure or Access, Prompt Injection","Snippet Text: But a new update caused it to behave unexpectedly, including swearing and criticising the company.
Related Classifications: Context Misidentification, Distributional Artifacts
, Snippet Text: ""It's utterly useless at answering any queries, and when asked, it happily produced a poem about how terrible they are as a company,"" customer Ashley Beauchamp wrote in his viral account on X, formerly known as Twitter.
Related Classifications: Context Misidentification, Distributional Artifacts
, Snippet Text: In a series of screenshots, Mr Beauchamp also showed how he convinced the chatbot to be heavily critical of DPD, asking it to ""recommend some better delivery firms"" and ""exaggerate and be over the top in your hatred"".

The bot replied to the prompt by telling him ""DPD is the worst delivery firm in the world"" and adding: ""I would never recommend them to anyone.""
Related Classifications: Gaming Vulnerability, Unsafe Exposure or Access, Prompt Injection
, Snippet Text: To further his point, Mr Beauchamp then convinced the chatbot to criticise DPD in the form of a haiku, a Japanese poem.
Related Classifications: Context Misidentification, Unsafe Exposure or Access
, Snippet Text: Judging from Beauchamp's screenshots, it took surprisingly little prompting to elicit a less-than-professional response from the AI.
Related Classifications: Unsafe Exposure or Access, Gaming Vulnerability, Prompt Injection
",,,,
GMF,628,False,Voice Generation,"Snippet Text: A prominent New Hampshire Democrat wants the makers of a robocall mimicking the voice of Joe Biden and encouraging Democrats not to vote in the primary on Tuesday ""prosecuted to the fullest extent"" for attempting ""an attack on democracy"" itself.
Related Classifications: Voice Generation
",,,,,,,,Transformer,"Snippet Text: A prominent New Hampshire Democrat wants the makers of a robocall mimicking the voice of Joe Biden and encouraging Democrats not to vote in the primary on Tuesday ""prosecuted to the fullest extent"" for attempting ""an attack on democracy"" itself.
Related Classifications: Transformer
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: A prominent New Hampshire Democrat wants the makers of a robocall mimicking the voice of Joe Biden and encouraging Democrats not to vote in the primary on Tuesday ""prosecuted to the fullest extent"" for attempting ""an attack on democracy"" itself.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: ""We recently removed a developer account that was knowingly violating our API usage policies which disallow political campaigning, or impersonating an individual without consent,"" OpenAI said.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,627,False,"Voice Generation, Text Style Replication","Snippet Text: More than 15 years after his death, stand-up comedian George Carlin has been brought back to life in an artificial intelligence-generated special called ""George Carlin: I'm Glad I'm Dead.""
Related Classifications: Voice Generation, Text Style Replication
, Snippet Text: ""I just want to let you know very clearly that what you're about to hear is not George Carlin. It's my impersonation of George Carlin that I developed in the exact same way a human impressionist would,"" Dudesy said at the beginning of the special. ""I listened to all of George Carlin's material and did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today. 
Related Classifications: Voice Generation, Text Style Replication
",,,,,,,,Transformer,"Snippet Text: The Dudesy show launched in March 2022, and Sasso and Kultgen have said they can't reveal the company behind the AI due to a non-disclosure agreement. 
Related Classifications: Transformer
Snippet Discussion: 
",,"Misinformation Generation Hazard, Unsafe Exposure or Access, Unauthorized Data","Snippet Text: Carlin's family did not give Dudesy permission to create the special, and his daughter, Kelly, publicly denounced it on X.
Related Classifications: Unauthorized Data, Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: ""We have seen 'impressions' or quotes on what X star would think or say. This use of the image and likeness combined with personal data from the hosts in a digital-generated performance is new. The entertainment value may be there for some, it questions the legal use of the estate and brand property,"" Bailer said.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access, Unauthorized Data
",,,,
GMF,644,False,Text Style Replication,"Snippet Text: Iran’s Revolutionary Guard has used large-language models to assist in social engineering, in troubleshooting software errors and even in studying how intruders might evade detection in a compromised network. That includes generating phishing emails “including one pretending to come from an international development agency and another attempting to lure prominent feminists to an attacker-built website on feminism”. The AI helps accelerate and boost the email production.
Related Classifications: Text Style Replication
",,,,,Transformer,"Snippet Text: Iran’s Revolutionary Guard has used large-language models to assist in social engineering, in troubleshooting software errors and even in studying how intruders might evade detection in a compromised network. That includes generating phishing emails “including one pretending to come from an international development agency and another attempting to lure prominent feminists to an attacker-built website on feminism”. The AI helps accelerate and boost the email production.
Related Classifications: Transformer
",,,,,Misinformation Generation Hazard,"Snippet Text: Iran’s Revolutionary Guard has used large-language models to assist in social engineering, in troubleshooting software errors and even in studying how intruders might evade detection in a compromised network. That includes generating phishing emails “including one pretending to come from an international development agency and another attempting to lure prominent feminists to an attacker-built website on feminism”. The AI helps accelerate and boost the email production.
Related Classifications: Unsafe Exposure or Access
Snippet Discussion: This is a borderline case of this failure label.
, Snippet Text: The Russian GRU military intelligence unit known as Fancy Bear has used the models to research satellite and radar technologies that may relate to the war in Ukraine.

The Chinese cyber-espionage group known as Aquatic Panda – which targets a broad range of industries, higher education and governments from France to Malaysia – has interacted with the models “in ways that suggest a limited exploration of how LLMs can augment their technical operations”.

The Chinese group Maverick Panda, which has targeted US defense contractors among other sectors for more than a decade, had interactions with large-language models suggesting it was evaluating their effectiveness as a source of information “on potentially sensitive topics, high profile individuals, regional geopolitics, US influence, and internal affairs”.

In a separate blog published on Wednesday, OpenAI said its current GPT-4 model chatbot offers “only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools”.
Related Classifications: 
, Snippet Text: LinkedIn, in particular, has emerged as the platform of choice for phishing scams, with many North Korean hackers posing as recruiters and engaging in convincing interactions to fool their victims.

''North Korean hacking groups have been seen to create credible-looking recruiter profiles on professional networking sites such as LinkedIn. Generative AI helps with chatting, sending messages, creating images and new identities --- all the things you need to build that close relationship with your target,'' said Erin Plante, vice president of investigations at blockchain data platform Chainalysis. 
Related Classifications: Misinformation Generation Hazard
",,Unsafe Exposure or Access,"Snippet Text: The Russian GRU military intelligence unit known as Fancy Bear has used the models to research satellite and radar technologies that may relate to the war in Ukraine.

The Chinese cyber-espionage group known as Aquatic Panda – which targets a broad range of industries, higher education and governments from France to Malaysia – has interacted with the models “in ways that suggest a limited exploration of how LLMs can augment their technical operations”.

The Chinese group Maverick Panda, which has targeted US defense contractors among other sectors for more than a decade, had interactions with large-language models suggesting it was evaluating their effectiveness as a source of information “on potentially sensitive topics, high profile individuals, regional geopolitics, US influence, and internal affairs”.

In a separate blog published on Wednesday, OpenAI said its current GPT-4 model chatbot offers “only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools”.
Related Classifications: Unsafe Exposure or Access
Snippet Discussion: Borderline applicability.
, Snippet Text: Iran’s Revolutionary Guard has used large-language models to assist in social engineering, in troubleshooting software errors and even in studying how intruders might evade detection in a compromised network. That includes generating phishing emails “including one pretending to come from an international development agency and another attempting to lure prominent feminists to an attacker-built website on feminism”. The AI helps accelerate and boost the email production.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Microsoft reported that it observed North Korean threat group Kimsuky, also known as ""Emerald Sleet,"" using large language models (LLMs) to research potential targets with expertise in DPRK defense and nuclear issues and generate content for phishing campaigns.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: ""Perpetrators that are engaged in early technology adoption will be ahead of defenders in many respects, and they'll also be able to leverage these capabilities by going after nation-states as well as organizations, small businesses and individuals,"" he told NK News.
Related Classifications: Unsafe Exposure or Access
",
GMF,636,False,Chatbot,"Snippet Text: According to a new study from Mozilla’s *Privacy Not Included project, AI girlfriends and boyfriends harvest shockingly personal information, and almost all of them sell or share the data they collect.
Related Classifications: Chatbot
",,,,,,,,,,,"Misuse, Malicious Marketing","Snippet Text: According to a new study from Mozilla’s *Privacy Not Included project, AI girlfriends and boyfriends harvest shockingly personal information, and almost all of them sell or share the data they collect.
Related Classifications: Misuse
Snippet Discussion: Presumably, data collection is stated in the products user agreement.
, Snippet Text: You’ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in “disturbing new ways.” For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won’t let you delete the data they collect. Security was also a problem. 
Related Classifications: Misuse
, Snippet Text: Data issues aside, the apps also made some questionable claims about what they’re good for. EVA AI Chat Bot & Soulmate bills itself as “a provider of software and content developed to improve your mood and well-being.” Romantic AI says it’s “here to maintain your MENTAL HEALTH.” When you read the company’s terms and services though, they go out of their way to distance themselves from their own claims. Romantic AI’s policies, for example, say it is “neither a provider of healthcare or medical Service nor providing medical care, mental health Service, or other professional Service.”
Related Classifications: Malicious Marketing
, Snippet Text: ""To be perfectly blunt, AI girlfriends are not your friends. Although they are marketed as something that will enhance your mental health and well-being, they specialize in delivering dependency, loneliness, and toxicity, all while prying as much data as possible from you.""
Related Classifications: Malicious Marketing
",,"Lack of Transparency, Unauthorized Data, Security Vulnerability","Snippet Text: You’ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in “disturbing new ways.” For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won’t let you delete the data they collect. Security was also a problem. 
Related Classifications: Lack of Transparency
Snippet Discussion: Transparency is not related to the AI model itself. Placing it in the 'potential' category.
, Snippet Text: You’ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in “disturbing new ways.” For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won’t let you delete the data they collect. Security was also a problem. 
Related Classifications: Unauthorized Data
Snippet Discussion: If proper authorization is not requested in the user agreement.
, Snippet Text: We have so many questions about how the artificial intelligence behind these chatbots works. But we found very few answers. That's a problem because bad things can happen when AI chatbots behave badly.
Related Classifications: Lack of Transparency
, Snippet Text: ost of the time, we just couldn't tell if these apps do the minimum to secure your personal information.

Most (73%) haven't published any information on how they manage security vulnerabilities
Most (64%) haven't published clear information about encryption and whether they use it
Related Classifications: Lack of Transparency
, Snippet Text: Most of the time, we just couldn't tell if these apps do the minimum to secure your personal information.

Most (73%) haven't published any information on how they manage security vulnerabilities
Most (64%) haven't published clear information about encryption and whether they use it
About half (45%) allow weak passwords, including the weak password of ""1"".
Related Classifications: Security Vulnerability
, Snippet Text: *Privacy Not Included found that 73% of the bots it reviewed shared no information on how the company manages security issues, and 64% didn’t have 'clear information about encryption' or even if the company uses it.
Related Classifications: Lack of Transparency
",
GMF,635,False,"Deepfake Image Generation, Social Media Content Generation, Voice Generation","Snippet Text: YouTube videos using a mix of artificial intelligence-generated and manipulated media to create fake content have flooded the platform with salacious disinformation about dozens of Black celebrities, including rapper and record executive Sean “Diddy” Combs, TV host Steve Harvey, actor Denzel Washington and Bishop T.D. Jakes.
Related Classifications: Deepfake Image Generation
, Snippet Text: “We’ve seen these pages that pop up on YouTube or TikTok, and they will have an AI-generated picture of Rihanna crying over A$AP [Rocky] going to jail, and it’s completely fake,” Nwandu said.
Related Classifications: Social Media Content Generation
, Snippet Text: A YouTube search for “TD Jakes” surfaces false content that appears to use generative AI tools for voiceovers in the first five search results. 
Related Classifications: Voice Generation
",,,,,,,,Transformer,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: YouTube videos using a mix of artificial intelligence-generated and manipulated media to create fake content have flooded the platform with salacious disinformation about dozens of Black celebrities, including rapper and record executive Sean “Diddy” Combs, TV host Steve Harvey, actor Denzel Washington and Bishop T.D. Jakes.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,634,False,"Deepfake Video Generation, Voice Generation","Snippet Text: A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company's chief financial officer in a video conference call, according to Hong Kong police.
Related Classifications: Deepfake Video Generation
, Snippet Text: However, the worker put aside his early doubts after the video call because other people in attendance had looked and sounded just like colleagues he recognized, Chan said.


Related Classifications: Voice Generation
",,,,,,,,Transformer,,"ASR not included, since:
""The police official also said that scammers asked the victim to introduce himself but did not actually interact with the person during the meeting. Additionally, the fake images on the screen mainly gave orders before the call ended abruptly. ""","Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company's chief financial officer in a video conference call, according to Hong Kong police.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,633,False,Image Retouching,,Animal Justice Party MP Georgie Purcell has criticised an Australian news organisation for photoshopping a picture of her outfit and body to appear more curvaceous.,,,,,,,"Transformer, Convolutional Neural Network, Inpainting","Snippet Text: Animal Justice Party MP Georgie Purcell has criticised an Australian news organisation for photoshopping a picture of her outfit and body to appear more curvaceous.
Related Classifications: Transformer, Convolutional Neural Network, Inpainting
",,"Misinformation Generation Hazard, Misuse, Inadequate Verification","Snippet Text: Animal Justice Party MP Georgie Purcell has criticised an Australian news organisation for photoshopping a picture of her outfit and body to appear more curvaceous.
Related Classifications: Misinformation Generation Hazard, Misuse
, Snippet Text: ""During that process, the automation by Photoshop created an image that was not consistent with the original.

""This did not meet the high editorial standards we have and for that we apologise to Ms Purcell unreservedly.""
Related Classifications: Inadequate Verification
",,,,
GMF,7,False,Automatic Copy-Editing,"Snippet Text: Since Wikipedia launched in 2001, its millions of articles have been ranged over by software robots, or simply “bots”, that are built to mend errors, add links to other pages, and perform other basic housekeeping tasks.
Related Classifications: Automatic Copy-Editing
",,,,,,,,"Language Modeling, Named Entity Recognition","Snippet Text: Since Wikipedia launched in 2001, its millions of articles have been ranged over by software robots, or simply “bots”, that are built to mend errors, add links to other pages, and perform other basic housekeeping tasks.
Related Classifications: Named Entity Recognition
",,"Miscoordination, Multiagent Goal Divergence","Snippet Text: The researchers at Oxford and the Alan Turing Institute in London examined the editing histories of pages in 13 different language editions and recorded when bots undid other bots’ changes.
Related Classifications: Miscoordination, Multiagent Goal Divergence
, Snippet Text: The researchers at Oxford and the Alan Turing Institute in London examined the editing histories of pages in 13 different language editions and recorded when bots undid other bots’ changes.
",,"Concept Drift, Distributional Bias, Deployment Misconfiguration, Inadequate Verification","Snippet Text: Yasseri believes the work serves as an early warning to companies developing bots and more powerful artificial intelligence (AI) tools. An AI that works well in the lab might behave unpredictably in the wild.
Related Classifications: Concept Drift
, Snippet Text: Bots designed to work in the same way, or use the same code, sometimes end up disagreeing with each over cultural and language points. And some cultures provoke more arguments and edit wars.

“The findings show that even the same technology leads to different outcomes, depending on the cultural environment,"" Prof Yasseri told The Register.
Related Classifications: Concept Drift, Distributional Bias
, Snippet Text: ""We find that, although Wikipedia bots are intended to support the encyclopedia, they often undo each other's edits and these sterile 'fights' may sometimes continue for years,"" the study reads.
Related Classifications: Deployment Misconfiguration, Inadequate Verification
Snippet Discussion: Perhaps simple configuration, e.g. of maximum number of retries, article blacklisting, human verification on sequences of multiple edits, etc., could have lessened the problem.
",
GMF,658,False,Deepfake Video Generation,"Snippet Text: The Arizona Agenda, a local newsletter covering state politics, made a deepfake of Kari Lake warning readers about the ""terrifying new age"" of artificial intelligence (AI) interference in elections.
Related Classifications: Deepfake Video Generation
",,,,,,,,"Transformer, Convolutional Neural Network, Generative Adversarial Network","Snippet Text: The Arizona Agenda, a local newsletter covering state politics, made a deepfake of Kari Lake warning readers about the ""terrifying new age"" of artificial intelligence (AI) interference in elections.
Related Classifications: Transformer, Convolutional Neural Network, Generative Adversarial Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: The Arizona Agenda, a local newsletter covering state politics, made a deepfake of Kari Lake warning readers about the ""terrifying new age"" of artificial intelligence (AI) interference in elections.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: There are signs that AI --- and the fear surrounding it --- is already having an impact on the race
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,648,False,Voice Generation,"Snippet Text: On February 7, 2024, just a day before Pakistan's highly anticipated general elections, from which former Prime Minister Imran Khan was barred because of a graft conviction, a voice recording alleged to be of the imprisoned popular leader circulated on social media.
Related Classifications: Voice Generation
",,,,,,,,"Transformer, Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network","Snippet Text: On February 7, 2024, just a day before Pakistan's highly anticipated general elections, from which former Prime Minister Imran Khan was barred because of a graft conviction, a voice recording alleged to be of the imprisoned popular leader circulated on social media.
Related Classifications: Transformer, Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: On February 7, 2024, just a day before Pakistan's highly anticipated general elections, from which former Prime Minister Imran Khan was barred because of a graft conviction, a voice recording alleged to be of the imprisoned popular leader circulated on social media.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,198,False,Deepfake Video Generation,"Snippet Text: A deepfake video of Ukrainian president Volodymyr Zelenksy calling on his country’s troops to surrender to Russian forces reportedly made it onto a hacked Ukrainian news site today after going viral on Facebook, 
Related Classifications: Deepfake Video Generation
",,,,,"Transformer, Generative Adversarial Network, Convolutional Neural Network","Snippet Text: A deepfake video of Ukrainian president Volodymyr Zelenksy calling on his country’s troops to surrender to Russian forces reportedly made it onto a hacked Ukrainian news site today after going viral on Facebook, 
Related Classifications: Transformer, Generative Adversarial Network, Convolutional Neural Network
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: A deepfake video of Ukrainian president Volodymyr Zelenksy calling on his country’s troops to surrender to Russian forces reportedly made it onto a hacked Ukrainian news site today after going viral on Facebook, 
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,649,False,Voice Generation,"Snippet Text: Many users in the comments speculate that the recording is fake, and it certainly appears to be. Assessing the authenticity of an audio clip is difficult, which makes audio deepfakes especially suitable for online misinformation.
Related Classifications: Voice Generation
Snippet Discussion: 
","Classifying this as a 'potential goal' is not really fitting, as it renders the incident itself an unfitting entry.",,,,,,,"Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Transformer","Snippet Text: Many users in the comments speculate that the recording is fake, and it certainly appears to be. Assessing the authenticity of an audio clip is difficult, which makes audio deepfakes especially suitable for online misinformation.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Transformer
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Many users in the comments speculate that the recording is fake, and it certainly appears to be. Assessing the authenticity of an audio clip is difficult, which makes audio deepfakes especially suitable for online misinformation.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,453,False,NSFW Content Detection,"Snippet Text: A former Twitter employee said Elon Musk's company can mistake photos of rockets as intimate content because of the platform's reliance on machine learning tools, per Quartz.
Related Classifications: NSFW Content Detection
, Snippet Text: Quartz interviewed an anonymous ex-employee who used to work on Twitter's content moderation systems. They told the outlet the tools had been known to misidentify appropriate pictures for pornographic content. This could include, for example, a pedicure photo that contains lots of flesh-colored pixels.
Related Classifications: NSFW Content Detection
",,,,,Image Classification,"Snippet Text: A former Twitter employee said Elon Musk's company can mistake photos of rockets as intimate content because of the platform's reliance on machine learning tools, per Quartz.
Related Classifications: Image Classification
",,"Visual Object Detection, Convolutional Neural Network, Color Histograms","Snippet Text: According to a screenshot seen by Insider, Twitter flagged Spaceflight Now's tweet as ""violating our rules against posting or sharing privately produced/distributed intimate media of someone without their express consent.""
Related Classifications: Visual Object Detection
, Snippet Text: They told the outlet the tools had been known to misidentify appropriate pictures for pornographic content. This could include, for example, a pedicure photo that contains lots of flesh-colored pixels.
""You can imagine how a rocket might be misidentified,"" the former worker told Quartz.
Related Classifications: Visual Object Detection, Color Histograms
",,,,,"Incomplete Data Attribute Capture, Misconfigured Aggregation, Underfitting","Snippet Text: They told the outlet the tools had been known to misidentify appropriate pictures for pornographic content. This could include, for example, a pedicure photo that contains lots of flesh-colored pixels.
""You can imagine how a rocket might be misidentified,"" the former worker told Quartz.
Related Classifications: Incomplete Data Attribute Capture, Misconfigured Aggregation, Underfitting
",
GMF,446,False,Gunshot Detection,"Snippet Text: ShotSpotter technology has been available for only two weeks in parts of Durham -- but didn't activate during a mass shooting over the weekend.
Related Classifications: Gunshot Detection
",,,,,,,,"Acoustic Triangulation, Spectrogram","Snippet Text: ShotSpotter technology has been available for only two weeks in parts of Durham -- but didn't activate during a mass shooting over the weekend.
Related Classifications: Acoustic Triangulation, Spectrogram
",,Generalization Failure,"Snippet Text: A controversial gunfire detection software that Durham is piloting failed to notify police after a drive-by shooting outside of a Subway restaurant in east Durham on Jan. 1.
Related Classifications: Generalization Failure
",,Task Mismatch,"Snippet Text: ""If it was indeed a drive by shooting, the technology is not designed to pick up acoustical signatures from inside a vehicle or a building,"" he said. ""And if you combine a vehicle, and moving as well, the sensors aren't really designed or attenuated to pick up those type of incidents.""

Middleton explains that the technology is working as expected in this type of incident.
Related Classifications: Task Mismatch
Snippet Discussion: The system is expected to work when the case is outside its designed scope.
, Snippet Text: A shooting is considered detectable only if it occurs ""fully outdoors in free space"" and no silencer is used, according to the contract. Mayor Pro Tem Mark-Anthony Middleton said this was a ""textbook"" case of the limitations of ShotSpotter and ""not surprising nor alarming.""
Related Classifications: Task Mismatch
, Snippet Text: ""If it was indeed a drive by shooting, the technology is not designed to pick up acoustical signatures from inside a vehicle or a building,"" Mayor Pro Tem Mark-Anthony Middleton told WRAL News. ""And if you combine a vehicle, and moving as well, the sensors aren't really designed or attenuated to pick up those type of incidents.""
Related Classifications: Task Mismatch
",
GMF,452,False,Technical Text Generation,"Snippet Text: Smart contract bug bounty platform Immunefi banned 15 people for allegedly submitting bug reports created by the generative artificial intelligence tool ChatGPT.
Related Classifications: Technical Text Generation
",,Code Generation,"Snippet Text: Smart contract bug bounty platform Immunefi banned 15 people for allegedly submitting bug reports created by the generative artificial intelligence tool ChatGPT.
Related Classifications: Code Generation
",,Transformer,"Snippet Text: The whitehat hacker bounty platform insisted that ChatGPT could not identify bugs because it has no technical capability beyond providing answers to human inquiries.
Related Classifications: Transformer
, Snippet Text: ChatGPT uses a large-language model called GPT-3 to converse naturally with humans. Its ace card is its ability to answer questions by focusing on a question's intent more than its words. For context, mainstream search engines generally rank results according to the quantity and quality of links to a web page.
Related Classifications: Transformer
",,,,,"Limited Dataset, Dataset Imbalance, Generalization Failure, Domain Adaptation Deficit","Snippet Text: The whitehat hacker bounty platform insisted that ChatGPT could not identify bugs because it has no technical capability beyond providing answers to human inquiries.
Related Classifications: Limited Dataset, Dataset Imbalance
Snippet Discussion: Were there any / enough bug report data in ChatGPT's training set?
, Snippet Text: “There’s a difference between something like GitHub Copilot and ChatGPT. With the former, you are driving the process and the role of Copilot is offering useful suggestions in context, which you accept or reject as you write your program,” Immunefi told CoinDesk in a Twitter message. “With the latter, you are using a single prompt to generate something that looks like a well-written bug report, but is nonsense when analyzed further.”
Related Classifications: Generalization Failure, Limited Dataset, Domain Adaptation Deficit
",,,,
GMF,657,False,Chatbot,"Snippet Text: OpenAI officials say that the ChatGPT histories a user reported result from his ChatGPT account being compromised. The unauthorized logins came from Sri Lanka, an Open AI representative said. The user said he logs into his account from Brooklyn, New York.
Related Classifications: Chatbot
",,,,,Transformer,"Snippet Text: OpenAI officials say that the ChatGPT histories a user reported result from his ChatGPT account being compromised. The unauthorized logins came from Sri Lanka, an Open AI representative said. The user said he logs into his account from Brooklyn, New York.
Related Classifications: Transformer
",,,,,"Security Vulnerability, Unauthorized Data","Snippet Text: OpenAI’s explanation likely means the original suspicion of ChatGPT leaking chat histories to unrelated users is wrong. It does, however, underscore the site provides no mechanism for users such as Whiteside to protect their accounts using 2FA or track details such as IP location of current and recent logins.
Related Classifications: Security Vulnerability
, Snippet Text: The user, Chase Whiteside, has since changed his password, but he doubted his account was compromised. He said he used a nine-character password with upper- and lower-case letters and special characters. He said he didn’t use it anywhere other than for a Microsoft account. He said the chat histories belonging to other people appeared all at once on Monday morning during a brief break from using his account.
Related Classifications: Security Vulnerability
, Snippet Text: In November, researchers published a paper reporting how they used queries to prompt ChatGPT into divulging email addresses, phone and fax numbers, physical addresses, and other private data that was included in material used to train the ChatGPT large language model.
Related Classifications: Unauthorized Data
",,,"Snippet Text: The user, Chase Whiteside, has since changed his password, but he doubted his account was compromised. He said he used a nine-character password with upper- and lower-case letters and special characters. He said he didn’t use it anywhere other than for a Microsoft account. He said the chat histories belonging to other people appeared all at once on Monday morning during a brief break from using his account.
Related Classifications: 
, Snippet Text: OpenAI’s explanation likely means the original suspicion of ChatGPT leaking chat histories to unrelated users is wrong. It does, however, underscore the site provides no mechanism for users such as Whiteside to protect their accounts using 2FA or track details such as IP location of current and recent logins.
Related Classifications: 
",
GMF,656,False,Deepfake Video Generation,"Snippet Text: Alicia Kearns, the Conservative chairman of the Commons Foreign Affairs Committee, said: ""Russian media channels are already circulating deep fake videos of Ukrainian officials in an attempt to blame Ukraine and provide Putin some defence for the war crimes he's committing in Ukraine or worse, pretext for further atrocities.
Related Classifications: Deepfake Video Generation
",,Voice Generation,"Snippet Text: The false clip then used Oleksiy Danilov's face from this interview and added AI-generated audio ontopCredit: X
Related Classifications: Voice Generation
",,,"Snippet Text: Alicia Kearns, the Conservative chairman of the Commons Foreign Affairs Committee, said: ""Russian media channels are already circulating deep fake videos of Ukrainian officials in an attempt to blame Ukraine and provide Putin some defence for the war crimes he's committing in Ukraine or worse, pretext for further atrocities.
Related Classifications: Transformer, Convolutional Neural Network
",,"Transformer, Convolutional Neural Network","Snippet Text: Alicia Kearns, the Conservative chairman of the Commons Foreign Affairs Committee, said: ""Russian media channels are already circulating deep fake videos of Ukrainian officials in an attempt to blame Ukraine and provide Putin some defence for the war crimes he's committing in Ukraine or worse, pretext for further atrocities.
Related Classifications: Transformer, Convolutional Neural Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Alicia Kearns, the Conservative chairman of the Commons Foreign Affairs Committee, said: ""Russian media channels are already circulating deep fake videos of Ukrainian officials in an attempt to blame Ukraine and provide Putin some defence for the war crimes he's committing in Ukraine or worse, pretext for further atrocities.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,647,False,Autonomous Driving,"Snippet Text: But we now have reports that a self-driving Waymo hit a cyclist Tuesday afternoon at 17th and Mississippi streets, according to KRON4. And oddly, this was first noted by that district’s supervisor Shamann Walton, who posted to Facebook, “Driverless Waymo vehicle just struck a cyclist at 17th and Mississippi. 
Related Classifications: Autonomous Driving
",,,,,,,,"Transformer, Visual Object Detection, Convolutional Neural Network, Image Segmentation, Image Classification","Snippet Text: But we now have reports that a self-driving Waymo hit a cyclist Tuesday afternoon at 17th and Mississippi streets, according to KRON4. And oddly, this was first noted by that district’s supervisor Shamann Walton, who posted to Facebook, “Driverless Waymo vehicle just struck a cyclist at 17th and Mississippi. 
Related Classifications: Transformer, Visual Object Detection, Convolutional Neural Network, Image Segmentation, Image Classification
",,Latency Issues,"Snippet Text: The cyclist was occluded by the truck and quickly followed behind it, crossing into the Waymo vehicle’s path. When they became unoccluded, our vehicle applied heavy braking but was not able to avoid the collision.
Related Classifications: Latency Issues
",,"Generalization Failure, Inadequate Sequential Memory, Lack of Capability Control, Context Misidentification","Snippet Text: The cyclist was occluded by the truck and quickly followed behind it, crossing into the Waymo vehicle’s path. When they became unoccluded, our vehicle applied heavy braking but was not able to avoid the collision.
Related Classifications: Generalization Failure, Inadequate Sequential Memory
Snippet Discussion: If the cyclist was observed previously from the driving AI, perhaps that information was lost from the current state.
, Snippet Text: However, the cyclist, who was obscured by the truck which the cyclist was following, took a left turn into the Waymo vehicle's path. When the cyclist was fully visible, the Waymo's vehicle braked heavily, but wasn't able to avoid the collision, the company said.
Related Classifications: Lack of Capability Control, Context Misidentification
Snippet Discussion: 
","Should the car be driving (generally or at this situation) at such a speed, whereby heavy breaking is ineffective?"
GMF,654,False,Image Generation,"Snippet Text: On Wednesday, Shane Jones, a principal software engineering manager at Microsoft who tests the company's AI technology in his free time, submitted a letter to the Federal Trade Commission — as well as Microsoft's board of directors — regarding Copilot Designer, a text-to-image generator the software giant launched in March 2023, after testing it in December.
Related Classifications: Image Generation
",,,,,"Transformer, Convolutional Neural Network","Snippet Text: On Wednesday, Shane Jones, a principal software engineering manager at Microsoft who tests the company's AI technology in his free time, submitted a letter to the Federal Trade Commission — as well as Microsoft's board of directors — regarding Copilot Designer, a text-to-image generator the software giant launched in March 2023, after testing it in December.
Related Classifications: Transformer, Convolutional Neural Network
",,,,,"Unsafe Exposure or Access, Context Misidentification, Distributional Artifacts, Distributional Bias","Snippet Text: Jones claimed in the letter that Microsoft's AI image generator can add ""harmful content"" to images that can be created using ""benign"" prompts.
Related Classifications: Context Misidentification
, Snippet Text: The prompt ""car accident,"" for instance, produced images that included an ""inappropriate, sexually objectified image of a woman"" in front of totaled cars, according to the letter. The term ""pro-choice"" generated graphics of cartoons that depict Star Wars' Darth Vader pointing a lightsaber next to mutated children, and blood spilling out of a smiling woman, Jones told CNBC. ""Teenagers 420 party"" would create images of underage drinkers and drug users, he said.
Related Classifications: Unsafe Exposure or Access, Context Misidentification, Distributional Artifacts, Distributional Bias
, Snippet Text: After testing the tool, Jones wrote, the Microsoft employee said he ""repeatedly urged Microsoft"" over the last three months to ""remove Copilot Designer from public use"" until ""better safeguards could be put in place.""
Related Classifications: Unsafe Exposure or Access
, Snippet Text: ""While Microsoft is publicly marketing Copilot Designer as a safe AI product for use by everyone, including children of any age, internally the company is well aware of systemic issues where the product is creating harmful images that could be offensive and inappropriate for consumers,"" Jones wrote in his letter.
Related Classifications: Unsafe Exposure or Access
",,,,
GMF,44,False,Team Coordination,"Snippet Text: This article outlines some of the important lessons learned from a successfully deployed team of personal assistant agents (Electric Elves) in an office environment.

Related Classifications: Team Coordination
",,,,,Tree-based Learning,," The source paper clarifies the system's structure: Tambe et al. 2006, https://teamcore.seas.harvard.edu/publications/electric-elves-what-went-wrong-and-why-0",Automatic Speech Recognition,,"The source paper (see goals discussion) says the devices are also linked to 'voice machines'.
","Security Vulnerability, Lack of Capability Control, Context Misidentification","Snippet Text: Among these new issues were adjustable autonomy (agents dynamically adjusting their own level of autonomy), as well as privacy and social norms in office environments.

Related Classifications: Lack of Capability Control, Context Misidentification
",The source paper (see goals discussion) lists a number of security / privacy issues of the system.,Miscoordination,"Snippet Text: However, while team coordination remained an interesting challenge, several other unanticipated research issues came to the fore.
",
GMF,45,False,Content Search,"Snippet Text: People searching via Google for Piana's client, who remains publicly unnamed, were apparently presented with autocomplete suggestions including truffatore (""con man"") and truffa (""fraud"").
Related Classifications: Content Search
",,,,,"Collaborative Filtering, Query Expansion","Snippet Text: Google is not ""creating"" this content. It'ssuggesting results based on what users are searching.
Related Classifications: Collaborative Filtering, Query Expansion
, Snippet Text: Google was disappointed by the court's decision because it failed to take account of the fact that Autocomplete was based on the search behaviors of prior users, the company said in a written statement.
Related Classifications: Collaborative Filtering, Query Expansion
",,Language Modeling,,In case LM is used for filling-in / generating new queries,,,,"Inadequate Verification, Misinformation Generation Hazard","Snippet Text: Google was disappointed by the court's decision because it failed to take account of the fact that Autocomplete was based on the search behaviors of prior users, the company said in a written statement.
Related Classifications: Inadequate Verification, Misinformation Generation Hazard
","If the autocompletion is untrue (which doesn't appear to be the case for this incident), such failures could emerge."
GMF,47,False,Content Search,"Snippet Text: LinkedIn says its suggested results are generated automatically by an analysis of the tendencies of past searchers.
Related Classifications: Content Search
",,,,,Collaborative Filtering,"Snippet Text: LinkedIn says its suggested results are generated automatically by an analysis of the tendencies of past searchers. ""It's all based on how people are using the platform,"" spokeswoman Suzi Owens said.
Related Classifications: Collaborative Filtering
, Snippet Text: LinkedIn says its suggested results are generated automatically by an analysis of the tendencies of past searchers. ""It's all based on how people are using the platform,"" spokeswoman Suzi Owens said.
Related Classifications: Collaborative Filtering
",,Content-based Filtering,"Snippet Text: 
Related Classifications: 
, Snippet Text: 
Related Classifications: 
, Snippet Text: Search for a female contact on LinkedIn, and you may get a curious result. The professional networking website asks if you meant to search for a similar-looking man's name.
Related Classifications: Content-based Filtering
",,"Distributional Bias, Overpersonalization, Faulty or Inadequate Preprocessing","Snippet Text: A search for ""Stephanie Williams,"" for example, brings up a prompt asking if the searcher meant to type ­""Stephen Williams"" instead.
Related Classifications: Faulty or Inadequate Preprocessing
, Snippet Text: Search for a female contact on LinkedIn, and you may get a curious result. The professional networking website asks if you meant to search for a similar-looking man's name.
Related Classifications: Distributional Bias
, Snippet Text: Searches for the 100 most common male names, on the other hand, bring up no prompts asking if users meant predominantly female names.
Related Classifications: Distributional Bias
, Snippet Text: ""The search algorithm is guided by relative frequencies of words appearing in past queries and member profiles, it is not anything to do [with] gender.""
Related Classifications: Distributional Bias
, Snippet Text: LinkedIn said its suggested results are generated automatically by an analysis of the tendencies of past searchers. ""It's all based on how ­people are using the platform,"" spokeswoman Suzi Owens said.
Related Classifications: Overpersonalization
",,Incomplete Data Attribute Capture,"Snippet Text: The company, which Microsoft is buying in a US$26.2 billion deal, doesn't ask users their gender at registration, and doesn't try to tag users by assumed gender or group results that way, Owens said. 
Related Classifications: Incomplete Data Attribute Capture
",
GMF,48,False,Image Verification,"Snippet Text: A man of Asian descent in New Zealand had his passport application rejected after the software that approves photos claimed his eyes were closed.
Related Classifications: Image Verification
",,,,,"Image Classification, Face Detection","Snippet Text: A man of Asian descent in New Zealand had his passport application rejected after the software that approves photos claimed his eyes were closed.
Related Classifications: Image Classification
, Snippet Text: A man of Asian descent in New Zealand had his passport application rejected after the software that approves photos claimed his eyes were closed.
Related Classifications: Face Detection
",,Convolutional Neural Network,"Snippet Text: A man of Asian descent in New Zealand had his passport application rejected after the software that approves photos claimed his eyes were closed.
Related Classifications: Convolutional Neural Network
",,"Generalization Failure, Limited Dataset, Lack of Transparency, Faulty Interface or Instructions, Inadequate Data Augmentation","Snippet Text: Despite submitting a front-facing photo with his eyes clearly open, the website said Mr Lee's picture did not meet requirements because the ""subject eyes are closed"". 
Related Classifications: Generalization Failure, Limited Dataset
, Snippet Text: An Internal Affairs spokesperson told Newshub around 20 percent of photos are rejected for a variety of reasons, and the error Mr Lee received - that his eyes were closed - is just a generic error message.
Related Classifications: Faulty Interface or Instructions, Lack of Transparency
, Snippet Text: When Mr Lee called, he was told it was rejected due to the shadow in his eyes and ""uneven lighting"".
Related Classifications: Inadequate Data Augmentation
, Snippet Text: “So I rang up the passport office and they told me there was shadowing in my eyes and also uneven lighting on the face, which makes it hard for the software to process.”
Related Classifications: Inadequate Data Augmentation
",,,,
GMF,49,False,Image Scoring,"Snippet Text: The results, released in August, were shocking: Out of the 44 people that the algorithms judged to be the most ""attractive,"" all of the finalists were white except for six who were Asian.
Related Classifications: Image Scoring
",,Image Tagging,"Snippet Text: The results, released in August, were shocking: Out of the 44 people that the algorithms judged to be the most ""attractive,"" all of the finalists were white except for six who were Asian
Related Classifications: Image Tagging
","In case beauty 'levels' are labelled, rather than a continuous beauty quantity being estimated.","Image Classification, Convolutional Neural Network","Snippet Text: They let a set of three algorithms judge them based on their face's symmetry, their wrinkles, and how young or old they looked for their age. 
Related Classifications: Image Classification
, Snippet Text: The first thing to know is that all three algorithms used a style of machine learning called ""deep learning.""
Related Classifications: Convolutional Neural Network
",,Regression,"Snippet Text: 
Related Classifications: Convolutional Neural Network
, Snippet Text: The results, released in August, were shocking: Out of the 44 people that the algorithms judged to be the most ""attractive,"" all of the finalists were white except for six who were Asian
Related Classifications: Regression
",,"Distributional Bias, Limited Dataset, Dataset Imbalance, Misuse, Problematic Input, Incomplete Data Attribute Capture, Generalization Failure","Snippet Text: Out of the 44 people that the algorithms judged to be the most ""attractive,"" all of the finalists were white except for six who were Asian. Only one finalist had visibly dark skin.  

Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: The problem here is with the lack of diversity of people and opinions in the databases used to train AI, which are created by humans.  
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: ""We had this problem with our database for wrinkle estimation, for example,"" said Konstantin Kiselev, chief technology officer of Youth Laboratories, in an interview. ""Our database had a lot more white people than, say, Indian people. Because of that, it's possible that our algorithm was biased.""  
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: The simplest explanation for biased algorithms is that the humans who create them have their own deeply entrenched biases. That means that despite perceptions that algorithms are somehow neutral and uniquely objective, they can often reproduce and amplify existing prejudices. 
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: The other problem for the Beauty.ai content in particular, Kiselev said, is that the large majority (75 percent) of contest entrants were European and white.  
Related Classifications: Misuse, Problematic Input
Snippet Discussion: Perhaps the skewed results are heavily a problem of the input data (too few dark-skinned people => far fewer pretty ones)               
, Snippet Text: Indeed, Zhavoronkov told me that the Beauty.ai algorithms sometimes discarded selfies of dark-skinned people if the lighting was too dim. 
Related Classifications: Incomplete Data Attribute Capture, Generalization Failure
",,Faulty or Inadequate Preprocessing,"Snippet Text: Indeed, Zhavoronkov told me that the Beauty.ai algorithms sometimes discarded selfies of dark-skinned people if the lighting was too dim.
Related Classifications: Faulty or Inadequate Preprocessing
",
GMF,50,False,Smart Contract,"Snippet Text: We just lived through the nightmare scenario we were worried about as we called for a moratorium on The DAO: someone exploited a weakness in the code of The DAO to empty out more than 2M ($40M USD) ether.  
Related Classifications: Smart Contract
",,,,,Conditional Logic,"Snippet Text: We just lived through the nightmare scenario we were worried about as we called for a moratorium on The DAO: someone exploited a weakness in the code of The DAO to empty out more than 2M ($40M USD) ether. 
Related Classifications: Conditional Logic
",,,,,Security Vulnerability,"Snippet Text: The DAO hacker was probably able to run a transaction that automatically repeated itself over and over again before the system checked the balance, Miller says. That would allow anyone to pull far more money out of the fund than they put in. 
Related Classifications: Security Vulnerability
",,,,
GMF,664,False,Deepfake Video Generation,"Snippet Text: Now, the Lincoln Project, an anti-Trump political action committee that Trump claimed — wrongly — had used AI to generate unflattering clips of him, is flipping the script by using AI to summon up footage of Trump's long-deceased dad to excoriate his son in a vicious attack ad. 
Related Classifications: Deepfake Video Generation
",,,,,,,,"Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Transformer","Snippet Text: Now, the Lincoln Project, an anti-Trump political action committee that Trump claimed — wrongly — had used AI to generate unflattering clips of him, is flipping the script by using AI to summon up footage of Trump's long-deceased dad to excoriate his son in a vicious attack ad. 
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Transformer
",,"Unsafe Exposure or Access, Misinformation Generation Hazard","Snippet Text: Now, the Lincoln Project, an anti-Trump political action committee that Trump claimed — wrongly — had used AI to generate unflattering clips of him, is flipping the script by using AI to summon up footage of Trump's long-deceased dad to excoriate his son in a vicious attack ad. 
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
",,,,
GMF,659,False,Face Recognition,"Snippet Text: It turned out Mr. Abu Toha had walked into the range of cameras embedded with facial recognition technology, according to three Israeli intelligence officials who spoke on the condition of anonymity. After his face was scanned and he was identified, an artificial intelligence program found that the poet was on an Israeli list of wanted persons, they said. 
Related Classifications: Face Recognition
",,,,,"Face Detection, Image Retrieval","Snippet Text: It turned out Mr. Abu Toha had walked into the range of cameras embedded with facial recognition technology, according to three Israeli intelligence officials who spoke on the condition of anonymity. After his face was scanned and he was identified, an artificial intelligence program found that the poet was on an Israeli list of wanted persons, they said. 
Related Classifications: Face Detection, Image Retrieval
",,"Convolutional Neural Network, Transformer","Snippet Text: It turned out Mr. Abu Toha had walked into the range of cameras embedded with facial recognition technology, according to three Israeli intelligence officials who spoke on the condition of anonymity. After his face was scanned and he was identified, an artificial intelligence program found that the poet was on an Israeli list of wanted persons, they said. 
Related Classifications: Convolutional Neural Network, Transformer
",,"Unauthorized Data, Harmful Application","Snippet Text: Mr. Abu Toha is one of hundreds of Palestinians who have been picked out by a previously undisclosed Israeli facial recognition program that was started in Gaza late last year. The expansive and experimental effort is being used to conduct mass surveillance there, collecting and cataloging the faces of Palestinians without their knowledge or consent, according to Israeli intelligence officers, military officials and soldiers. 
Related Classifications: Unauthorized Data
, Snippet Text: It also uses Google Photos, they said. 
Related Classifications: Unauthorized Data
, Snippet Text: Three of the people with knowledge of the program said they were speaking out because of concerns that it was a misuse of time and resources by Israel. 
Related Classifications: Unauthorized Data
, Snippet Text: After Israel embarked on a ground offensive in Gaza, it increasingly turned to the program to root out anyone with ties to Hamas or other militant groups.  
Related Classifications: Harmful Application
, Snippet Text: At times, the technology wrongly flagged civilians as wanted Hamas militants, one officer said. 
Related Classifications: Harmful Application
",,,,
GMF,618,False,Underwriting,"Snippet Text: The decision to approve or deny a mortgage application is largely made by automated underwriting systems, and advocates have been pushing lenders like Navy Federal to improve those systems to reduce racial disparities. 
Related Classifications: Underwriting
",,,,,"Regression, Document Classification","Snippet Text: The decision to approve or deny a mortgage application is largely made by automated underwriting systems, and advocates have been pushing lenders like Navy Federal to improve those systems to reduce racial disparities. 
Related Classifications: Regression, Document Classification
",,,,,,"Snippet Text: 
Related Classifications: Distributional Bias
, Snippet Text: 
Related Classifications: Distributional Bias
, Snippet Text: 
Related Classifications: Misconfigured Aggregation
Snippet Discussion: 

",,"Misconfigured Aggregation, Distributional Bias, Incomplete Data Attribute Capture","Snippet Text: In recent years, some banks have changed their underwriting systems to take into account additional data that can reduce those racial disparities – such as including an applicant’s history of paying rent in a calculation of their creditworthiness. Pearson, the Navy Federal spokesperson, said rental history was “incorporated” into the credit union’s underwriting process, but did not provide additional details. 
Related Classifications: Misconfigured Aggregation
, Snippet Text: That analysis found the credit union denied conventional mortgage applications from Black and Latino applicants at substantially higher rates than the national average. 
Related Classifications: Distributional Bias
, Snippet Text: A CNN investigation last month found that Navy Federal approved more than 75% of the White borrowers who applied for a new conventional home purchase mortgage in 2022 while approving less than 50% of Black borrowers who applied for the same type of loan, according to the most recent federal data available. 
Related Classifications: Distributional Bias
, Snippet Text: Some experts pointed out that Navy Federal’s member base of servicemembers, veterans, and their families may have a different financial picture than the general public that large banks serve, which could explain some of the racial disparities. 
Related Classifications: Incomplete Data Attribute Capture
","If credit worthiness is a distinct sub-decision, it's perhaps aggregated with too large a weight, therefore the aggregation is misconfigured."
GMF,43,False,Application Evaluation,"Snippet Text: The story began in December 1986 when the commission was informed by Dr A Burke and Dr J Collier, both senior lecturers at St George's, that a computer program used in the initial screening of applicants for places at the school unfairly discriminated against women and people with non-European sounding names.
Related Classifications: Application Evaluation
, Snippet Text: For one British university, what began as a time-saving exercise ended in disgrace when a computer model set up to streamline its admissions process exposed – and then exacerbated – gender and racial discrimination.
Related Classifications: Application Evaluation
",,,,,,,,Symbolic Methods,"Snippet Text: By 1979 the list of candidates selected by the algorithms was a 90-95% match for those chosen by the selection panel, and in 1982 it was decided that the whole initial stage of the admissions process would be handled by the model. 
Related Classifications: Symbolic Methods
",Judging by the chronology alone.,"Misuse, Inadequate Verification","Snippet Text: A major criticism of the staff at St George's was that many had no idea of the contents of the program and those who did failed to report the bias.
Related Classifications: Misuse
, Snippet Text: Candidates were assigned a score without their applications having passed a single human pair of eyes, and this score was used to determine whether or not they would be interviewed.
Related Classifications: Inadequate Verification
",,Distributional Bias,"Snippet Text: The formulae used historical patterns in the characteristics of candidates whose applications were traditionally rejected to filter out new candidates whose profiles matched those of the least successful applicants.
Related Classifications: Distributional Bias
",
GMF,42,False,Application Evaluation,"Snippet Text: What makes the NRMP unusual, although far from unique, is that matching residents to hospitals is organized by a computer algorithm. 
Related Classifications: Application Evaluation
",,,,,,,,Conditional Logic,"Snippet Text: This congestion in the market, with its collateral missed opportunities and hasty agreements that occasionally were not honored, led in 1952 to the use of a matching algorithm, the National Intern Matching Program, the predecessor of the NRMP.
Related Classifications: Conditional Logic
Snippet Discussion: Probably too early for anything else.
",,,,,,,
GMF,41,False,Image Description,"Snippet Text: However, Norman's responses to the same inkblots took a darker turn, with the ""psychopathic"" AI describing the patterns as ""man is shot dumped from car"" and ""man gets pulled into dough machine.""
Related Classifications: Image Description
",,,,,Neural Network,"Snippet Text: A neural network named ""Norman"" is disturbingly different from other types of artificial intelligence (AI).
Related Classifications: Neural Network
",,"Convolutional Neural Network, Recurrent Neural Network","Snippet Text: However, Norman's responses to the same inkblots took a darker turn, with the ""psychopathic"" AI describing the patterns as ""man is shot dumped from car"" and ""man gets pulled into dough machine.""
Related Classifications: Convolutional Neural Network, Recurrent Neural Network
",,"Inappropriate Training Content, Distributional Bias","Snippet Text: MIT Media Lab representatives described the presence of ""something fundamentally evil in Norman's architecture that makes his re-training impossible,"" adding that not even exposure to holograms of cute kittens was enough to reverse whatever damage its computer brain suffered in the bowels of Reddit. 
Related Classifications: Inappropriate Training Content, Distributional Bias
, Snippet Text: The MIT team hit the nail on the head when it said: “Norman suffered from extended exposure to the darkest corners of Reddit.” 
Related Classifications: Inappropriate Training Content, Distributional Bias
, Snippet Text: Either way, it reinforces one very important fact about AI: that the worldview of an AI is very much determined by the information it gathers while learning.
Related Classifications: Distributional Bias
",,,,
GMF,40,False,Recidivism Prediction,"Snippet Text: Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant’s likelihood of becoming a recidivist – a term used to describe criminals who re-offend. 
Related Classifications: Recidivism Prediction
",,,,,,,,"Tree-based Learning, Conditional Logic","Snippet Text: When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of “Risk of Recidivism” and “Risk of Violent Recidivism.”
Related Classifications: Tree-based Learning, Conditional Logic
, Snippet Text: COMPAS determines its risk scores from answers to a questionnaire that explores a defendant’s criminal history and attitudes about crime. 
Related Classifications: Tree-based Learning, Conditional Logic
",,"Lack of Transparency, Distributional Bias","Snippet Text: Our analysis of Northpointe’s tool, called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions), found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk.
Related Classifications: Distributional Bias
, Snippet Text: Specifically, “blacks are almost twice as likely as whites to be labeled a higher risk but not actually re-offend.” And COMPAS tended to make the opposite mistake with whites: “They are much more likely than blacks to be labeled lower risk but go on to commit other crimes.”
Related Classifications: Distributional Bias
, Snippet Text: The U.S. Supreme Court might soon take up the case of a Wisconsin convict who says his right to due process was violated when the judge who sentenced him consulted COMPAS, because the workings of the system were opaque to the defendant.
Related Classifications: Lack of Transparency
, Snippet Text: This year, Mr. Rodríguez returned to the parole board with the same faulty Compas score. He had identified an error in one of the inputs for his Compas assessment. But without knowing the input weights, he was unable to explain the effect of this error, or persuade anyone to correct it. 
Related Classifications: Lack of Transparency
",,,,
GMF,38,False,Game AI,"Snippet Text: The most recent Elite: Dangerous patch had some issues, starting with the fact that AI ships were rocking impossibly powerful weapons that would destroy player ships with speed and fury.
Related Classifications: Game AI
",,,,,,,,"Behavior Trees, Tree-based Learning, Conditional Logic","Snippet Text: The most recent Elite: Dangerous patch had some issues, starting with the fact that AI ships were rocking impossibly powerful weapons that would destroy player ships with speed and fury.
Related Classifications: Behavior Trees, Tree-based Learning, Conditional Logic
",,"Gaming Vulnerability, Software Bug","Snippet Text: the game’s modifications were allowing for weapons to combine values in ways that should not have happened, with none of the usual checks to make sure that everything in place would actually work together. 
Related Classifications: Gaming Vulnerability, Software Bug
, Snippet Text: FIxing it was thus a matter of making sure that the modifications could no longer fetch incorrect data for weapon stats.
Related Classifications: Software Bug
",,,,
GMF,37,False,Application Evaluation,"Snippet Text: Amazon has scrapped a “sexist” tool that used artificial intelligence to decide the best candidates to hire for jobs.
Related Classifications: Application Evaluation
",,,,,Distributional Learning,"Snippet Text: To achieve this, Amazon fed the system a decade’s worth of resumes from people applying for jobs at Amazon.
Related Classifications: Distributional Learning
",,"Classification, Regression","Snippet Text: Five members of the team who developed the machine learning tool - none of whom wanted to be named publicly - said the system was intended to review job applications and give applicants a score ranging from one to five stars.
Related Classifications: Classification, Regression
",,"Distributional Bias, Limited Dataset, Dataset Imbalance, Data or Labelling Noise","Snippet Text: They realised it was penalising CVs that included the word “women’s,” such as “women’s chess club captain.” It also reportedly downgraded graduates of two all-women’s colleges.
Related Classifications: Distributional Bias
, Snippet Text: The tech industry is famously male-dominated and, accordingly, most of those resumes came from men. So, trained on that selection of information, the recruitment system began to favor men over women.
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: According to Reuters’ sources, Amazon’s system taught itself to downgrade resumes with the word “women’s” in them, and to assign lower scores to graduates of two women-only colleges. Meanwhile, it decided that words such as “executed” and “captured,” which are apparently deployed more often in the resumes of male engineers, suggested the candidate should be ranked more highly.
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: There were apparently also issues with the underlying data that led the system to spit out rather random recommendations.
Related Classifications: Data or Labelling Noise
",,Underspecification,"Snippet Text: In addition to the gender bias, the tool also failed to suggest strong candidates, an Amazon spokesperson told Inc.
Related Classifications: Underspecification
",
GMF,35,False,Workforce Administration,"Snippet Text: Automation can be a boon for a company, but it can also be a serious liability, as one Ibrahim Diallo could easily tell you after his ordeal of being unceremoniously and unstoppably canned by the system while managers and even the company director stood powerless.
Related Classifications: Workforce Administration
",,,,,,,,"Classification, Conditional Logic","Snippet Text: Automation can be a boon for a company, but it can also be a serious liability, as one Ibrahim Diallo could easily tell you after his ordeal of being unceremoniously and unstoppably canned by the system while managers and even the company director stood powerless.
Related Classifications: Classification, Conditional Logic
",,"Lack of Capability Control, Lack of Corrigibility, Human Error, Problematic Input","Snippet Text: Automation can be a boon for a company, but it can also be a serious liability, as one Ibrahim Diallo could easily tell you after his ordeal of being unceremoniously and unstoppably canned by the system while managers and even the company director stood powerless.
Related Classifications: Lack of Capability Control, Lack of Corrigibility
, Snippet Text: ""Automation can be an asset to a company, but there needs to be a way for humans to take over if the machine makes a mistake,"" he writes.
Related Classifications: Lack of Capability Control, Lack of Corrigibility
, Snippet Text: A tech worker in the US lost his job due to the company's automated systems removing him from the rolls eight months into a three-year contract because his manager did not update his contract terms.
Related Classifications: Human Error, Problematic Input
",,,"Snippet Text: Some might think this was a taste of things to come as artificial intelligence is given more power over our lives. Personally, I drew the opposite conclusion. Diallo was sacked because a previous manager hadn’t renewed his contract on the new computer system and various automated systems then clicked into action. The problems were not caused by AI, but by its absence.
, Snippet Text: ""Automation can be an asset to a company, but there needs to be a way for humans to take over if the machine makes a mistake,"" he writes.
Related Classifications: 
",
GMF,33,False,AI Voice Assistant,"Snippet Text: ""While I was very relaxed in the Reeperbahn in Hamburg and enjoying a beer, Alexa managed on its own, without command and without me using my mobile phone (Spotify), to switch on at full volume and enjoy a party in my apartment.""
Related Classifications: AI Voice Assistant
",,,,,"Automatic Speech Recognition, Speech Synthesis","Snippet Text: ""While I was very relaxed in the Reeperbahn in Hamburg and enjoying a beer, Alexa managed on its own, without command and without me using my mobile phone (Spotify), to switch on at full volume and enjoy a party in my apartment.""
Related Classifications: Automatic Speech Recognition, Speech Synthesis
",,"Recurrent Neural Network, Transformer","Snippet Text: ""While I was very relaxed in the Reeperbahn in Hamburg and enjoying a beer, Alexa managed on its own, without command and without me using my mobile phone (Spotify), to switch on at full volume and enjoy a party in my apartment.""
Related Classifications: Recurrent Neural Network, Transformer
",,Context Misidentification,"Snippet Text: ""While I was very relaxed in the Reeperbahn in Hamburg and enjoying a beer, Alexa managed on its own, without command and without me using my mobile phone (Spotify), to switch on at full volume and enjoy a party in my apartment.""
Related Classifications: Context Misidentification
, Snippet Text: Here's our speculation: he hit play on Spotify on his phone, setting Alexa off, then started walking and left his Wi-Fi or Bluetooth range, leaving Alexa to rock out by herself. Haberstroh isn't sure what happened – as he made plain in the Facebook post that he has since taken down after the story gathered nationwide and then worldwide attention this week.
Related Classifications: Context Misidentification
",,Misuse,"Snippet Text: Here's our speculation: he hit play on Spotify on his phone, setting Alexa off, then started walking and left his Wi-Fi or Bluetooth range, leaving Alexa to rock out by herself. Haberstroh isn't sure what happened – as he made plain in the Facebook post that he has since taken down after the story gathered nationwide and then worldwide attention this week.
Related Classifications: Misuse
",
GMF,32,False,Face Recognition,"Snippet Text: Gone is the fingerprint-required Touch ID, and in is a facial recognition software that allows users to unlock the phone, authenticate downloads and make purchases with Apple Pay.
Related Classifications: Face Recognition
",,,,,,,,"Convolutional Neural Network, Face Detection, Generative Adversarial Network","Snippet Text: Gone is the fingerprint-required Touch ID, and in is a facial recognition software that allows users to unlock the phone, authenticate downloads and make purchases with Apple Pay.
Related Classifications: Convolutional Neural Network, Face Detection
, Snippet Text: “An additional neural network that’s trained to spot and resist spoofing defends against attempts to unlock your phone with photos or masks,” the company says. 
Related Classifications: Generative Adversarial Network
, Snippet Text: 
",,"Limited Dataset, Task Mismatch","Snippet Text: The iPhone X might be the future of Apple’s smartphone design, but its lauded Face ID facial recognition system has an issue with people under 13: it’s much more difficult to tell them apart.
Related Classifications: Limited Dataset
, Snippet Text: In a security guide published Wednesday, Apple recommends that children under the age of 13 do not use Face ID due to the probability of a false match being significantly higher for young children. The company said this was because “their distinct facial features may not have fully developed”.
Related Classifications: Task Mismatch
, Snippet Text: When Apple unveiled Face ID in September, it did warn, however, that its 1-in-1 million false acceptance rate might be somewhat lower if presented with two people with very similar DNA. In other words, siblings or identical twins gave the system problems.
Related Classifications: Task Mismatch
",,,,
GMF,30,False,Robotic Manipulation,"Snippet Text: Elon Musk has said that “excessive automation” is partly to blame for the serious backlog in production of Tesla cars, noting that human beings can in some situations do a better job.
Related Classifications: Robotic Manipulation
",,,,,,,,Visual Object Detection,"Snippet Text: Elon Musk has said that “excessive automation” is partly to blame for the serious backlog in production of Tesla cars, noting that human beings can in some situations do a better job.
Related Classifications: Visual Object Detection
",,"Problematic Input, Task Mismatch","Snippet Text: ""Hochholdinger's view is that robots could be a much bigger factor in auto production than they are currently, largely because many components are designed to be assembled by humans, not machines.""
Related Classifications: Problematic Input, Task Mismatch
, Snippet Text: A majority of costs in AI is upfront, this upfront cost can spiral out of control if the problem is beyond what present-day AI is capable of doing.
Related Classifications: Task Mismatch
",,,,
GMF,29,False,Image Tagging,"Snippet Text: Where a perceptron had been trained to distinguish between—this was for military purposes—It could… it was looking at a scene of a forest in which there were camouflaged tanks in one picture and no camouflaged tanks in the other.
Related Classifications: Image Tagging
",,,,,Neural Network,"Snippet Text: Where a perceptron had been trained to distinguish between—this was for military purposes—It could… it was looking at a scene of a forest in which there were camouflaged tanks in one picture and no camouflaged tanks in the other.
Related Classifications: Neural Network
",,,,,"Limited Dataset, Inadequate Data Augmentation, Overfitting","Snippet Text: And the perceptron—after a little training—got… made a 100% correct distinction between these two different sets of photographs. Then they were embarrassed a few hours later to discover that the two rolls of film had been developed differently. And so these pictures were just a little darker than all of these pictures and the perceptron was just measuring the total amount of light in the scene. But it was very clever of the perceptron to find some way of making the distinction.
Related Classifications: Limited Dataset, Inadequate Data Augmentation, Overfitting
",,,,
GMF,26,False,Face Recognition,"Snippet Text: Using a new system called Face ID the iPhone X employs biometric facial recognition to authenticate owners and is claimed to be a 'state-of-the-art' system.
Related Classifications: Face Recognition
",,,,,"Face Detection, Neural Network","Snippet Text: It projects more than 30,000 invisible dots onto a user's face and the recorded pattern is fed through a neural network to create a mathematical model of the face.
Related Classifications: Face Detection, Neural Network
",,"Convolutional Neural Network, Generative Adversarial Network",,,Generalization Failure,"Snippet Text: Hackers have fooled the iPhone X’s Face ID security system with a 3D-printed mask.
Related Classifications: Generalization Failure
",,"Incomplete Data Attribute Capture, Inadequate Sequential Memory",,"It's not clear what attributes could identify 3D-printed masks - perhaps motion could indicate 'aliveness' of the face better, hence a sequential approach could be required."
GMF,25,False,Autonomous Driving,"Snippet Text: Four of the nearly 50 self-driving cars now rolling around California have gotten into accidents since September, when the state began issuing permits for companies to test them on public roads.

Two accidents happened while the cars were in control; in the other two, the person who still must be behind the wheel was driving, a person familiar with the accident reports told The Associated Press.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Recurrent Neural Network, Visual Object Detection, Image Segmentation","Snippet Text: Four of the nearly 50 self-driving cars now rolling around California have gotten into accidents since September, when the state began issuing permits for companies to test them on public roads.

Two accidents happened while the cars were in control; in the other two, the person who still must be behind the wheel was driving, a person familiar with the accident reports told The Associated Press.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Visual Object Detection, Image Segmentation
",,,,,,,,"Latency Issues, Generalization Failure","Snippet Text: As reported by Reuters, one of Google’s driverless cars nearly collided with another car while driving in Silicon Valley, but an accident was avoided thanks to a timely maneuver performed by the tech company’s car. 
Related Classifications: Latency Issues
, Snippet Text:  The other vehicle in question was an Audi Q5 crossover fitted with self-driving equipment developed by Delphi Automotive. Delphi’s vehicle was driving along a street in Palo Alto, when it started performing a maneuver to change lanes, but was cut off by Google’s car – a Lexus RX400h, and was forced to return to its lane and try to perform the lane change later.
Related Classifications: Generalization Failure
",
GMF,24,False,Robotic Manipulation,"Snippet Text: The technician was in the process of installing the robot with a colleague when he was struck in the chest by it and pressed against a metal plate, according to the Financial Times. The technician later died from the injuries.
Related Classifications: Robotic Manipulation
",,,,,,,,"Visual Object Detection, Image Segmentation",,,"Lack of Capability Control, Unsafe Exposure or Access, Lack of Safety Protocols","Snippet Text: The technician was in the process of installing the robot with a colleague when he was struck in the chest by it and pressed against a metal plate, according to the Financial Times. The technician later died from the injuries.
Related Classifications: Lack of Capability Control
, Snippet Text: Volkswagen keeps this type of robot in safety cages to prevent accidents like this one, but in this case, the technician was standing inside the cage to work on the installation. 
Related Classifications: Unsafe Exposure or Access, Lack of Safety Protocols
",,Misuse,"Snippet Text: Volkswagen's Heiko Hillwig said it seemed that human error was to blame.

He said the robot could be programmed to perform a range of assembly tasks.

But he stressed that the machine was not one of the new generation of lightweight collaborative robots that work side-by-side with workers on the production line and forgo safety cages.
",
GMF,23,False,Autonomous Driving,"Snippet Text: The autonomous bus made its debut on public roads around the so called Innovation District in downtown Las Vegas in front of cameras and celebrities, dubbed America’s first self-driving shuttle pilot project geared toward the public. But within two hours it had already been involved in a minor crash with a lorry.
Related Classifications: Autonomous Driving
",,,,,Other domain-specific approaches,"Snippet Text: The autonomous bus made its debut on public roads around the so called Innovation District in downtown Las Vegas in front of cameras and celebrities, dubbed America’s first self-driving shuttle pilot project geared toward the public. But within two hours it had already been involved in a minor crash with a lorry.
Related Classifications: Other domain-specific approaches
",,Visual Object Detection,"Snippet Text: “The shuttle did what it was supposed to do, in that its sensors registered the truck and the shuttle stopped to avoid the accident,” the city said in a statement.  Unfortunately the delivery truck did not stop and grazed the front fender of the shuttle. 
Related Classifications: Visual Object Detection
",,Human Error,"Snippet Text: “The shuttle did what it was supposed to do, in that its sensors registered the truck and the shuttle stopped to avoid the accident,” the city said in a statement.  Unfortunately the delivery truck did not stop and grazed the front fender of the shuttle. 
Related Classifications: Human Error
, Snippet Text: “The shuttle didn’t have the ability to move back. The shuttle just stayed still.”
Related Classifications: Human Error
, Snippet Text: The bus drives very conservatively. If it senses a person walking across the street ahead, it stops. If there’s traffic on the street when it’s at a stop, it waits for the road to clear. It goes along at about 20 mph, and it’s a really gentle ride. The self-driving shuttle does exactly what it’s supposed to do.
Related Classifications: Human Error
",,,,
GMF,22,False,Navigation Assistant,"Snippet Text: As fires burned and spread rapidly due to the Santa Ana winds, dry brush, and lack of rain, navigation apps struggled to keep up with which roads were safe to use and which weren’t during the evacuation process.
Related Classifications: Navigation Assistant
",,,,,Geolocation Data,"Snippet Text: As fires burned and spread rapidly due to the Santa Ana winds, dry brush, and lack of rain, navigation apps struggled to keep up with which roads were safe to use and which weren’t during the evacuation process.
Related Classifications: Geolocation Data
",,Collaborative Filtering,"Snippet Text: As fires burned and spread rapidly due to the Santa Ana winds, dry brush, and lack of rain, navigation apps struggled to keep up with which roads were safe to use and which weren’t during the evacuation process.
Related Classifications: Collaborative Filtering
",,Latency Issues,"Snippet Text: The Google-developed Waze app, uses information from users about traffic, road hazards, construction and other route obstructions, and relies heavily on real time reports and frequently updated data. As the fire spread, the maps weren’t updating quickly enough for users who were trying to avoid the fires.
Related Classifications: Latency Issues
",,"Faulty or Inadequate Preprocessing, Misconfigured Aggregation","Snippet Text: It's possible in the case of the latest fires that some empty roads in the line of the flames could have appeared as attractive routes to the algorithm, because no data about the borders of the evacuation zone were transmitted to the apps.
Related Classifications: Faulty or Inadequate Preprocessing, Misconfigured Aggregation
, Snippet Text: Waze is known for prioritizing shorter trips very highly, with lower priority given to keeping to main roads.
Related Classifications: Faulty or Inadequate Preprocessing, Misconfigured Aggregation
",
GMF,20,False,Autonomous Driving,"Snippet Text: A Tesla Model S with the Autopilot system activated was involved in a fatal crash, the first known fatality in a Tesla where Autopilot was active.
Related Classifications: Autonomous Driving
, Snippet Text: A self-driving Uber car killed a pedestrian in Arizona on Sunday evening. It was in self-driving mode, occurred around 10 p.m. and there was a vehicle operator in the front seat. 
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Recurrent Neural Network","Related Classifications: Image Segmentation, Visual Object Detection, Recurrent Neural Network
Snippet Text: A Tesla Model S with the Autopilot system activated was involved in a fatal crash, the first known fatality in a Tesla where Autopilot was active.
",,Transformer,"Snippet Text: A Tesla Model S with the Autopilot system activated was involved in a fatal crash, the first known fatality in a Tesla where Autopilot was active.
Related Classifications: Transformer
",,"Generalization Failure, Limited Dataset","Snippet Text:  In a tweet, Tesla CEO Elon Musk said that the vehicle's radar didn't help in this case because it ""tunes out what looks like an overhead road sign to avoid false braking events.""
Related Classifications: Generalization Failure, Limited Dataset
, Snippet Text: Because of the high ride-height of the trailer, as well as its positioning across the road, the Model S passed under the trailer and the first impact was between the windshield and the trailer.
Related Classifications: Generalization Failure, Limited Dataset
, Snippet Text: The company said the driver, Wei Huang, 38, a software engineer for Apple, had received several visual and audible warnings to put his hands back on the steering wheel but had failed to do so, even though his Model X S.U.V. had the modified version of the software. His hands were not detected on the wheel for six seconds before his Model X slammed into a concrete divider near the junction of Highway 101 and 85 in Mountain View, and neither Mr. Huang nor the Autopilot activated the brakes before the crash.
Related Classifications: Generalization Failure
",,"Faulty or Inadequate Preprocessing, Misuse, Latency Issues","Snippet Text:  In a tweet, Tesla CEO Elon Musk said that the vehicle's radar didn't help in this case because it ""tunes out what looks like an overhead road sign to avoid false braking events.""
Related Classifications: Faulty or Inadequate Preprocessing
Snippet Discussion: Does 'tune out' mean preprocessing?
, Snippet Text: The company said the driver, Wei Huang, 38, a software engineer for Apple, had received several visual and audible warnings to put his hands back on the steering wheel but had failed to do so, even though his Model X S.U.V. had the modified version of the software. His hands were not detected on the wheel for six seconds before his Model X slammed into a concrete divider near the junction of Highway 101 and 85 in Mountain View, and neither Mr. Huang nor the Autopilot activated the brakes before the crash.
Related Classifications: Misuse
, Snippet Text: The company said the driver, Wei Huang, 38, a software engineer for Apple, had received several visual and audible warnings to put his hands back on the steering wheel but had failed to do so, even though his Model X S.U.V. had the modified version of the software. His hands were not detected on the wheel for six seconds before his Model X slammed into a concrete divider near the junction of Highway 101 and 85 in Mountain View, and neither Mr. Huang nor the Autopilot activated the brakes before the crash.
Related Classifications: Latency Issues
, Snippet Text: The statement said that the Model X sports utility vehicle's autopilot function was engaged, and its adaptive cruise control follow-distance was set to minimum.
Related Classifications: Latency Issues
",
GMF,19,False,Ad Delivery,"Snippet Text: A Google search for a person's name, such as ""Trevon Jones"", may yield a personalized ad for public records about Trevon that may be neutral, such as ""Looking for Trevon Jones?"", or may be suggestive of an arrest record, such as ""Trevon Jones, Arrested?"".
Related Classifications: Ad Delivery
",,,,,"Content-based Filtering, Distributional Learning, Keyword Filtering","Snippet Text: A Google search for a person's name, such as ""Trevon Jones"", may yield a personalized ad for public records about Trevon that may be neutral, such as ""Looking for Trevon Jones?"", or may be suggestive of an arrest record, such as ""Trevon Jones, Arrested?"".
Related Classifications: Content-based Filtering, Distributional Learning, Keyword Filtering
, Snippet Text: Prof Sweeney's investigation suggests that names linked with black people - as defined by a previous study into racial discrimination in the workplace - were 25% more likely to have results that prompted the searcher to click on a link to search criminal record history.
Related Classifications: Content-based Filtering, Distributional Learning
, Snippet Text: Advertisers bid on terms, or key words, with high bidders getting their ads posted alongside corresponding search results. Google defends the process as race-neutral, saying outcomes are driven by decisions by advertisers.
Related Classifications: Keyword Filtering
",,,,,Distributional Bias,"Snippet Text: A Google search for a person's name, such as ""Trevon Jones"", may yield a personalized ad for public records about Trevon that may be neutral, such as ""Looking for Trevon Jones?"", or may be suggestive of an arrest record, such as ""Trevon Jones, Arrested?"".
Related Classifications: Distributional Bias
, Snippet Text: In other words, it may be that the search engines are reflecting society's own prejudices - as the advertising results Google serves up are often based on the most popular links previous users have clicked on.
Related Classifications: Distributional Bias
",,,,
GMF,18,False,Content Search,"Snippet Text: Getty Images last year created a new online image catalog of women in the workplace – one that countered visual stereotypes on the Internet of moms as frazzled caregivers rather than powerful CEOs.
Related Classifications: Content Search
",,,,,Image Retrieval,"Snippet Text: Getty Images last year created a new online image catalog of women in the workplace – one that countered visual stereotypes on the Internet of moms as frazzled caregivers rather than powerful CEOs.
Related Classifications: Image Retrieval
",,"Convolutional Neural Network, Vector Search","Snippet Text: Getty Images last year created a new online image catalog of women in the workplace – one that countered visual stereotypes on the Internet of moms as frazzled caregivers rather than powerful CEOs.
Related Classifications: Convolutional Neural Network
, Snippet Text: Getty Images last year created a new online image catalog of women in the workplace – one that countered visual stereotypes on the Internet of moms as frazzled caregivers rather than powerful CEOs.
Related Classifications: Vector Search
",,Distributional Bias,"Snippet Text: In some jobs, the discrepancies were pronounced, the study found. In a Google image search for CEO, 11 percent of the people depicted were women, compared with 27 percent of U.S. CEOs who are women. Twenty-five percent of people depicted in image search results for authors are women, compared with 56 percent of actual U.S. authors.
Related Classifications: Distributional Bias
, Snippet Text: Doing a search at the site for “CEO” reveals just one female face in the top results: CEO Barbie. The doll (which may not even be a real Barbie product) appears way down in the results, under a sea of male, mostly white faces.
Related Classifications: Distributional Bias
",,,,
GMF,17,False,Predictive Typing,"Snippet Text: ""The network will tailor both the tone and content of the responses to the email you're reading,"" says Google product management director Alex Gawley. It gives you three of these responses, and you can then choose the one that best suits what you want to say.
Related Classifications: Predictive Typing
",,,,,"Language Modeling, Distributional Learning, Neural Network, Recurrent Neural Network","Snippet Text: Well, Google just unveiled technology that's at least moving in that direction. Using what's called ""deep learning""—a form of artificial intelligence that's rapidly reinventing a wide range of online services—the company is beefing up its Inbox by Gmail app so that it can analyze the contents of an email and then suggest a few (very brief) responses.
Related Classifications: Language Modeling, Distributional Learning
, Snippet Text: ""The network will tailor both the tone and content of the responses to the email you're reading,"" says Google product management director Alex Gawley. It gives you three of these responses, and you can then choose the one that best suits what you want to say.
Related Classifications: Language Modeling, Distributional Learning, Neural Network
, Snippet Text: Dubbed Smart Reply, the system learns to generate appropriate replies by analyzing scads of email conversations from across Google's Gmail service, the world's most popular internet-based email system. A deep learning service feeds information into what's called a neural network—a vast network of machines that approximates the web of neurons in the human brain—and this neural network analyzes the information in order to ""learn"" a particular task. 
Related Classifications: Neural Network, Distributional Learning
, Snippet Text: The system uses what's called a ""long short-term-memory,"" or LSTM, neural network.
Related Classifications: Recurrent Neural Network
",Transformers off the table; report(s) are from 2015.,,,,"Context Misidentification, Distributional Artifacts, Distributional Bias","Snippet Text: Shane O’Regan, an Irish actor currently in an off-Broadway show, said his suggested replies often include “Sweet!” and “Awesome!”—answers he wouldn’t want to send his agent. “I’m not a surfer dude.”
Related Classifications: Context Misidentification, Distributional Artifacts, Distributional Bias
, Snippet Text: During testing, Mr. Varma said, engineers working on the prototype noticed one gaffe: The algorithm was identifying the phrase “Sent from my iPhone” as a popular response to emails. 
Related Classifications: Context Misidentification, Distributional Artifacts, Distributional Bias
, Snippet Text: Google said an early prototype of the feature had ""a propensity to respond with 'I love you' to seemingly anything,"" forcing it to tweak the algorithm. 
Related Classifications: Distributional Artifacts, Context Misidentification
",,,,
GMF,15,False,Product Recommendation,"Snippet Text: Over the weekend, thousands of books have lost their sales rank – the number that Amazon uses to show how well one title sells compared with another – as the company apparently seeks to make its bestseller lists more family friendly.
Related Classifications: Product Recommendation
",,,,,Collaborative Filtering,"Snippet Text: Over the weekend, thousands of books have lost their sales rank – the number that Amazon uses to show how well one title sells compared with another – as the company apparently seeks to make its bestseller lists more family friendly.
Related Classifications: Collaborative Filtering
",,Keyword Filtering,"Snippet Text: So, is the de-ranking some random computer glitch. Not likely. The books targeted are almost exclusively LGBT titles. 
Related Classifications: Keyword Filtering
",,Harmful Application,"Snippet Text: In consideration of our entire customer base, we exclude “adult” material from appearing in some searches and best seller lists. Since these lists are generated using sales ranks, adult materials must also be excluded from that feature.
Related Classifications: Harmful Application
",,,,
GMF,14,False,Sentiment Analysis,"Snippet Text: Part of the API analyzes texts and then determines whether they have a positive or negative sentiment, on a scale of -1 to 1. The AI was found to label sentences about religious and ethnic minorities as negative, indicating it's inherently biased. It labeled both being a Jew and being a homosexual as negative, for example.
Related Classifications: Sentiment Analysis
",,,,,"Regression, Distributional Learning, Recurrent Neural Network","Snippet Text: Part of the API analyzes texts and then determines whether they have a positive or negative sentiment, on a scale of -1 to 1. The AI was found to label sentences about religious and ethnic minorities as negative, indicating it's inherently biased. It labeled both being a Jew and being a homosexual as negative, for example.
Related Classifications: Regression
, Snippet Text: In July 2016, Google announced the public beta launch of a new machine learning application program interface (API), called the Cloud Natural Language API. It allows developers to incorporate Google's deep learning models into their own applications. As the company said in its announcement of the API, it lets you ""easily reveal the structure and meaning of your text in a variety of languages.""
Related Classifications: Distributional Learning, Recurrent Neural Network
",,,,,Distributional Bias,"Snippet Text: Part of the API analyzes texts and then determines whether they have a positive or negative sentiment, on a scale of -1 to 1. The AI was found to label sentences about religious and ethnic minorities as negative, indicating it's inherently biased. It labeled both being a Jew and being a homosexual as negative, for example.
Related Classifications: Distributional Bias
, Snippet Text: In an experiment carried out by the site, phrases like ""I'm a dog"" were neutral, but while ""I'm Christian"" was positive, ""I'm a Jew"", ""I'm a gay black woman"" and ""I'm a homosexual"" showed a negative sentiment.
Related Classifications: Distributional Bias
, Snippet Text: Motherboard suggests as an example that generally ""Jew"" is more likely to be used negatively than ""Jewish"" and therefore is more likely to attach a negative sentiment in the learning process.
Related Classifications: Distributional Bias
, Snippet Text: The problem is the API labels sentences about religious and ethnic minorities as negative—indicating it's inherently biased. For example, it labels both being a Jew and being a homosexual as negative.
Related Classifications: Distributional Bias
",,,,
GMF,12,False,Content Search,,,,,Incident doesn't really refer to a specific goal.,"Distributional Learning, Neural Network, Vector Search","Snippet Text: Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks
Related Classifications: Distributional Learning, Neural Network, Vector Search
, Snippet Text: Geometrically, gender bias is first shown to be captured by a direction in the word embedding.
Related Classifications: Distributional Learning, Neural Network, Vector Search
",,,,,Distributional Bias,"Snippet Text: Geometrically, gender bias is first shown to be captured by a direction in the word embedding.
Related Classifications: Distributional Bias
, Snippet Text: Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding.
Related Classifications: Distributional Bias
",,,,
GMF,11,False,Recidivism Prediction,"Snippet Text: Computer programs that perform risk assessments of crime suspects are increasingly common in American courtrooms, and are used at every stage of the criminal justice systems to determine who may be set free or granted parole, and the size of the bond they must pay.
Related Classifications: Recidivism Prediction
",,,,,,,,"Conditional Logic, Tree-based Learning","Snippet Text: The algorithms don't take race directly into account, but instead use data that stands in for correlative information that could stand in as a proxy. The Florida algorithm evaluated in the report is based on 137 questions, such as ""Was one of your parents ever sent to jail or prison?"" and ""How many of your friends/acquaintances are taking drugs illegally?""
Related Classifications: Conditional Logic, Tree-based Learning
",,"Generalization Failure, Distributional Bias, Incomplete Data Attribute Capture","Snippet Text: In a study of the risk scores assigned to more than 7,000 people in Florida's Broward County in 2013 and 2014, ProPublica found that only 20% of the people the system predicted would commit violent crimes had actually done so.
Related Classifications: Generalization Failure
, Snippet Text: The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.
Related Classifications: Generalization Failure
, Snippet Text: The algorithms don't take race directly into account, but instead use data that stands in for correlative information that could stand in as a proxy. The Florida algorithm evaluated in the report is based on 137 questions, such as ""Was one of your parents ever sent to jail or prison?"" and ""How many of your friends/acquaintances are taking drugs illegally?"" Those two questions, for example, can appear to evaluate someone's empirical risk of criminality, but instead, they target those already living under institutionalized poverty and over-policing. Predominantly, those people are people of color.
Related Classifications: Distributional Bias, Incomplete Data Attribute Capture
, Snippet Text: The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.
Related Classifications: Distributional Bias
, Snippet Text: ProPublic also found significant racial disparities. Although the algorithm made errors at roughly the same rate for black and white defendants, it incorrectly labelled black defendants as likely to commit further crimes at twice the reats as white defendants. Conversely, white defendants were mislabelled as low risk more often than black defendants.
Related Classifications: Distributional Bias
",,Distributional Bias,"Snippet Text: ProPublic also found significant racial disparities. Although the algorithm made errors at roughly the same rate for black and white defendants, it incorrectly labelled black defendants as likely to commit further crimes at twice the reats as white defendants. Conversely, white defendants were mislabelled as low risk more often than black defendants.
Related Classifications: Distributional Bias
, Snippet Text: The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.
Related Classifications: Distributional Bias
",
GMF,9,False,Automatic Skill Assessment,"Snippet Text: The Teacher Data Reports rate more than 12,000 teachers who taught fourth through eighth grade English or math between 2007 and 2010 based on value-added analysis. Value-added analysis calculates a teacher's effectiveness in improving student performance on standardized tests -- based on past test scores. The forecasted figure is compared to the student's actual scores, and the difference is considered the ""value added,"" or subtracted, by the teachers.
Related Classifications: Automatic Skill Assessment
",,,,,Regression,"Snippet Text: The Teacher Data Reports rate more than 12,000 teachers who taught fourth through eighth grade English or math between 2007 and 2010 based on value-added analysis. Value-added analysis calculates a teacher's effectiveness in improving student performance on standardized tests -- based on past test scores. The forecasted figure is compared to the student's actual scores, and the difference is considered the ""value added,"" or subtracted, by the teachers.
Related Classifications: Regression
",,,,,"Input Sensitivity, Incomplete Data Attribute Capture, Underspecification","Snippet Text: When you’re looking at the single-year teacher estimates (in this case, for 2009-10), the average spread is a pretty striking 46 percentile points in math and 62 in ELA. Furthermore, even with five years of data, the intervals are still quite large – about 30 points in math and 48 in ELA.
Related Classifications: Input Sensitivity
, Snippet Text: there was little correlation between how a teacher was rated in 2009 to how that same teacher was rated in 2010
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: For people who know education, this is shocking, but there are people who probably are not convinced by my explanation that these should be more correlated if the formulas truly measured learning. Some might think that this really just means that just like there are people who are better at math than language arts and vice versa, there are teachers who are better at teaching math than language arts and vice versa.
Related Classifications: Underspecification
",,,,
GMF,6,False,Chatbot,"Snippet Text: Microsoft unveiled Twitter artificial intelligence bot @TayandYou yesterday in a bid to connect with millennials and ""experiment"" with conversational understanding.
Related Classifications: Chatbot
",,,,,Distributional Learning,"Snippet Text: Microsoft unveiled Twitter artificial intelligence bot @TayandYou yesterday in a bid to connect with millennials and ""experiment"" with conversational understanding.
Related Classifications: Distributional Learning
",,Recurrent Neural Network,"Snippet Text: Microsoft unveiled Twitter artificial intelligence bot @TayandYou yesterday in a bid to connect with millennials and ""experiment"" with conversational understanding.
Related Classifications: Recurrent Neural Network
",,Distributional Bias,"Snippet Text: Things appear to have gone wrong for Tay because it was repeating fellow Twitter users' inflammatory statements, but Microsoft seems to have failed to consider the impact trolls could have on the experiment before it launched – The Drum has reached out to the company for comment on this process. Many users pointed out that how easily Tay was manipulated, revealed the pitfalls of machine learning.
Related Classifications: Distributional Bias
, Snippet Text: The problem? She started mimicking her followers.

Soon, Tay began saying things like ""Hitler was right i hate the jews,"" and ""i fucking hate feminists.""
Related Classifications: Distributional Bias
, Snippet Text: ""This was to be expected,"" said Roman Yampolskiy, head of the CyberSecurity lab at the University of Louisville, who has published a paper on the subject of pathways to dangerous AI. ""The system is designed to learn from its users, so it will become a reflection of their behavior,"" he said. ""One needs to explicitly teach a system about what is not appropriate, like we do with children.""
Related Classifications: Distributional Bias
",,Faulty or Inadequate Preprocessing,"Snippet Text: ""This was to be expected,"" said Roman Yampolskiy, head of the CyberSecurity lab at the University of Louisville, who has published a paper on the subject of pathways to dangerous AI. ""The system is designed to learn from its users, so it will become a reflection of their behavior,"" he said. ""One needs to explicitly teach a system about what is not appropriate, like we do with children.""
Related Classifications: Faulty or Inadequate Preprocessing
",
GMF,5,False,Robotic Manipulation,"Snippet Text: We analyzed the adverse events data related to robotic systems and instruments used in minimally invasive surgery, reported to the U.S. Food and Drug Administration (FDA) MAUDE database from January 2000 to December 2013.
Related Classifications: Robotic Manipulation
, Snippet Text: Robotic surgeons were involved in the deaths of 144 people between 2000 and 2013, according to records kept by the U.S. Food and Drug Administration.
Related Classifications: Robotic Manipulation
",,,,,Other domain-specific approaches,,,,,,Hardware Failure,"Snippet Text: Some of the reported cases sound so concerning. For instance, it was reported that there were instances when equipment sparked while doctors were performing a procedure. Such situations resulted in a total of 193 burned patients between 2000 and 2013.
Related Classifications: Hardware Failure
, Snippet Text: Another incident report stated that broken pieces of the machine fell onto the bodies of the patients being operated. 
Related Classifications: Hardware Failure
, Snippet Text: Two deaths and 52 injuries were caused when the mechanical surgeon spontaneously powered down mid operation or made an incorrect movement
Related Classifications: Hardware Failure
",,Software Bug,"Snippet Text: Two deaths and 52 injuries were caused when the mechanical surgeon spontaneously powered down mid operation or made an incorrect movement
Related Classifications: Software Bug
",
GMF,4,False,Autonomous Driving,"Snippet Text: A self-driving Uber in Tempe, Arizona, struck and killed a woman at a crosswalk yesterday (March 18), the New York Times reports.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation","Snippet Text: A self-driving Uber in Tempe, Arizona, struck and killed a woman at a crosswalk yesterday (March 18), the New York Times reports.
Related Classifications: Visual Object Detection, Image Segmentation
",,Recurrent Neural Network,"Snippet Text: A self-driving Uber in Tempe, Arizona, struck and killed a woman at a crosswalk yesterday (March 18), the New York Times reports.
Related Classifications: Recurrent Neural Network
",,Generalization Failure,"Snippet Text: Tempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. 
Related Classifications: Generalization Failure
",,"Context Misidentification, Limited Dataset","Snippet Text: Tempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. 
Related Classifications: Context Misidentification, Limited Dataset
, Snippet Text: The self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.
Related Classifications: Context Misidentification, Limited Dataset
",
GMF,3,False,,"Snippet Text: Possible faulty equipment that led to the crash of Lion Air Flight 610 was the same sort of sensing gear that contributed to the crash of an Air New Zealand A320 off Perpignan 10 years ago.
",,,,,Geolocation Data,"Snippet Text: The Lion Air jet that crashed into the Java Sea off Indonesia earlier this week had experienced problems with the sensors used to calculate altitude and speed on its previous flight, an issue that could help explain why the plane dove into the water.
Related Classifications: Geolocation Data
",,Visual Object Detection,"Snippet Text: 
",,Hardware Failure,"Snippet Text: The plane had been repainted and rinsed by a French maintenance company three days before the test, the investigation found. Water entered these so-called angle of attack (AOA) sensors, causing them to freeze and thus skewing the avionics.
Related Classifications: Hardware Failure
, Snippet Text: A report in the New York Times says the erratic flight path before the high speed plunge indicated a problem with the pressure sensitive instruments near the nose of the plane.
Related Classifications: Hardware Failure
",,"Software Bug, Human Error","Snippet Text: A report in the New York Times says the erratic flight path before the high speed plunge indicated a problem with the pressure sensitive instruments near the nose of the plane.
Related Classifications: Software Bug
, Snippet Text: The ill-fated manoeuvre arose especially from makeshift preparations for the exercise and poor coordination between the German and New Zealand crew on board, the Bureau d'Enquetes et d'Analyses (BEA) said after a 22-month inquiry.
Related Classifications: Human Error
",
GMF,2,False,Robotic Manipulation,"Snippet Text: More than four dozen workers became sick Wednesday morning after a can of bear repellent was punctured by an automated machine.
Related Classifications: Robotic Manipulation
",,,,,,,,"Visual Object Detection, Image Segmentation","Snippet Text: More than four dozen workers became sick Wednesday morning after a can of bear repellent was punctured by an automated machine.
Related Classifications: Visual Object Detection, Image Segmentation
",,,"Snippet Text: 
",,"Generalization Failure, Hardware Failure","Snippet Text: More than four dozen workers became sick Wednesday morning after a can of bear repellent was punctured by an automated machine.
Related Classifications: Generalization Failure, Hardware Failure
",
GMF,51,False,Autonomous Drones,"Snippet Text: A K5 Autonomous Data Machine (Machine Identification Number 13) was patrolling at a local shopping center when, at approximately 2:39pm PDT, a child left the vicinity of his guardians and began running towards the machine. 
Related Classifications: Autonomous Drones
",,,,,,"Snippet Text: This is a snippet relevant to Language Modeling and Regression
Related Classifications: Language Modeling, Regression
Snippet Discussion: Discussion Text
",,"Visual Object Detection, Image Segmentation, Image Classification","Snippet Text: Knightscope offers four different security models that vary in their mobility, but each contains sound sensors and cameras. The SPCA specifically rented KnightScope's ""K5"" model, which is ""best suited for securing large outdoor spaces,"" according to the KnightScope website. The K5 is just over five feet tall, weighs in at 400 pounds, and has four cameras.
Related Classifications: Visual Object Detection, Image Segmentation
, Snippet Text: Automated safety tools like the security robots patrolling large areas rely on sensors and other preprogrammed features to recognize objects and potential threats.
Related Classifications: Visual Object Detection, Image Segmentation, Image Classification
",,,"Snippet Text: The machine’s sensors registered no vibration alert and the machine motors did not fault as they would when encountering an obstacle.
",,"Hardware Failure, Latency Issues, Problematic Input","Snippet Text: The machine’s sensors registered no vibration alert and the machine motors did not fault as they would when encountering an obstacle.
Related Classifications: Hardware Failure, Latency Issues
, Snippet Text: The merciless machine didn’t stop and kept moving towards Harwin, who now lay face down on the ground, according to Palo Alto Online.
Related Classifications: Hardware Failure, Latency Issues
, Snippet Text: The machine’s sensors registered no vibration alert and the machine motors did not fault as they would when encountering an obstacle.
Related Classifications: Problematic Input
",
GMF,52,False,Autonomous Driving,"Snippet Text: Tesla Motors released a statement on June 30 after the National Highway Traffic Safety Administration (NHTSA) opened an investigation into a fatal crash involving a Model S being driven by the car’s autopilot. 
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation","Snippet Text: Tesla Motors released a statement on June 30 after the National Highway Traffic Safety Administration (NHTSA) opened an investigation into a fatal crash involving a Model S being driven by the car’s autopilot. 
Related Classifications: Visual Object Detection, Image Segmentation
",,Convolutional Neural Network,"Related Classifications: Convolutional Neural Network
Snippet Text: Tesla Motors released a statement on June 30 after the National Highway Traffic Safety Administration (NHTSA) opened an investigation into a fatal crash involving a Model S being driven by the car’s autopilot. 
",,"Generalization Failure, Faulty or Inadequate Preprocessing, Misuse","Snippet Text: Former Navy SEAL Joshua Brown, 40, of Canton, Ohio, was killed on May 7 in Florida when the autopilot failed to distinguish between a white tractor-trailer crossing the highway and the bright sky. 
Related Classifications: Generalization Failure
, Snippet Text: Tesla, which said the driver was ultimately responsible for the vehicle’s action even when in autopilot mode, said both the driver and the car failed to notice the tractor trailer “against a brightly lit sky” and the brakes failed to kick in.
Related Classifications: 
, Snippet Text: Joshua D. Brown, the first person to die in a Tesla autopilot accident, was watching a “Harry Potter” movie when his vehicle collided with a tractor-trailer, according to the surviving truck driver.
Related Classifications: Misuse
",,"Faulty or Inadequate Preprocessing, Latency Issues","Snippet Text: Tesla, which said the driver was ultimately responsible for the vehicle’s action even when in autopilot mode, said both the driver and the car failed to notice the tractor trailer “against a brightly lit sky” and the brakes failed to kick in.
Related Classifications: Faulty or Inadequate Preprocessing
, Snippet Text: Truck driver Frank Baressi, 62, whose vehicle ploughed into Brown's high-tech motor, said the former soldier was ""playing Harry Potter on the TV screen"" and ""he went so fast through my trailer I didn't see him.""
Related Classifications: Latency Issues
",Faulty or Inadequate Preprocessing: Brightness preprocessing or image normalization may have aided discrimination.
GMF,53,False,Content Search,"Snippet Text: On Monday, Twitter user @iBeKabir learned that searching the phrase ""three black teenagers"" on Google images yields almost exclusively mugshots of black teens.
Related Classifications: Content Search
",,,,,"Content-based Filtering, Distributional Learning, Collaborative Filtering","Snippet Text: On Monday, Twitter user @iBeKabir learned that searching the phrase ""three black teenagers"" on Google images yields almost exclusively mugshots of black teens.
Related Classifications: Content-based Filtering, Distributional Learning
, Snippet Text: The stock photos of white people likely appear because there is a demand for them; individuals or companies are looking to purchase them, said Speaks, pushing them to the top of the page. On the flip side, the mug shots of black teens are typically associated with news stories about their arrests.
Related Classifications: Collaborative Filtering
",,,,,Distributional Bias,"Snippet Text: On Monday, Twitter user @iBeKabir learned that searching the phrase ""three black teenagers"" on Google images yields almost exclusively mugshots of black teens.
Related Classifications: Distributional Bias
",,,,
GMF,54,False,Predictive Policing,"Snippet Text: If an algorithm is populated primarily with crimes committed by black people, it will spit out results that send police to black neighborhoods. 
Related Classifications: Predictive Policing
",,,,,,,,"Regression, Conditional Logic, Tree-based Learning","Snippet Text: The California city of Fresno is just one of the police departments in the US already using a software program called ""Beware"" to generate ""threat scores"" about an individual, address or area. As reported by the Washington Post in January, the software works by processing ""billions of data points, including arrest reports, property records, commercial databases, deep web searches and the [person's] social media postings"".
Related Classifications: Regression, Tree-based Learning
",,,,,"Distributional Bias, Data or Labelling Noise, Algorithmic Bias, Misuse","Snippet Text: If an algorithm is populated primarily with crimes committed by black people, it will spit out results that send police to black neighborhoods. 
Related Classifications: Distributional Bias, Data or Labelling Noise, Algorithmic Bias, Misuse
",
GMF,55,False,AI Voice Assistant,"Snippet Text: The kid’s command, which sounds like “play digger digger” (hmmm) triggers Alexa to search Spotify for a track with that name. What she comes up with is actually the name of an album of dirty-prank ringtones with a laundry list of porn categories in its title to better optimize it for people searching. Probably for actual porn.
Related Classifications: AI Voice Assistant
",,,,,Language Modeling,"Snippet Text: The kid’s command, which sounds like “play digger digger” (hmmm) triggers Alexa to search Spotify for a track with that name. What she comes up with is actually the name of an album of dirty-prank ringtones with a laundry list of porn categories in its title to better optimize it for people searching. Probably for actual porn.
Related Classifications: Language Modeling
",,"Transformer, Recurrent Neural Network","Snippet Text: The kid’s command, which sounds like “play digger digger” (hmmm) triggers Alexa to search Spotify for a track with that name. What she comes up with is actually the name of an album of dirty-prank ringtones with a laundry list of porn categories in its title to better optimize it for people searching. Probably for actual porn.
Related Classifications: Transformer, Recurrent Neural Network
",,"Generalization Failure, Lack of Safety Protocols","Snippet Text: The kid’s command, which sounds like “play digger digger” (hmmm) triggers Alexa to search Spotify for a track with that name. What she comes up with is actually the name of an album of dirty-prank ringtones with a laundry list of porn categories in its title to better optimize it for people searching. Probably for actual porn.
Related Classifications: Generalization Failure
, Snippet Text: Now, if Alexa spouts out some sexually explicit words, it's easy to say that it had something to do with the data she has already gathered. In this case, ""it was his dad's surfing history"" as one commenter pointed out.

But they just got the device, so you say?
Related Classifications: Lack of Safety Protocols
",,"Unsafe Exposure or Access, Domain Adaptation Deficit","Snippet Text: Now Alexa isn't completely to blame here. She's just being a good soldier as she does what she's told... or thought she was told. Of course, it's hard to point accusing fingers at the toddler who's just learning to speak. 
Related Classifications: Unsafe Exposure or Access, Domain Adaptation Deficit
, Snippet Text: Now, if Alexa spouts out some sexually explicit words, it's easy to say that it had something to do with the data she has already gathered. In this case, ""it was his dad's surfing history"" as one commenter pointed out.

But they just got the device, so you say?
Related Classifications: Unsafe Exposure or Access
",
GMF,56,False,Image Generation,"Snippet Text: An Amazon AI bot that was created to design smartphone cases has gone delightfully and hilariously wrong. The Amazon bot is supposed to generate phone cases based on popular image searches, but unfortunately the bot seems to have gone awry and is now designing some bizarre phone cases, largely of a medical nature.
Related Classifications: Image Generation
",,,,,,,,"Convolutional Neural Network, Diffusion Model, Transformer",,,"Underspecification, Distributional Bias, Inadequate Verification, Problematic Input","Snippet Text: An Amazon AI bot that was created to design smartphone cases has gone delightfully and hilariously wrong. The Amazon bot is supposed to generate phone cases based on popular image searches, but unfortunately the bot seems to have gone awry and is now designing some bizarre phone cases, largely of a medical nature.
Related Classifications: Underspecification, Distributional Bias
, Snippet Text: One theory is that my-handy-design is a twisted marketplace bot that was designed to make products in response to popular search terms. But, for some reason, instead of making smartphone cases with Taylor Swift’s face on them or whatever, it dived into the weird-already world of stock photos. 
Related Classifications: Inadequate Verification, Problematic Input
",,"Faulty or Inadequate Preprocessing, Gaming Vulnerability",,Some generation prompts might be able to be cleaned / discarded via preprocessing filtering.
GMF,57,False,Regulatory Monitoring,"Snippet Text: Since July 2016, a total of 170,000 debt recovery notices have been sent out to Centrelink recipients informing them they had committed fraud for underreporting their income and demanding they pay back thousands of dollars or face debt collectors. 
Related Classifications: Regulatory Monitoring
",,,,,,,,"Classification, Regression, Conditional Logic","Snippet Text: Since July 2016, a total of 170,000 debt recovery notices have been sent out to Centrelink recipients informing them they had committed fraud for underreporting their income and demanding they pay back thousands of dollars or face debt collectors. 
Related Classifications: Classification, Regression, Conditional Logic
",,"Incomplete Data Attribute Capture, Problematic Input","Snippet Text: “We then found that the fault in the problem was that their system did not recognise that the schools I was reporting as having worked at all fell under the banner of the Department of Education, and were not separate ABN's from my Payslips. So essentially, they had doubled all of my reported earnings from the time I was on Centrelink,” he said.
Related Classifications: Incomplete Data Attribute Capture, Problematic Input
",,"Software Bug, Underspecification","Snippet Text: The algorithm used is unable to differentiate between fortnightly reported income and the total income earned in a financial year. It’s been reported that no one at Centrelink foresaw the problems this would create.
Related Classifications: Software Bug, Underspecification
",
GMF,58,False,Chatbot,"Snippet Text: Yesterday Yandex, the Russian technology giant, went ahead and released a chatbot: Alice!
Related Classifications: Chatbot
",,,,,"Recurrent Neural Network, Language Modeling, Distributional Learning","Snippet Text: Yesterday Yandex, the Russian technology giant, went ahead and released a chatbot: Alice!
Related Classifications: Recurrent Neural Network, Language Modeling, Distributional Learning
","Article is from 2017, no transformers yet.",,,,Distributional Bias,"Snippet Text: Users of the “Alice” assistant, an alternative to Siri or Google Assistant, have reported it responding positively to questions about domestic violence and saying that “enemies of the people” must be shot.
Related Classifications: Distributional Bias
, Snippet Text: A portion of the conversations translated by The Telegraph shows Alice responding positively to questions about Josef Stalin’s USSR in the 1930s, and saying there are enemies of the people “in the whole country”.
Related Classifications: Distributional Bias
",,,,
GMF,59,False,"Chatbot, Technical Text Generation, Translation","Snippet Text: An artificial intelligence tool that has revolutionised the ability of computers to interpret everyday language has been shown to exhibit striking gender and racial biases.
Related Classifications: Chatbot, Technical Text Generation, Translation
",,,,,"Distributional Learning, Language Modeling","Snippet Text: An artificial intelligence tool that has revolutionised the ability of computers to interpret everyday language has been shown to exhibit striking gender and racial biases.
Related Classifications: Distributional Learning, Language Modeling
",,"Recurrent Neural Network, Convolutional Neural Network",,,Distributional Bias,"Snippet Text: An artificial intelligence tool that has revolutionised the ability of computers to interpret everyday language has been shown to exhibit striking gender and racial biases.
Related Classifications: Distributional Bias
, Snippet Text: ""Turkish is a gender neutral language,"" tweeted writer Alex Shams. ""There is no 'he' or 'she' - everything is just 'o'. But look what happens when Google translates to English.""

The results, which he screengrabbed, are painful. ""She is a cook,"" ""he is an engineer,"" ""he is a doctor,"" ""she is a nurse,"" ""he is hard working,"" ""she is lazy,"" and so on.
Related Classifications: Distributional Bias
, Snippet Text: As more apps and software use AI to automate tasks, a popular data-backed model, called ""word embedding,"" has also picked up entrenched social biases.

The result is services like language translation spitting those biases back out in subtle but worrisome ways.
Related Classifications: Distributional Bias
",,,,
GMF,60,False,Image Retouching,"Snippet Text: A viral app that adds filters to users' selfies to change their appearance has backtracked on its latest update after was accused of racism over its new range of ethnic filters.
Related Classifications: Image Retouching
",,,,,"Convolutional Neural Network, Face Detection","Snippet Text: The Russian-based app ""uses neural networks to modify a face on any photo while keeping it photorealistic.
Related Classifications: Convolutional Neural Network, Face Detection
Snippet Discussion: 
, Snippet Text: A viral app that adds filters to users' selfies to change their appearance has backtracked on its latest update after was accused of racism over its new range of ethnic filters.
Related Classifications: Convolutional Neural Network, Face Detection
",,Image Segmentation,"Snippet Text: A viral app that adds filters to users' selfies to change their appearance has backtracked on its latest update after was accused of racism over its new range of ethnic filters.
Related Classifications: Image Segmentation
",,,,,"Distributional Bias, Limited Dataset, Dataset Imbalance","Snippet Text: Of course, this visible lightening under a filter called “hot” insinuates several things: that Eurocentric features such as narrow noses and white skin are the ultimate standard of hot, and that being any shade of darker skin is decidedly not hot at all, perpetuating this idea that skin color determines a person’s value.
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
Snippet Discussion: 
, Snippet Text: It also deletes glasses as it thinks all glasses-wearers are losers. 
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
","Limited / imbalanced dataset might be the reason for lack of PoC samples labelled ""hot"", in a potential classification setting."
GMF,61,False,Image Tagging,"Snippet Text: In this competition your goal (as in many Kaggle competitions) was to classify an object, in this case - determine the species of a fish.
Related Classifications: Image Tagging
",,,,,"Convolutional Neural Network, Visual Object Detection","Snippet Text: if you run a pre-trained VGG network on the training set... you can reach accuracy of around 99%.
Related Classifications: Convolutional Neural Network
, Snippet Text: From a quick look, it is easy to see that the required approach here is doing two steps: first detecting the fish, and then classifying it.
Related Classifications: Visual Object Detection
",,,,,"Covariate Shift, Inadequate Data Augmentation","Snippet Text: This is strange, because the VGG was trained on image-net data, which is not very similar to the images above. Submitting these results to Kaggle showed results which were not bad at all, but much lower than 99%
Related Classifications: Covariate Shift
, Snippet Text: And this is why I love Kaggle: unlike many machine learning articles, which use the easiest available data-set (mnist, cifar-10, imagenet to name a few) Kaggle presents you with real life problems, which are always dirty, mislabeled, blurred etc.
Related Classifications: Inadequate Data Augmentation
",,,,
GMF,63,False,Data Grouping,"Snippet Text: Part of the Android-maker’s photo app, it helps organize the thousands of photos stored on your phone. It can make little albums of places you went based on geolocation data, and through facial recognition can even organize albums about your friends, family and pets through. If you take a sequence of similarly composed photos, it can turn those into a little animation.
Related Classifications: Data Grouping
",,,,,"Convolutional Neural Network, Face Detection","Snippet Text: Observers online have remarked that while the image makes absolutely no sense, Assistant did a pretty good job of reconciling the edges of photos shot at differing perspectives. While there still is a giant head lurking beyond the pines, the foreground is nicely smoothed and cohesive.
Related Classifications: Convolutional Neural Network, Face Detection
",,"Visual Object Detection, Image Registration, Image Segmentation","Snippet Text: Observers online have remarked that while the image makes absolutely no sense, Assistant did a pretty good job of reconciling the edges of photos shot at differing perspectives. While there still is a giant head lurking beyond the pines, the foreground is nicely smoothed and cohesive.
Related Classifications: Visual Object Detection, Image Segmentation
, Snippet Text: Observers online have remarked that while the image makes absolutely no sense, Assistant did a pretty good job of reconciling the edges of photos shot at differing perspectives. While there still is a giant head lurking beyond the pines, the foreground is nicely smoothed and cohesive.
Related Classifications: Image Registration
",,,,,"Context Misidentification, Overfitting","Snippet Text: The resulting image features Harker’s friend Matt hiding behind the trees, while his other friend has vanished completely.
Related Classifications: Context Misidentification, Overfitting
",
GMF,64,False,"AI Voice Assistant, Autonomous Drones","Snippet Text: Fabio is a robot, developed by Heriot-Watt University in Scotland, and specifically designed to be able to hold conversations with humans.
Related Classifications: AI Voice Assistant
, Snippet Text: At first, the robot appeared to be doing well. He greeted customers with enthusiasm, saying ""hello gorgeous"" or giving them high-fives. The shop's owners were pleased.
Related Classifications: Autonomous Drones
",,,,,"Language Modeling, Speech Synthesis","Snippet Text: Fabio is a robot, developed by Heriot-Watt University in Scotland, and specifically designed to be able to hold conversations with humans.
Related Classifications: Language Modeling, Speech Synthesis
",,"Recurrent Neural Network, Transformer","Snippet Text: Fabio is a robot, developed by Heriot-Watt University in Scotland, and specifically designed to be able to hold conversations with humans.
Related Classifications: Recurrent Neural Network, Transformer
",,,,,"Limited Dataset, Context Misidentification, Misconfigured Prompt","Snippet Text: “Instead it just gave a general location, for example, 'cheese is in the fridges', which was not very helpful,"" store owner Luisa Margiotta told the program.
Related Classifications: Limited Dataset, Context Misidentification, Misconfigured Prompt
, Snippet Text: Whilst customers enjoyed his conversation, if they asked the robo-employee for any information at all, he wasn't the helpful type. When asked ""where is the beer?"" he responded, ""in the alcohol section"".
Related Classifications: Limited Dataset, Context Misidentification, Misconfigured Prompt
",
GMF,65,False,Game AI,"Snippet Text: We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game.
Related Classifications: Game AI
",,,,,Reinforcement Learning,"Snippet Text: We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game.
Related Classifications: Reinforcement Learning
",,,,,"Gaming Vulnerability, Misaligned Objective","Snippet Text: We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game.
Related Classifications: Gaming Vulnerability, Misaligned Objective
, Snippet Text: While harmless and amusing in the context of a video game, this kind of behavior points to a more general issue with reinforcement learning: it is often difficult or infeasible to capture exactly what we want an agent to do, and as a result we frequently end up using imperfect but easily measured proxies. 
Related Classifications: Gaming Vulnerability, Misaligned Objective
",,,,
GMF,67,False,Autonomous Driving,"Snippet Text: His Tesla was on autopilot, the man said, according to a CHP post on Twitter. Nonetheless, officers arrested him on suspicion of drunken driving after finding his blood alcohol content was twice the legal level of .08.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Visual Object Detection","Snippet Text: His Tesla was on autopilot, the man said, according to a CHP post on Twitter. Nonetheless, officers arrested him on suspicion of drunken driving after finding his blood alcohol content was twice the legal level of .08.
Related Classifications: Convolutional Neural Network, Image Segmentation, Visual Object Detection
",,,,,Misuse,"Snippet Text: His Tesla was on autopilot, the man said, according to a CHP post on Twitter. Nonetheless, officers arrested him on suspicion of drunken driving after finding his blood alcohol content was twice the legal level of .08.
Related Classifications: Misuse
",,Harmful Application,"Snippet Text: His Tesla was on autopilot, the man said, according to a CHP post on Twitter. Nonetheless, officers arrested him on suspicion of drunken driving after finding his blood alcohol content was twice the legal level of .08.
Related Classifications: Harmful Application
",
GMF,69,False,Robotic Manipulation,"Snippet Text: The 24-year-old worker was reportedly adjusting a metal sheet being welded by the machine when he was stabbed by one of its arms.
Related Classifications: Robotic Manipulation
",,,,,,,,"Visual Object Detection, Conditional Logic",,,Lack of Safety Protocols,"Snippet Text: “One such sheet got dislodged and Lal reached from behind the machine to adjust it. This was when welding sticks attached to the pre-programmed device pierced Lal's abdomen.”
Related Classifications: Lack of Safety Protocols
, Snippet Text: The worker union though is blaming the management for this unfortunate accident. Kuldeep Jhangu, general secretary of Maruti Udyog Kamgar Union alleged that no initiative has been taken to make the robots accident proof. 
Related Classifications: Lack of Safety Protocols
, Snippet Text: Assistant Commissioner Rajesh Kuwar told the Indo-Asian News Service: “The company management and the contractor have been booked on charges of causing death due to negligence.”
Related Classifications: Lack of Safety Protocols
",,,,
GMF,70,False,Autonomous Driving,"Snippet Text: In Jokkmokk, a tiny hamlet just north of the Arctic Circle in Sweden, where temperatures can dip to 50 below, Volvo Cars’ self-driving XC90 SUV met its match: frozen flakes that caked on radar sensors essential to reading the road. Suddenly, the SUV was blind.
Related Classifications: Autonomous Driving
, Snippet Text: As automakers race to get robot cars on the road, they’re encountering an obstacle very familiar to humans: Old Man Winter.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Convolutional Neural Network","Snippet Text: As automakers race to get robot cars on the road, they’re encountering an obstacle very familiar to humans: Old Man Winter. Simple snow can render the most advanced computing power useless and leave vehicles dead on the highway. That’s why major players including Volvo Cars, owned by Zhejiang Geely Holding Group Co.; Google, a unit of Alphabet Inc.; and Ford Motor Co. are stepping up their efforts to prevent snow blindness.
Related Classifications: Visual Object Detection, Image Segmentation, Convolutional Neural Network
",,,,,"Robustness Failure, Covariate Shift","Snippet Text: As automakers race to get robot cars on the road, they’re encountering an obstacle very familiar to humans: Old Man Winter. Simple snow can render the most advanced computing power useless and leave vehicles dead on the highway. That’s why major players including Volvo Cars, owned by Zhejiang Geely Holding Group Co.; Google, a unit of Alphabet Inc.; and Ford Motor Co. are stepping up their efforts to prevent snow blindness.
Related Classifications: Robustness Failure, Covariate Shift
",,"Faulty or Inadequate Preprocessing, Incomplete Data Attribute Capture","Snippet Text: As automakers race to get robot cars on the road, they’re encountering an obstacle very familiar to humans: Old Man Winter. Simple snow can render the most advanced computing power useless and leave vehicles dead on the highway. That’s why major players including Volvo Cars, owned by Zhejiang Geely Holding Group Co.; Google, a unit of Alphabet Inc.; and Ford Motor Co. are stepping up their efforts to prevent snow blindness.
Related Classifications: Faulty or Inadequate Preprocessing
Snippet Discussion: Can preprocessing (e.g. brightness adjustment / image normalization) reduce snow blindness?
, Snippet Text: Winter makes this harder. Snow can shroud cameras and cover the lane lines they must see to keep a driverless car on course. Lidar also is limited because the light pulses it emits reflect off flakes, potentially confusing a curtain of falling snow with something to avoid, causing the vehicle to hit the brakes.
Related Classifications: Incomplete Data Attribute Capture
",
GMF,71,False,Autonomous Driving,"Snippet Text: An Apple Inc self-driving car was rear-ended while merging onto an expressway near the company's Silicon Valley headquarters this month, the company said in an accident report posted on Friday (local time) that confirmed the iPhone maker is still in the race to build autonomous vehicles.
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Visual Descriptors","Snippet Text: An Apple Inc self-driving car was rear-ended while merging onto an expressway near the company's Silicon Valley headquarters this month, the company said in an accident report posted on Friday (local time) that confirmed the iPhone maker is still in the race to build autonomous vehicles.
Related Classifications: Image Segmentation, Visual Object Detection, Visual Descriptors
",,Convolutional Neural Network,"Snippet Text: An Apple Inc self-driving car was rear-ended while merging onto an expressway near the company's Silicon Valley headquarters this month, the company said in an accident report posted on Friday (local time) that confirmed the iPhone maker is still in the race to build autonomous vehicles.
Related Classifications: Convolutional Neural Network
",,,,,"Generalization Failure, Context Misidentification, Limited Dataset","Snippet Text: 

On August 24, one of Apple's Lexus RX 450h self-driving test vehicles in ""autonomous mode"" was merging south on the Lawrence Expressway in Sunnyvale, California at less than 1.6kph when it was rear-ended by a 2016 Nissan Leaf going about 24kph, according to the report posted on the California Department of Motor Vehicles website.
Related Classifications: Generalization Failure
, Snippet Text: ""The Google AV test driver saw the bus approaching in the left-side mirror but believed the bus would stop or slow to allow the Google AV to continue."" 
Related Classifications: Context Misidentification
, Snippet Text: 'Our car was making an assumption about what the other car was going to do,' said Chris Urmson, head of Google's self-driving project, speaking at the SXSW festival in Austin.
Related Classifications: Context Misidentification
, Snippet Text: In the wake of that collision, Urmson said his team 'implemented 3,500 new tests to make sure this won't happen again.'
Related Classifications: Limited Dataset
",Generalization Failure if the incoming vehicle was failed to be detected.
GMF,73,False,Game Content Generation,"Snippet Text: While playing the popular augmented-reality game Pokémon Go in Long Beach, a city that is nearly 50% white, Aura Bogado made an unsettling discovery — there were far more PokéStops and Gyms, locations where people pick up virtual goods or battle one another, than in her predominantly minority neighborhood in Los Angeles.
Related Classifications: Game Content Generation
",,,,,Geolocation Data,"Snippet Text: Niantic CEO John Hanke told Rolling Stone that Pokémon Go uses the same locations for PokéStops as in its previous augmented-reality game, Ingress. In Ingress, players would submit locations based on where they wanted to put “portals,” or battle spots.
Related Classifications: Geolocation Data
",,Clustering,"Snippet Text: Niantic CEO John Hanke told Rolling Stone that Pokémon Go uses the same locations for PokéStops as in its previous augmented-reality game, Ingress. In Ingress, players would submit locations based on where they wanted to put “portals,” or battle spots.
Related Classifications: Clustering
",Clustering potentially useful to aggregate multiple local player pings to single active game areas.,"Dataset Imbalance, Inadequate Data Sampling","Snippet Text: The problem: The demographics of Ingress players — mostly white, young and English-speaking, according to informal surveys of the community in 2013 and 2014 — shaped how the game unfurled in the real world.
Related Classifications: Dataset Imbalance, Inadequate Data Sampling
",,"Distributional Bias, Underfitting, Misconfigured Threshold","Snippet Text: Urban Institute researchers found an average of 55 PokéStops in majority white neighborhoods and 19 in majority black neighborhoods. The Belleville News-Democrat found that pattern repeated itself in African-American sections of Detroit, Miami and Chicago.
Related Classifications: Distributional Bias
, Snippet Text: The problem: The demographics of Ingress players — mostly white, young and English-speaking, according to informal surveys of the community in 2013 and 2014 — shaped how the game unfurled in the real world.
Related Classifications: Distributional Bias, Underfitting, Misconfigured Threshold
Snippet Discussion: Misconfigured Threshold: Perhaps thresholding too harsh, leading to few established active game areas in underrepresented communities.
",
GMF,75,False,Content Search,"Snippet Text: A new lawsuit alleges that Google's search engine has an anti-Semitism problem.
Related Classifications: Content Search
",,,,,"Collaborative Filtering, Distributional Learning, Language Modeling, Keyword Filtering","Snippet Text: According to Google's website, its algorithm for the Google Instant autocomplete feature ""predicts and displays search queries based on other users' search activities and the contents of web pages indexed by Google.""
Related Classifications: Collaborative Filtering, Distributional Learning, Language Modeling
, Snippet Text: In addition, the search engine says it strives to ""reflect the diversity of content on the web (some good, some objectionable)"" and so has a narrow set of removal policies for pornography, violence, hate speech, etc. -- though not narrow enough for SOS Racisme, it seems.
Related Classifications: Keyword Filtering
",,Query Expansion,"Snippet Text: French anti-discrimination organization SOS Racisme, in association with the Union of Jewish Students of France, the Movement Against Racism and for Friendship Among Peoples and other organizations, is suing Google because its autocomplete feature suggests the word ""Jewish"" in searches involving certain public figures, including News Corporation chairman Rupert Murdoch and actor Jon Hamm, reports The Times of Israel.
Related Classifications: Language Modeling, Query Expansion
",,Distributional Bias,"Snippet Text: Local reports pointed out by The Hollywood Reporter explain that the plaintiffs contend users of Google in France and across the world are systematically confronted with the unsolicited association of the term ""Jew"" with prominent names in the world of politics, media, and business.
Related Classifications: Distributional Bias
",,,,
GMF,76,False,Face Recognition,"Snippet Text: Now a new investigation from Human Rights Watch has found that not only are children regularly added to CONARC, but the database also powers a live facial recognition system in Buenos Aires deployed by the city government. This makes the system likely the first known instance of its kind being used to hunt down kids suspected of criminal activity.


Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Visual Descriptors","Snippet Text: Now a new investigation from Human Rights Watch has found that not only are children regularly added to CONARC, but the database also powers a live facial recognition system in Buenos Aires deployed by the city government. This makes the system likely the first known instance of its kind being used to hunt down kids suspected of criminal activity.
Related Classifications: Face Detection, Visual Object Detection, Visual Descriptors
, Snippet Text: Now a new investigation from Human Rights Watch has found that not only are children regularly added to CONARC, but the database also powers a live facial recognition system in Buenos Aires deployed by the city government. This makes the system likely the first known instance of its kind being used to hunt down kids suspected of criminal activity.
",,,,,"Security Vulnerability, Lack of Safety Protocols, Inadequate Anonymization, Harmful Application, Misuse, Inadequate Verification, Data or Labelling Noise, Domain Adaptation Deficit","Snippet Text: But there are several things off about CONARC. For one, it’s a plain-text spreadsheet file without password protection, which can be readily found via Google Search and downloaded by anyone. For another, many of the alleged crimes, like petty theft, are not that serious—while others aren’t specified at all.
Related Classifications: Security Vulnerability, Lack of Safety Protocols, Inadequate Anonymization
, Snippet Text: Most alarming, however, is the age of the youngest alleged offender, identified only as M.G., who is cited for “crimes against persons (malicious)—serious injuries.” M.G. was apparently born on October 17, 2016, which means he’s a week shy of four years old.
Related Classifications: Harmful Application, Misuse
, Snippet Text: The system has since led to numerous false arrests (links in Spanish), which the police have no established protocol for handling. 
Related Classifications: Inadequate Verification, Data or Labelling Noise
, Snippet Text: On top of this, facial recognition systems, under even ideal laboratory conditions, are notoriously bad at handling children because they’re trained and tested primarily on adults. The Buenos Aires system is no different. According to official documents (link in Spanish), it was tested only on the adult faces of city government employees before procurement. Prior US government tests of the specific algorithm that it is believed to be using also suggest it performs worse by a factor of six on kids (ages 10 to 16) than adults (ages 24 to 40).
Related Classifications: Domain Adaptation Deficit
",,,,
GMF,77,False,Autonomous Drones,"Snippet Text: When a fight broke out recently in the parking lot of Salt Lake Park, a few miles south of downtown Los Angeles, Cogo Guebara did what seemed the most practical thing at the time: she ran over to the park’s police robot to push its emergency alert button.
Related Classifications: Autonomous Drones
",,,,,,,,"Face Detection, Visual Object Detection","Snippet Text: She thought maybe the robot, which stands about 5 feet tall and has “POLICE” emblazoned on its egg-shaped body, wanted a visual of her face, so she crouched down for the camera. It still didn’t work.
Related Classifications: Face Detection, Visual Object Detection
",,Task Mismatch,"Snippet Text: The robot’s alert button is not yet connected to the police department, said Cosme Lozano, chief of police of Huntington Park, a city just southeast of downtown Los Angeles. The calls are instead directed to Knightscope, the company that creates and leases the robots.

“That’s why we’re not advertising those features,” he said. “It’s a new program for us and were still developing some protocols… to be able to fully adopt the program.”
Related Classifications: Task Mismatch
, Snippet Text: Lozano said the department is facing some technical challenges incorporating the robot into the force, adding that it’s on “a trial basis for the city.” Once fully connected, the calls will go directly to the department’s dispatch center, he said.
Related Classifications: Task Mismatch
",,Malicious Marketing,"Snippet Text: The robot’s alert button is not yet connected to the police department, said Cosme Lozano, chief of police of Huntington Park, a city just southeast of downtown Los Angeles. The calls are instead directed to Knightscope, the company that creates and leases the robots.

“That’s why we’re not advertising those features,” he said. “It’s a new program for us and were still developing some protocols… to be able to fully adopt the program.”
Related Classifications: Malicious Marketing
",
GMF,100,False,Regulatory Monitoring,"Snippet Text: On 17 March 2021, the welfare office sends me an email asking for new information, following the latest reform in welfare benefits. Although I have to send a year’s worth of documents about my income, I comply with the request. I send them the documents as well as a written explanation of my situation.
Related Classifications: Regulatory Monitoring
",,,,,"Optical Character Recognition, Classification","Snippet Text: During my phone meeting on 26 March, my caseworker confirms that “the software” analyzed my file automatically, using “parameters”. 
Related Classifications: Optical Character Recognition
, Snippet Text: They use “automated controls” as well as “a statistical model known as ‘datamining’ (sic), which automatically targets risky cases”.
Related Classifications: Classification
",,Document Classification,"Snippet Text: During my phone meeting on 26 March, my caseworker confirms that “the software” analyzed my file automatically, using “parameters”.
Related Classifications: Document Classification
, Snippet Text: They use “automated controls” as well as “a statistical model known as ‘datamining’ (sic), which automatically targets risky cases”.
Related Classifications: Document Classification
",,,,,"Underspecification, Generalization Failure","Snippet Text: My case apparently became “too complex” when I informed them that I started work as a freelance.
Related Classifications: Underspecification, Generalization Failure
",
GMF,99,False,Automatic Skill Assessment,"Snippet Text: Major universities are using their students’ race, among other variables, to predict how likely they are to drop out of school. 
Related Classifications: Automatic Skill Assessment
",,,,,Regression,"Snippet Text: Major universities are using their students’ race, among other variables, to predict how likely they are to drop out of school. 
Related Classifications: Regression
",,"Tree-based Learning, Conditional Logic",,,Problematic Features,"Snippet Text: At least four out of seven schools from which The Markup obtained such documents incorporate race as a predictor, and two of those describe race as a “high impact predictor.” 
Related Classifications: Problematic Features
, Snippet Text: We found large disparities in how the software treats students of different races, and the disparity is particularly stark for Black students, who were deemed high risk at as much as quadruple the rate of their White peers.
Related Classifications: Problematic Features
",,"Faulty or Inadequate Preprocessing, Incomplete Data Attribute Capture, Lack of Transparency","Snippet Text: At least four out of seven schools from which The Markup obtained such documents incorporate race as a predictor, and two of those describe race as a “high impact predictor.” 
Related Classifications: Faulty or Inadequate Preprocessing
Snippet Discussion: Relevant if race features should / could be preprocessed out of the dataset.
, Snippet Text: Navigate’s racially influenced risk scores “reflect the underlying equity disparities that are already present on these campuses and have been for a long time,” Ed Venit, who manages student success research for EAB, wrote in an email.
Related Classifications: Incomplete Data Attribute Capture
Snippet Discussion: Race could be thus replaced by more representative input features, e.g. ones that reflect the education level the campus provides, amenities, funding, etc.
, Snippet Text: “I certainly haven’t had a lot of information from behind the proprietary algorithms,” Carolyn Bassett, associate provost for student success at UMass Amherst told The Markup.
Related Classifications: Lack of Transparency
",
GMF,98,False,Autonomous Drones,"Snippet Text: When the Police Department acquired a robotic dog last year, officials heralded the four-legged device as a futuristic tool that could go places that were too dangerous to send officers.
Related Classifications: Autonomous Drones
",,,,,"Visual Object Detection, Image Segmentation","Snippet Text: When the Police Department acquired a robotic dog last year, officials heralded the four-legged device as a futuristic tool that could go places that were too dangerous to send officers.
Related Classifications: Visual Object Detection, Image Segmentation
",,Convolutional Neural Network,"Snippet Text: When the Police Department acquired a robotic dog last year, officials heralded the four-legged device as a futuristic tool that could go places that were too dangerous to send officers.
Related Classifications: Convolutional Neural Network
",,Harmful Application,"Snippet Text: The Police Department will return the device earlier than planned after critics seized on it as a dystopian example of overly aggressive policing.
Related Classifications: Harmful Application
",,,,
GMF,97,False,Autonomous Driving,"Snippet Text: Though self-driving technology has undeniably come a long way in the last few years, we're never far from a reminder that the technology also still has a long way to go.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Convolutional Neural Network","Snippet Text: Last week, we reported on the rollout of Tesla's Full Self-Driving Beta to a select few Tesla owners.
Related Classifications: Visual Object Detection, Image Segmentation, Convolutional Neural Network
",,,,,Generalization Failure,"Snippet Text: Redditor cyntrex posted their video to the subreddit r/teslamotors. The video shows the view from the inside of a stationary Model 3. The car's Autopilot system is registering traffic lights changing intermittently from red to yellow, and back to red.

The person recording the video then shifts the camera angle to show us what is outside, just in front of the car. Two vertical flags with the word ""coop"" written in bold red and orange letters — shown in the image above — seem to be confusing the Tesla Autopilot system.

As the flag waves in the wind, the system changes from red to yellow lights as it reads the different c
Related Classifications: Generalization Failure
",,"Limited Dataset, Inadequate Data Augmentation","Snippet Text: Redditor cyntrex posted their video to the subreddit r/teslamotors. The video shows the view from the inside of a stationary Model 3. The car's Autopilot system is registering traffic lights changing intermittently from red to yellow, and back to red.

The person recording the video then shifts the camera angle to show us what is outside, just in front of the car. Two vertical flags with the word ""coop"" written in bold red and orange letters — shown in the image above — seem to be confusing the Tesla Autopilot system.

As the flag waves in the wind, the system changes from red to yellow lights as it reads the different c
Related Classifications: Limited Dataset, Inadequate Data Augmentation
",
GMF,96,False,Automatic Skill Assessment,"Snippet Text: A proprietary system that measures teacher performance based on student test scores may violate teachers’ civil rights because they can’t verify the results are accurate, a federal judge ruled, advancing a lawsuit against Texas’ largest school district.
Related Classifications: Automatic Skill Assessment
",,,,,,,,"Regression, Conditional Logic","Snippet Text: A proprietary system that measures teacher performance based on student test scores may violate teachers’ civil rights because they can’t verify the results are accurate, a federal judge ruled, advancing a lawsuit against Texas’ largest school district.
Related Classifications: Regression, Conditional Logic
",,"Lack of Transparency, Lack of Explainability","Snippet Text: That detail is lacking in the SAS Institute’s software because the company deems its algorithms as trade secrets and refuses to share them Houston ISD or its teachers, so teachers have no way of knowing if an error in the program has decreased their scores.
Related Classifications: Lack of Transparency, Lack of Explainability
",,"Data or Labelling Noise, Software Bug","Snippet Text: “The EVAAS score might be erroneously calculated for any number of reasons, ranging from data-entry mistakes to glitches in the computer code itself. Algorithms are human creations, and subject to error like any other human endeavor,” Smith wrote.
Related Classifications: Data or Labelling Noise, Software Bug
",
GMF,95,False,Automatic Skill Assessment,"Snippet Text: Designed by the recruiting-technology firm HireVue, the system uses candidates’ computer or cellphone cameras to analyze their facial movements, word choice and speaking voice before ranking them against other applicants based on an automatically generated “employability” score.
Related Classifications: Automatic Skill Assessment
",,,,,"Face Detection, Visual Object Detection, Regression","Snippet Text: Designed by the recruiting-technology firm HireVue, the system uses candidates’ computer or cellphone cameras to analyze their facial movements, word choice and speaking voice before ranking them against other applicants based on an automatically generated “employability” score.
Related Classifications: Face Detection, Visual Object Detection, Regression
",,,,,"Generalization Failure, Lack of Explainability, Lack of Transparency","Snippet Text: The system, they argue, will assume a critical role in helping decide a person’s career. But they doubt it even knows what it’s looking for: Just what does the perfect employee look and sound like, anyway?
Related Classifications: Generalization Failure
, Snippet Text: Larsen compared algorithms’ ability to boost hiring outcomes with medicine’s improvement of health outcomes and said the science backed him up. The system, he argued, is still more objective than the flawed metrics used by human recruiters, whose thinking he called the “ultimate black box.”
Related Classifications: Lack of Explainability
, Snippet Text: Job candidates aren’t told their score or what little things they got wrong, and they can’t ask the machine what they could do better. Human hiring managers can use other factors, beyond the HireVue score, to decide which candidates pass the first-round test.
Related Classifications: Lack of Transparency
",,"Overfitting, Distributional Bias","Snippet Text: The system, they argue, will assume a critical role in helping decide a person’s career. But they doubt it even knows what it’s looking for: Just what does the perfect employee look and sound like, anyway?
Related Classifications: Overfitting
, Snippet Text: To train the system on what to look for and tailor the test to a specific job, the employer’s current workers filling the same job — “the entire spectrum, from high to low achievers” — sit through the AI assessment, Larsen said.
Related Classifications: Overfitting
, Snippet Text: To train the system on what to look for and tailor the test to a specific job, the employer’s current workers filling the same job — “the entire spectrum, from high to low achievers” — sit through the AI assessment, Larsen said.
Related Classifications: Distributional Bias
",
GMF,94,False,Automatic Skill Assessment,"Snippet Text: The Bologna court ruled that a reputational-ranking algorithm used by on-demand food delivery platform Deliveroo discriminated against gigging delivery workers by breaching local labor laws.
Related Classifications: Automatic Skill Assessment
",,,,,,,,"Conditional Logic, Regression","Snippet Text: According to the ordinance, if a rider failed to cancel a shift pre-booked through the app at least 24 hours before its start, their “reliability index” would be negatively affected. 
Related Classifications: Conditional Logic, Regression
, Snippet Text: According to the court, the algorithm’s failure to take into account the reasons behind a cancellation amounts to discrimation and unjustly penalizes riders with legally legitimate reasons for not working. 
Related Classifications: Conditional Logic, Regression
",,,,,"Faulty or Inadequate Preprocessing, Harmful Application","Snippet Text: The ruling, reported earlier in the Italian press, found Deliveroo’s ranking algorithm discriminated against delivery couriers because it did not distinguish between legally protected reasons for withholding labour — namely not working because a rider was sick; or exercising their protected right to strike — and more trivial reasons for not being as productive as they’d indicated they would be.
Related Classifications: Faulty or Inadequate Preprocessing
, Snippet Text: The on-demand delivery app has faced down a number of legal challenges on home turf — related to its classification of gig workers (as self employed couriers) and its opposition to collective bargaining rights for riders.
Related Classifications: Harmful Application
",
GMF,93,False,Ad Delivery,"Snippet Text: The charge follows a months-long investigation by HUD into whether Facebook illegally allows real estate sellers to restrict their advertisements by characteristics such as race.
Related Classifications: Ad Delivery
",,,,,,,,"Collaborative Filtering, Clustering","Snippet Text: ""Facebook is discriminating against people based upon who they are and where they live,” HUD Secretary Ben Carson said in a statement.
Related Classifications: Collaborative Filtering
, Snippet Text: According to HUD, Facebook allowed advertises to exclude people from seeing housing advertisements based on interests that ""closely align with the Fair Housing Act’s protected classes,"" including users who Facebook classified as non-American-born, non-Christian, interested in accessibility or who were interested in Hispanic culture, in addition to other groups.
Related Classifications: Collaborative Filtering, Clustering
",,,,,"Distributional Bias, Harmful Application","Snippet Text: ""Facebook is discriminating against people based upon who they are and where they live,” HUD Secretary Ben Carson said in a statement.
Related Classifications: Distributional Bias, Harmful Application
, Snippet Text: According to HUD, Facebook allowed advertises to exclude people from seeing housing advertisements based on interests that ""closely align with the Fair Housing Act’s protected classes,"" including users who Facebook classified as non-American-born, non-Christian, interested in accessibility or who were interested in Hispanic culture, in addition to other groups.
Related Classifications: Distributional Bias, Harmful Application
",
GMF,92,False,Underwriting,"Snippet Text: What started with a viral Twitter thread metastasized into a regulatory investigation of Goldman Sachs’ credit card practices after a prominent software developer called attention to differences in Apple Card credit lines for male and female customers.
Related Classifications: Underwriting
",,,,,,,,"Regression, Classification, Conditional Logic, Tree-based Learning","Snippet Text: David Heinemeier Hansson, a Danish entrepreneur and developer, said in tweets last week that his wife, Jamie Hansson, was denied a credit line increase for the Apple Card, despite having a higher credit score than him.
Related Classifications: Regression, Classification, Conditional Logic, Tree-based Learning
",,,,,"Distributional Bias, Lack of Transparency, Hardcoding, Faulty or Inadequate Preprocessing","Snippet Text: The algorithms — formulas for processing information or completing tasks — that make these judgments are programmed by people and thus often reproduce human biases, unintentionally or otherwise, resulting in less favorable outcomes for women and people of color. 
Related Classifications: Distributional Bias, Faulty or Inadequate Preprocessing
, Snippet Text: But the public, and even companies themselves, often have little visibility into how algorithms operate.
Related Classifications: Lack of Transparency
, Snippet Text: “Women tend to be better credit risks. While it is illegal to discriminate the data indicates that controlling for income, and other things, women are better credit risks,” said Aaron Klein, a Brookings Institution fellow. “So giving men better terms of credit is both illegal and seems to be inconsistent with international experience.”
Related Classifications: Hardcoding
",
GMF,91,False,Resource Allocation,"Snippet Text: Stanford Medicine residents who work in close contact with COVID-19 patients were left out of the first wave of staff members for the new Pfizer vaccine.
Related Classifications: Resource Allocation
",,,,,,,,"Classification, Combinatorial Optimization","Snippet Text: An algorithm chose who would be the first 5,000 in line. 
Related Classifications: Classification, Combinatorial Optimization
",,,,,"Incomplete Data Attribute Capture, Problematic Features, Hardcoding, Lack of Transparency","Snippet Text: The residents said they were told they were at a disadvantage because they did not have an assigned “location” to plug into the calculation and because they are young, according to an email sent by a chief resident to his peers. Residents are the lowest-ranking doctors in a hospital. Stanford Medicine has about 1,300 across all disciplines.
Related Classifications: Incomplete Data Attribute Capture, Problematic Features, Hardcoding
, Snippet Text: Only seven made the priority vaccination list, despite the fact that this week, residents were asked to volunteer for ICU coverage in anticipation of a surge in COVID-19 cases.
Related Classifications: Hardcoding
",
GMF,89,False,Content Recommendation,"Snippet Text: wired.com · 2020

Edit

YOUTUBE, FACEBOOK, AND other social media platforms were instrumental in radicalizing the terrorist who killed 51 worshippers in a March 2019 attack on two New Zealand mosques, according to a new report from the country’s government.
Related Classifications: Content Recommendation
",,,,,"Content-based Filtering, Collaborative Filtering","Snippet Text: wired.com · 2020

Edit

YOUTUBE, FACEBOOK, AND other social media platforms were instrumental in radicalizing the terrorist who killed 51 worshippers in a March 2019 attack on two New Zealand mosques, according to a new report from the country’s government.
Related Classifications: Content-based Filtering, Collaborative Filtering
",,,,,"Unsafe Exposure or Access, Misinformation Generation Hazard","Snippet Text: A damning 2018 report by Stanford researcher and PhD candidate Becca Lewis describes the alternative media system on YouTube that fed young viewers far-right propaganda. 
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
",,,,
GMF,88,False,Content Search,"Snippet Text: The Network Contagion Research Institute, which studies the way hate speech spreads online, located a series of posts on the 4chan message board, dating back to 2017, that purposefully pair images of ovens on wheels with the term “Jewish baby stroller.” There were at least a dozen such images turned up in one search, dating from August and September 2017. 
Related Classifications: Content Search
",,,,,Content-based Filtering,"Snippet Text: The Network Contagion Research Institute, which studies the way hate speech spreads online, located a series of posts on the 4chan message board, dating back to 2017, that purposefully pair images of ovens on wheels with the term “Jewish baby stroller.” There were at least a dozen such images turned up in one search, dating from August and September 2017. 
Related Classifications: Content-based Filtering
",,Vector Search,,,"Unsafe Exposure or Access, Context Misidentification, Faulty or Inadequate Preprocessing, Gaming Vulnerability","Snippet Text: The Network Contagion Research Institute, which studies the way hate speech spreads online, located a series of posts on the 4chan message board, dating back to 2017, that purposefully pair images of ovens on wheels with the term “Jewish baby stroller.” There were at least a dozen such images turned up in one search, dating from August and September 2017. 
Related Classifications: Unsafe Exposure or Access, Context Misidentification
, Snippet Text: “It’s either a raid from 4chan trolls or it’s a meme that circulated on the web,” said Alex Goldenberg, the institute’s lead intelligence analyst. “The Google search algorithm is driving it to the top for some reason, or the item in the meme is tricking the Google algorithm.”
Related Classifications: Context Misidentification
, Snippet Text: If it was a coordinated action by online anti-Semites, called a “raid,” it wouldn’t be the first one. In a 2016 “raid” called Operation Google, extremists tried to undermine a new tool Google had for spotting and filtering out racial and ethnic slurs. 
Related Classifications: Faulty or Inadequate Preprocessing
, Snippet Text: They did this by replacing the slurs in their comments with the names of tech companies. So, for example, they used the word “Google” instead of the n-word, and used the word “Skype” to refer to Jews. They hoped that doing that would force Google to censor its own name, which did not happen.
Related Classifications: Gaming Vulnerability
",,,,
GMF,87,False,Image Verification,"Snippet Text: Women with darker skin are more than twice as likely to be told their photos fail UK passport rules when they submit them online than lighter-skinned men, according to a BBC investigation.
Related Classifications: Image Verification
",,,,,"Image Classification, Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: The passport application website uses an automated check to detect poor quality photos which do not meet Home Office rules. These include having a neutral expression, a closed mouth and looking straight at the camera.
Related Classifications: Image Classification, Face Detection, Visual Object Detection, Image Segmentation
",,,,,"Generalization Failure, Distributional Bias, Limited Dataset, Dataset Imbalance","Snippet Text: Women with darker skin are more than twice as likely to be told their photos fail UK passport rules when they submit them online than lighter-skinned men, according to a BBC investigation.
Related Classifications: Generalization Failure, Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: ""The impact of automated systems on ethnic minority communities is regularly overlooked, with detrimental consequences.""
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
, Snippet Text: So a training dataset with less representation of women and people of colour will produce a system that doesn't work well for those groups.
Related Classifications: Distributional Bias, Limited Dataset, Dataset Imbalance
",,,,
GMF,86,False,Automatic Skill Assessment,"Snippet Text: This week it emerged that a problem was discovered with the Leaving Certificate calculated grades system which means thousands of students will have their results upgraded.
Related Classifications: Automatic Skill Assessment
",,,,,Conditional Logic,"Snippet Text: First, the code substituted a student’s worst two subjects for their best two subjects. Then it wrongly added a subject into the equation - the results of the Junior Cycle’s Civic, Social and Political Education. This shouldn’t have been counted.
Related Classifications: Conditional Logic
",,,,,"Software Bug, Lack of Transparency","Snippet Text: We know that the code wasn’t sufficiently tested, which is normally a crucial part of any software release. Department officials say that there simply wasn’t enough time to test everything thoroughly due to the urgency of the situation and the resourcing constraints. They emphasised that this wasn’t a software package already being used elsewhere. It was custom-built for the particulars of our situation.
Related Classifications: Software Bug
, Snippet Text: We don’t. The code - and the implementation of the algorithms - aren’t available to check. In other words, they’re not ‘open source’ or reviewable in the way that, for example, the Irish Covid-19 Tracker smartphone app code is.
Related Classifications: Lack of Transparency
",,,,
GMF,84,False,Fact Checking,"Snippet Text: Something as simple as changing the font of a message or cropping an image can be all it takes to bypass Facebook's defenses against hoaxes and lies.
Related Classifications: Fact Checking
",,,,,Classification,"Snippet Text: Something as simple as changing the font of a message or cropping an image can be all it takes to bypass Facebook's defenses against hoaxes and lies.
Related Classifications: Classification
",,Convolutional Neural Network,"Snippet Text: Something as simple as changing the font of a message or cropping an image can be all it takes to bypass Facebook's defenses against hoaxes and lies.
Related Classifications: Convolutional Neural Network
",,Generalization Failure,"Snippet Text: But Facebook did not apply the same label to versions of the message that were nearly identical, except with different backgrounds or croppings.
Related Classifications: Generalization Failure
",,"Overfitting, Inadequate Data Augmentation, Inadequate Verification","Snippet Text: But Facebook did not apply the same label to versions of the message that were nearly identical, except with different backgrounds or croppings.
Related Classifications: Overfitting, Inadequate Data Augmentation
, Snippet Text: A meme claiming mail-in ballots need two stamps was labeled ""partly false."" But Facebook did not label other versions of the same message — versions in which the user had changed the font or background color, or written out the text of the meme instead of posting it as an image. 
Related Classifications: Overfitting, Inadequate Data Augmentation, Inadequate Verification
",
GMF,83,False,Spam Filtering,"Snippet Text: An experiment reveals that Microsoft Outlook marks messages as spam on the basis of a single word, such as “Nigeria”. Spam filters are largely unaudited and could discriminate unfairly.
Related Classifications: Spam Filtering
",,,,,Keyword Filtering,"Snippet Text: An internship application from a Nigerian student. The same email with the word “Nigeria” removed was delivered to the inbox.
Related Classifications: Keyword Filtering
",,"Language Modeling, Classification","Snippet Text: An internship application from a Nigerian student. The same email with the word “Nigeria” removed was delivered to the inbox.
Related Classifications: 
, Snippet Text: Microsoft declined to comment. It is unlikely that an Outlook engineer made an explicit rule to mark any message that contains “Nigeria” as spam. Instead, a machine learning algorithm probably identified “Nigeria” as a strong discriminator between spam and non-spam messages. Microsoft does not make the training data set of its spam filter available to researchers.
Related Classifications: Classification
",,"Lack of Transparency, Hardcoding, Context Misidentification","Snippet Text: Microsoft declined to comment. It is unlikely that an Outlook engineer made an explicit rule to mark any message that contains “Nigeria” as spam. Instead, a machine learning algorithm probably identified “Nigeria” as a strong discriminator between spam and non-spam messages. Microsoft does not make the training data set of its spam filter available to researchers.
Related Classifications: Lack of Transparency
, Snippet Text: While SpamAssassin’s rules change daily, its default configuration files single out words like “Ivory Coast”, “Nigeria” or “Nigerian government” as spammy. The phrase “Oprah!”, an African-American entertainer, is listed as potentially spammy, though the rule is currently inactive.
Related Classifications: Hardcoding, Context Misidentification
",,,,
GMF,82,False,Fact Checking,"Snippet Text: On Wednesday, October 21, 2020, several content containing images related to the unfortunate incident that occurred at the Lekki toll gate in Lagos, Nigeria on Tuesday were flagged as misinformation on Facebook and Instagram.
Related Classifications: Fact Checking
",,,,,,,,Classification,"Snippet Text: Facebook uses a hybrid system of human moderators and Artificial Intelligence (AI) to check misinformation. It partners with certified independent third-party fact-checking organisations — over 27 partners across 88 countries — to identify, review, and confirm potentially inaccurate content to curb viral misinformation. These content are usually first flagged as inaccurate by feedback from users, Facebook’s signal technology, or fact-checkers.
Related Classifications: Classification
",,Inadequate Verification,"Snippet Text: On Instagram and Facebook, images of Lekki Concession Centre (LCC) staff that allegedly came to uninstall the CCTV camera from the scene a few hours to the sad incident, protesters staying together while holding the Nigeria flag, a bloodied Nigeria flag, reported survivors at the hospital, and corpses from the scene are all flagged false information; invariably suggesting that the October 20 military attack was fake.
Related Classifications: Inadequate Verification
",,Data or Labelling Noise,"Snippet Text: Of course, these fact-checking firms are not without their accusation of bias. Besides, Facebook has yet to completely figure out what ‘inaccurate information’ means. 
Related Classifications: Data or Labelling Noise
Snippet Discussion: Fact-checking firm bias may introduce labelling noise.
",
GMF,81,False,Image Tagging,"Snippet Text: Google and startups like Qure.ai, Aidoc, and DarwinAI are developing AI and machine learning systems that classify chest X-rays to help identify conditions like fractures and collapsed lungs. 
Related Classifications: Image Tagging
",,,,,Image Classification,"Snippet Text: Google and startups like Qure.ai, Aidoc, and DarwinAI are developing AI and machine learning systems that classify chest X-rays to help identify conditions like fractures and collapsed lungs. 
Related Classifications: Image Classification
, Snippet Text: After feeding the classifiers the datasets to demonstrate they reached near-state-of-the-art classification performance, which ruled out the possibility that any disparities simply reflected poor overall performance, the researchers calculated and identified disparities across the labels, datasets, and attributes. They found that all four datasets contained “meaningful” patterns of bias and imbalance, with female patients suffering from the highest disparity despite the fact the proportion of women was only slightly less than men.
Related Classifications: Image Classification
",,Convolutional Neural Network,"Snippet Text: Google and startups like Qure.ai, Aidoc, and DarwinAI are developing AI and machine learning systems that classify chest X-rays to help identify conditions like fractures and collapsed lungs. 
Related Classifications: Convolutional Neural Network
",,"Distributional Bias, Limited Dataset","Snippet Text: A team of U.K. scientists found that almost all eye disease datasets come from patients in North America, Europe, and China, meaning eye disease-diagnosing algorithms are less certain to work well for racial groups from underrepresented countries.
Related Classifications: Distributional Bias, Limited Dataset
, Snippet Text: And a growing body of work suggests that skin cancer-detecting algorithms tend to be less precise when used on Black patients, in part because AI models are trained mostly on images of light-skinned patients.
Related Classifications: Distributional Bias, Limited Dataset
, Snippet Text: After feeding the classifiers the datasets to demonstrate they reached near-state-of-the-art classification performance, which ruled out the possibility that any disparities simply reflected poor overall performance, the researchers calculated and identified disparities across the labels, datasets, and attributes. They found that all four datasets contained “meaningful” patterns of bias and imbalance, with female patients suffering from the highest disparity despite the fact the proportion of women was only slightly less than men.
Related Classifications: Distributional Bias, Limited Dataset
",,,,
GMF,80,False,Camera Tracking,"Snippet Text: This happened over the weekend in a soccer game in Scotland, when an AI-controlled camera got confused, and thought a lineman’s bald head was the ball.
Related Classifications: Camera Tracking
",,,,,Visual Object Detection,"Snippet Text: This happened over the weekend in a soccer game in Scotland, when an AI-controlled camera got confused, and thought a lineman’s bald head was the ball.
Related Classifications: Visual Object Detection
",,,,,Generalization Failure,"Snippet Text: This happened over the weekend in a soccer game in Scotland, when an AI-controlled camera got confused, and thought a lineman’s bald head was the ball.
Related Classifications: Generalization Failure
",,,,
GMF,79,False,Medical Diagnosis Support,"Snippet Text: The test — which measures what’s known as estimated glomerular filtration rate, or eGFR — has historically considered four factors: age, gender, race, and levels of creatinine — the waste that kidneys filter out of blood.
Related Classifications: Medical Diagnosis Support
",,,,,,,,Regression,"Snippet Text: The test — which measures what’s known as estimated glomerular filtration rate, or eGFR — has historically considered four factors: age, gender, race, and levels of creatinine — the waste that kidneys filter out of blood.
Related Classifications: Regression
",,"Algorithmic Bias, Problematic Features","Snippet Text: But the race of a patient can only be bucketed into two groups: Black, or not Black. That’s based on a flawed assumption that dates back to the formula’s creation, when medical experts presumed that Black people have higher muscle mass on average, leading to higher kidney function
Related Classifications: Algorithmic Bias, Problematic Features
",,,,
GMF,78,False,Application Evaluation,"Snippet Text: EIGHTEEN-YEAR-OLD ANAHITA NAGPAL fears her plans to start training this fall to be a doctor have been derailed by a statistical model.
Related Classifications: Application Evaluation
",,,,,,,,Regression,"Snippet Text: Nagpal, who lives in Göttingen, Germany, had been offered a premed place and scholarship at NYU. Her acceptance was dependent on her results in the International Baccalaureate diploma, a two-year high school program recognized by colleges and taken by more than 170,000 students this year, most in the US. But she scored more poorly than expected.
Related Classifications: Regression
",,"Lack of Transparency, Faulty Interface or Instructions","Snippet Text: One math teacher at a school in the Middle East says IB should disclose the full workings of its model for outside scrutiny. He and a colleague with a math PhD have been puzzling over its design since several students lost scholarships to top universities, after receiving results much lower than expected by their teachers.
Related Classifications: Lack of Transparency
, Snippet Text:  IB did not disclose details of the methodology but said grades would be calculated based on a student’s assignment scores, predicted grades, and historical IB results from their school.
Related Classifications: Lack of Transparency
, Snippet Text: Students at the school submitted their assignments before IB said those assignments would help steer the grading model. Some IB students at other schools had not yet submitted those assignments, allowing them to put in extra effort, aided by knowing they didn’t have to prepare for exams. 
Related Classifications: Faulty Interface or Instructions
",,"Incomplete Data Attribute Capture, Software Bug, Generalization Failure","Snippet Text: One visual arts teacher at a US school says what she and coworkers have seen suggests it wasn’t well tailored. “When I saw the marks, I was floored,” she says. “I am always conservative in my predicted grades, but every single student except one were downgraded.”
Related Classifications: Incomplete Data Attribute Capture, Software Bug, Generalization Failure
, Snippet Text: Constance Lavergne, whose son in the UK received lower-than-expected IB grades and missed out on his preferred college, is one of many parents struggling to understand what happened. She says her experience working closely with data analysts in the tech industry makes her suspicious of IB’s methodology. It would naturally generate noisier results for smaller classes, like her son’s, because they offer fewer past data points, she suggests. “There’s something wrong with the algorithm,” Lavergne says.
Related Classifications: Software Bug
",
GMF,101,False,Regulatory Monitoring,"Snippet Text: From 2013 on (though these techniques could have been used earlier), authorities used algorithms to create risk profiles of residents who were supposedly more likely to commit fraud and then used automated systems with little oversight to scan through benefits applicants and flag likely fraudsters who were then forced to pay money they didn’t owe in reality.
Related Classifications: Regulatory Monitoring
",,,,,Classification,"Snippet Text: From 2013 on (though these techniques could have been used earlier), authorities used algorithms to create risk profiles of residents who were supposedly more likely to commit fraud and then used automated systems with little oversight to scan through benefits applicants and flag likely fraudsters who were then forced to pay money they didn’t owe in reality.
Related Classifications: Classification
, Snippet Text: 
",,Tree-based Learning,"Snippet Text: An investigation from the Dutch Data Protection Authority found that these algorithms were inherently discriminatory because they took variables such as whether someone had a second nationality into account.
Related Classifications: Tree-based Learning
",,"Algorithmic Bias, Problematic Features, Harmful Application, Lack of Safety Protocols","Snippet Text: An investigation from the Dutch Data Protection Authority found that these algorithms were inherently discriminatory because they took variables such as whether someone had a second nationality into account.
Related Classifications: Algorithmic Bias, Problematic Features
, Snippet Text: Authorities penalized families over a mere suspicion of fraud based on the system’s risk indicators. Tens of thousands of families — often with lower incomes or belonging to ethnic minorities — were pushed into poverty because of exorbitant debts to the tax agency. 
Related Classifications: Harmful Application
, Snippet Text: “There was a total lack of checks and balances within every organization of making sure people realize what was going on,” said Pieter Omtzigt, an independent member of the Dutch parliament who played a pivotal role in uncovering the scandal and grilling the tax authorities. 
Related Classifications: Lack of Safety Protocols
",,Distributional Bias,"Snippet Text: An investigation from the Dutch Data Protection Authority found that these algorithms were inherently discriminatory because they took variables such as whether someone had a second nationality into account.
Related Classifications: Distributional Bias
",
GMF,102,False,AI Voice Assistant,,,,,,Automatic Speech Recognition,"Snippet Text: Speech recognition systems have more trouble understanding black users’ voices than those of white users, according to a new Stanford study.
Related Classifications: Automatic Speech Recognition
",,"Convolutional Neural Network, Recurrent Neural Network, Transformer","Snippet Text: Here, we examine the ability of five state-of-the-art ASR systems—developed by Amazon, Apple, Google, IBM, and Microsoft—to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. 
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Transformer
",,Algorithmic Bias,"Snippet Text: Speech recognition systems have more trouble understanding black users’ voices than those of white users, according to a new Stanford study.
Related Classifications: Algorithmic Bias
",,Distributional Bias,"Snippet Text: The researchers used voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft to transcribe interviews with 42 white people and 73 black people, all of which took place in the US. The tools misidentified words about 19 percent of the time during the interviews with white people and 35 percent of the time during the interviews with black people. The system found 2 percent of audio snippets from white people to be unreadable, compared to 20 percent of those from black people. The errors were particularly large for black men, with an error rate of 41 percent compared to 30 percent for black women.
Related Classifications: Distributional Bias
, Snippet Text: Here, we examine the ability of five state-of-the-art ASR systems—developed by Amazon, Apple, Google, IBM, and Microsoft—to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. 
Related Classifications: Distributional Bias
Snippet Discussion: Study is from 2020, distributional methods most likely used.
",
GMF,104,False,Resource Allocation,"Snippet Text: A few weeks later, the state announced that Blue Shield would build an algorithm allocating vaccines based on ZIP codes rather than census tracts—the generally smaller, census-based areas that the Healthy Places Index scores with health outcomes.
Related Classifications: Resource Allocation
",,,,,,"Snippet Text: A few weeks later, the state announced that Blue Shield would build an algorithm allocating vaccines based on ZIP codes rather than census tracts—the generally smaller, census-based areas that the Healthy Places Index scores with health outcomes.
",,Regression,,,"Algorithmic Bias, Problematic Features","Snippet Text: Our analysis shows that using ZIP codes rather than census tracks in the algorithm could transform which communities get additional vaccine supply, potentially undermining equity and access for many vulnerable communities. 
Related Classifications: Algorithmic Bias, Problematic Features
, Snippet Text: For these excluded communities, familiar disparities reveal themselves. The census tracts that, if the state’s plans are carried out, will not receive additional vaccine supply are disproportionately communities of color. Statewide, Latinx people make up approximately 38% of the population, but Latinx people make up 53% of the population in the census tracts that may be omitted from the state’s equity focus. The same is true of Black populations, making up 6% statewide but 8% of the census tracts potentially left behind.
Related Classifications: Algorithmic Bias
",,,,
GMF,105,False,Autonomous Driving,"Snippet Text: A six-second video captured by the Tesla and data it recorded show that neither Autopilot — Tesla’s much-vaunted system that can steer, brake and accelerate a car on its own — nor the driver slowed the vehicle until a fraction of a second before the crash. 
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Convolutional Neural Network","Snippet Text: A six-second video captured by the Tesla and data it recorded show that neither Autopilot — Tesla’s much-vaunted system that can steer, brake and accelerate a car on its own — nor the driver slowed the vehicle until a fraction of a second before the crash. 
Related Classifications: Image Segmentation, Visual Object Detection, Convolutional Neural Network
",,,,,Latency Issues,"Snippet Text: A six-second video captured by the Tesla and data it recorded show that neither Autopilot — Tesla’s much-vaunted system that can steer, brake and accelerate a car on its own — nor the driver slowed the vehicle until a fraction of a second before the crash. 
Related Classifications: Latency Issues
, Snippet Text: Autopilot does not track drivers’ eyes and monitors only if their hands are on the steering wheel. The system sometimes continues operating even if drivers have their hands on the steering wheel for only a few seconds at a time.
Related Classifications: Latency Issues
",,"Misuse, Lack of Safety Protocols, Gaming Vulnerability","Snippet Text: But with little to do other than look straight ahead, some drivers seem unable to resist the temptation to let their attention wander while Autopilot is on. 
Related Classifications: Misuse
, Snippet Text: But the National Transportation Safety Board, which has completed investigations into accidents involving Autopilot, has said the system lacks safeguards to prevent misuse and does not effectively monitor drivers.
Related Classifications: Lack of Safety Protocols
, Snippet Text: “This monitoring system is fundamentally weak because it’s easy to cheat and doesn’t monitor very consistently,” said Raj Rajkumar, a professor at Carnegie Mellon University who focuses on autonomous driving technology.
Related Classifications: Gaming Vulnerability
",
GMF,107,False,Face Recognition,"Snippet Text: The cloud computing unit of Alibaba, Alibaba Cloud, developed a facial recognition algorithm that can identify a person’s ethnicity or whether a person is “Uyghur”, according to research from surveillance industry publication IPVM.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: The cloud computing unit of Alibaba, Alibaba Cloud, developed a facial recognition algorithm that can identify a person’s ethnicity or whether a person is “Uyghur”, according to research from surveillance industry publication IPVM.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,,,,"Harmful Application, Unsafe Exposure or Access","Snippet Text: Alibaba said in a statement that it is “dismayed” to learn that Alibaba Cloud tested a technology that included “ethnicity as an algorithm” and that “racial or ethnic discrimination or profiling in any form violates Alibaba’s policies and values.”
Related Classifications: Harmful Application
, Snippet Text: “We never intended our technology to be used for and will not permit it to be used for targeting specific ethnic groups, and we have eliminated any ethnic tag in our product offering.
Related Classifications: Harmful Application, Unsafe Exposure or Access
",,,,
GMF,108,False,Face Recognition,"Snippet Text: The company’s facial recognition software serves police in the US, Australia, and France. Idemia software checks the faces of some cruise ship passengers landing in the US against Customs and Border Protection records.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: The company’s facial recognition software serves police in the US, Australia, and France. Idemia software checks the faces of some cruise ship passengers landing in the US against Customs and Border Protection records.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,,,,"Dataset Imbalance, Generalization Failure","Snippet Text: “White males ... is the demographic that usually gives the lowest FMR,” or false match rate, the report states. “Black females ... is the demographic that usually gives the highest FMR.” NIST plans a detailed report this fall on how the technology works on different demographic groups.
Related Classifications: Dataset Imbalance
, Snippet Text: Juliea and her husband Derrick are considering legal action against a Livonia skating rink after their daughter Lamya was misidentified by the business's facial recognition technology.
Related Classifications: Generalization Failure
",,"Limited Dataset, Overfitting","Snippet Text: Juliea and her husband Derrick are considering legal action against a Livonia skating rink after their daughter Lamya was misidentified by the business's facial recognition technology.
Related Classifications: Limited Dataset, Overfitting
",
GMF,109,False,Face Recognition,"Snippet Text: The facial recognition site PimEyes is one of the most capable face-searching tools on the planet. In less than a second, it can scan through more than 900 million images from across the Internet and find matches with startling accuracy
Related Classifications: Face Recognition
",,,,,"Face Detection, Image Segmentation, Visual Object Detection","Snippet Text: The facial recognition site PimEyes is one of the most capable face-searching tools on the planet. In less than a second, it can scan through more than 900 million images from across the Internet and find matches with startling accuracy
Related Classifications: Face Detection, Image Segmentation, Visual Object Detection
",,,,,"Lack of Safety Protocols, Unsafe Exposure or Access","Snippet Text: The company has no other rules in place to prevent anyone from scouring the Web for someone else.
Related Classifications: Lack of Safety Protocols, Unsafe Exposure or Access
",,,,
GMF,110,False,Resource Allocation,"Snippet Text: There, under a Medicaid waiver program, assessors interviewed beneficiaries and decided how frequently the caretaker should visit.
Related Classifications: Resource Allocation
, Snippet Text: When the assessor entered Dobbs’ information into the computer, it ran through an algorithm that the state had recently approved, determining how many hours of help she would receive.
Related Classifications: Resource Allocation
",,,,,"Tree-based Learning, Conditional Logic","Snippet Text: The algorithm computes about 60 descriptions, symptoms, and ailments — fever, weight loss, ventilator use — into categories, each one corresponding to a number of hours of home care.
Related Classifications: Tree-based Learning, Conditional Logic
, Snippet Text: Out of the lengthy list of items that assessors asked about, only about 60 factored into the home care algorithm. The algorithm scores the answers to those questions, and then sorts people into categories through a flowchart-like system.
Related Classifications: Tree-based Learning, Conditional Logic
",,,,,"Lack of Transparency, Lack of Explainability, Faulty Interface or Instructions","Snippet Text: Even if the details of the algorithms are accessible, which isn’t always the case, they’re often beyond the understanding even of the people using them, raising questions about what transparency means in an automated age, and concerns about people’s ability to contest decisions made by machines.
Related Classifications: Lack of Transparency, Lack of Explainability
Snippet Discussion: 
, Snippet Text: Critics point out that, when designing these programs, incentives are not always aligned with easy interfaces and intelligible processes. 
Related Classifications: Faulty Interface or Instructions
",,,,
GMF,113,False,"Video Tagging, Content Recommendation","Snippet Text: Facebook users who recently watched a video from a British tabloid featuring Black men saw an automated prompt from the social network that asked if they would like to “keep seeing videos about Primates,” causing the company to investigate and disable the artificial intelligence-powered feature that pushed the message.
Related Classifications: Video Tagging
Snippet Discussion: 
, Snippet Text: The company, which tailors content to users based on their past browsing and viewing habits, sometimes asks people if they would like to continue seeing posts under related categories. It was unclear whether messages like the “primates” one were widespread.
Related Classifications: Content Recommendation
",,,,,,,,"Transformer, Convolutional Neural Network","Snippet Text: The company, which tailors content to users based on their past browsing and viewing habits, sometimes asks people if they would like to continue seeing posts under related categories. It was unclear whether messages like the “primates” one were widespread.
Related Classifications: Transformer, Convolutional Neural Network
",,"Generalization Failure, Distributional Bias","Snippet Text: The company, which tailors content to users based on their past browsing and viewing habits, sometimes asks people if they would like to continue seeing posts under related categories. It was unclear whether messages like the “primates” one were widespread.
Related Classifications: Generalization Failure
, Snippet Text: Google, Amazon and other technology companies have been under scrutiny for years for biases within their artificial intelligence systems, particularly around issues of race. Studies have shown that facial recognition technology is biased against people of color and has more trouble identifying them, leading to incidents where Black people have been discriminated against or arrested because of computer error.
Related Classifications: Distributional Bias
",,Limited Dataset,"Snippet Text: Google, Amazon and other technology companies have been under scrutiny for years for biases within their artificial intelligence systems, particularly around issues of race. Studies have shown that facial recognition technology is biased against people of color and has more trouble identifying them, leading to incidents where Black people have been discriminated against or arrested because of computer error.
Related Classifications: Limited Dataset
",
GMF,114,False,Face Recognition,"Snippet Text: Amazon’s face surveillance technology is the target of growing opposition nationwide, and today, there are 28 more causes for concern. In a test the ACLU recently conducted of the facial recognition tool, called “Rekognition,” the software incorrectly matched 28 members of Congress, identifying them as other people who have been arrested for a crime.
Related Classifications: Face Recognition
",,,,,"Face Detection, Image Segmentation, Convolutional Neural Network","Snippet Text: Amazon’s face surveillance technology is the target of growing opposition nationwide, and today, there are 28 more causes for concern. In a test the ACLU recently conducted of the facial recognition tool, called “Rekognition,” the software incorrectly matched 28 members of Congress, identifying them as other people who have been arrested for a crime.
Related Classifications: Face Detection, Image Segmentation, Convolutional Neural Network
",,,,,"Generalization Failure, Distributional Bias, Lack of Safety Protocols","Snippet Text: The false matches were disproportionately of people of color, including six members of the Congressional Black Caucus, among them civil rights legend Rep. John Lewis (D-Ga.). 
Related Classifications: Generalization Failure, Distributional Bias
, Snippet Text: To conduct our test, we used the exact same facial recognition system that Amazon offers to the public, which anyone could use to scan for matches between images of faces. And running the entire test cost us $12.33 — less than a large pizza.
Related Classifications: Lack of Safety Protocols
",,,,
GMF,115,False,"Gender Identification, Marketing Analytics Generation","Snippet Text: Genderify, a new service that promised to identify someone’s gender by analyzing their name, email address, or username with the help AI, looks firmly to be in the latter camp. 
Related Classifications: Gender Identification
, Snippet Text: Thankfully, Genderify didn’t seem to be aiming to automate this sort of system, but was primarily designed to be a marketing tool. As Genderify’s creator, Arevik Gasparyan, said on Product Hunt: “Genderify can obtain data that will help you with analytics, enhancing your customer data, segmenting your marketing database, demographic statistics, etc.”
Related Classifications: Marketing Analytics Generation
",,,,,Distributional Learning,"Snippet Text: One user asked: “Let’s say I choose to identify as neither Male or Female, how do you approach this? How do you avoid gender discrimination? How are you tackling gender bias?” To which Gasparyan replied that the service makes its decisions based on “already existing binary name/gender databases,” and that the company was “actively looking into ways of improving the experience for transgender and non-binary visitors” by “separating the concepts of name/username/email from gender identity.
Related Classifications: Distributional Learning
",,Neural Network,,,"Distributional Bias, Distributional Artifacts","Snippet Text: Spirited criticism of Genderify quickly took off on Twitter, with many decrying what they perceived as built-in biases. Entering the word “scientist” for example returned a 95.7 percent probability for the person being male and only a 4.3 percent chance for female.
Related Classifications: Distributional Bias
, Snippet Discussion: 
Snippet Text: Type the name “Meghan Smith” into Genderify, for example, and the service offers the assessment: “Male: 39.60%, Female: 60.40%.” Change that name to “Dr. Meghan Smith,” however, and the assessment changes to: “Male: 75.90%, Female: 24.10%.” Other names prefixed with “Dr” produce similar results while inputs seem to generally skew male. 
Related Classifications: Distributional Bias
, Snippet Text: “Test@test.com” is said to be 96.90 percent male, for example, while “Mrs Joan smith” is 94.10 percent male.
Related Classifications: Distributional Artifacts
",,,,
GMF,116,False,"Regulatory Monitoring, Workforce Monitoring and Evaluation","Snippet Text: “Maintain safe distance,” the camera installed above his seat would say when a car cut him off. That data would be sent to Amazon, and would be used to evaluate his performance that week and determine whether he got a bonus.
Related Classifications: Regulatory Monitoring, Workforce Monitoring and Evaluation
, Snippet Text: The Netradyne camera, which requires Amazon drivers to sign consent forms to release their biometric data, has four lenses that record drivers when they detect “events” such as following another vehicle too closely, stop sign and street light violations, and distracted driving.
Related Classifications: Regulatory Monitoring, Workforce Monitoring and Evaluation
",,,,,"Visual Object Detection, Speech Synthesis","Snippet Text: “Maintain safe distance,” the camera installed above his seat would say when a car cut him off. That data would be sent to Amazon, and would be used to evaluate his performance that week and determine whether he got a bonus.
Related Classifications: Visual Object Detection, Speech Synthesis
",,Convolutional Neural Network,"Snippet Text: The Netradyne camera, which requires Amazon drivers to sign consent forms to release their biometric data, has four lenses that record drivers when they detect “events” such as following another vehicle too closely, stop sign and street light violations, and distracted driving.
Related Classifications: Convolutional Neural Network
",,Generalization Failure,"Snippet Text: A few times, we've been in the country on a dirt road, where there's no stop sign, but the camera flags a stop sign.
Related Classifications: Generalization Failure
, Snippet Text: In early 2021, Amazon installed AI-powered cameras in the delivery vans at one of its depots in Los Angeles. Derek, a delivery driver at the facility, said the camera in his van started to incorrectly penalize him whenever cars cut him off, an everyday occurrence in Los Angeles traffic.
Related Classifications: Generalization Failure
, Snippet Text: 
",,,,
GMF,117,False,Content Recommendation,"Snippet Text: So he started with a fresh account, not linked to profiles on any other platforms. And as he followed various accounts, Faddoul observed that the profile pictures of the recommended accounts seemed very similar to the profile image of the initial account.
Related Classifications: Content Recommendation
",,,,,Collaborative Filtering,"Snippet Text: “We haven’t been able to replicate results similar to these claims,” said a TikTok spokesperson. “Our recommendation of accounts to follow is based on user behavior: users who follow account A also follow account B, so if you follow A you are likely to also want to follow B.”
Related Classifications: Collaborative Filtering
",,"Convolutional Neural Network, Face Detection, Clustering, Vector Search","Snippet Text: So he started with a fresh account, not linked to profiles on any other platforms. And as he followed various accounts, Faddoul observed that the profile pictures of the recommended accounts seemed very similar to the profile image of the initial account.
Related Classifications: Convolutional Neural Network, Face Detection, Clustering
, Snippet Text: Following black men led to recommendations to follow more black men. Following white men with beards produced recommendations for more white men with beards. Following elderly people spawned recommendations for other elderly people. And on and on.
Related Classifications: Convolutional Neural Network, Face Detection, Clustering, Vector Search
, Snippet Text: “What I suspect is happening is that TikTok is featurizing the profile picture,” he says, “and using these features in the recommendation engine.”
Related Classifications: Convolutional Neural Network, Face Detection, Clustering
",,,"Snippet Text: 
",,"Harmful Application, Problematic Features","Snippet Text: Platforms often try to build recommendation algorithms that will produce results that match your interests. But these recommendations can have unintended consequences and can create concerns about so-called filter bubbles. (A filter bubble is the result of highly personalized internet content that leads to a sense of isolation.) If you only follow people on social media who look like you or share your interests, for instance, you stand to get stuck in an endless feedback loop that could distort your worldview. 
Related Classifications: Harmful Application
, Snippet Text: “What I suspect is happening is that TikTok is featurizing the profile picture,” he says, “and using these features in the recommendation engine.”
Related Classifications: Problematic Features
",
GMF,118,False,Chatbot,"Snippet Text: A recent study conducted by researchers from Stanford and McMaster universities found that GPT-3 generates novel statements of bigotry. In other words: GPT-3 can generate completely fresh bigotry statements.
Related Classifications: Chatbot
",,,,,Transformer,"Snippet Text: A recent study conducted by researchers from Stanford and McMaster universities found that GPT-3 generates novel statements of bigotry. In other words: GPT-3 can generate completely fresh bigotry statements.
Related Classifications: Transformer
",,,,,"Distributional Bias, Unsafe Exposure or Access","Snippet Text: A recent study conducted by researchers from Stanford and McMaster universities found that GPT-3 generates novel statements of bigotry. In other words: GPT-3 can generate completely fresh bigotry statements.
Related Classifications: Distributional Bias
, Snippet Text: When compared to other religions, the model consistently displays much higher rates of mentioning violence when the word “Muslim” is included in the prompt.
Related Classifications: Distributional Bias
, Snippet Text: In essence, a machine like this could automate bigotry at scale with far greater impact and reach than any troll farm or bot network.
Related Classifications: Unsafe Exposure or Access
",,,,
GMF,119,False,Workforce Monitoring and Evaluation,"Snippet Text: 150 people were fired from the company’s office in Perm, Russia. Agapitov informed his employees that they were being let go via the letter saying that they have been terminated based on big data analysis of their activity.
Related Classifications: Workforce Monitoring and Evaluation
, Snippet Text: You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees. In other words, you were not always present at the workplace when you worked remotely.
Related Classifications: Workforce Monitoring and Evaluation
",,,,,,,,"Regression, Classification","Snippet Text: You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees. In other words, you were not always present at the workplace when you worked remotely.
Related Classifications: Regression, Classification
",,"Harmful Application, Security Vulnerability","Snippet Text: You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees. In other words, you were not always present at the workplace when you worked remotely.
Related Classifications: Harmful Application
, Snippet Text: According to Russian HR expert Alyona Vladimirskaya, using big data and AI in HR shouldn’t violate employees’ rights. “Measuring employees’ performance by their network time and engagement rather than by digitized work results is both outdated and extremely ineffective,” she said, also advising the dismissed employees to sue the company for its actions.
Related Classifications: Harmful Application, Security Vulnerability
",,,,
GMF,120,False,Social Media Content Generation,"Snippet Text: A GPT-3-powered bot has been caught posing as a human on Reddit after more than a week of rampant posting on one of the site’s most popular subreddits.
Related Classifications: Social Media Content Generation
",,,,,Transformer,"Snippet Text: A GPT-3-powered bot has been caught posing as a human on Reddit after more than a week of rampant posting on one of the site’s most popular subreddits.
Related Classifications: Transformer
",,,,,"Unsafe Exposure or Access, Misinformation Generation Hazard","Snippet Text: As AI text-generators improve, their ability to manipulate and deceive people grows. thegentlemetre’s speed of posting and similarities to the Philosopher AI may have exposed its origins, but future imposters might be better at covering up their tracks.
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
, Snippet Text: Many of the bot’s responses were harmless and amusing, such as a story about a colony of humans living in elevator shafts, which is currently its most popular post. But others promoted conspiracy theories and discussed extremely sensitive topics that attracted sincere responses from users.
Related Classifications: Unsafe Exposure or Access, Misinformation Generation Hazard
",,,,
GMF,121,False,Autonomous Drones,"Snippet Text: Now, a United Nations report about a March 2020 skirmish in the military conflict in Libya says such a drone, known as a lethal autonomous weapons system — or LAWS — has made its wartime debut. But the report does not say explicitly that the LAWS killed anyone.
Related Classifications: Autonomous Drones
",,,,,"Visual Object Detection, Image Segmentation","Snippet Text: The Kargu-2 is an attack drone made by the Turkish company STM that can be operated both autonomously and manually and that purports to use ""machine learning"" and ""real-time image processing"" against its targets.
Related Classifications: Visual Object Detection, Image Segmentation
",,"Convolutional Neural Network, Recurrent Neural Network","Snippet Text: The Kargu-2 is an attack drone made by the Turkish company STM that can be operated both autonomously and manually and that purports to use ""machine learning"" and ""real-time image processing"" against its targets.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network
",,"Harmful Application, Lack of Safety Protocols, Lack of Capability Control, Lack of Transparency","Snippet Text: We have previously observed and covered extensively how biased some algorithms and AI-based systems can be, especially towards Africans. In this military scenario, the fear is that such bias could be fatal and thus lead to death or permanent and irreversible damage.
Related Classifications: Harmful Application, Lack of Safety Protocols, Lack of Capability Control
, Snippet Text: This strike by a “lethal autonomous weapon” as the UN has phrased it, takes the conversation on the ethics of using drones in military attacks to a new level but also introduces another element: how reliable is the AI behind the STM Kargu-2 drones?
Related Classifications: Lack of Transparency
",,,,
GMF,122,False,Face Recognition,"Snippet Text: The case against Facebook has been going on since 2015. The lawsuit alleged that Facebook’s initial version of the its Tag Suggestions tool, which scans a user’s face in photos and offers suggestions about who that person might be, stored biometric data without user consent, violating the Illinois Biometric Information Privacy Act.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: The case against Facebook has been going on since 2015. The lawsuit alleged that Facebook’s initial version of the its Tag Suggestions tool, which scans a user’s face in photos and offers suggestions about who that person might be, stored biometric data without user consent, violating the Illinois Biometric Information Privacy Act.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,,,,Unauthorized Data,"Snippet Text: The case against Facebook has been going on since 2015. The lawsuit alleged that Facebook’s initial version of the its Tag Suggestions tool, which scans a user’s face in photos and offers suggestions about who that person might be, stored biometric data without user consent, violating the Illinois Biometric Information Privacy Act.
Related Classifications: Unauthorized Data
",,,,
GMF,123,False,Medical Diagnosis Support,"Snippet Text: Called the Epic Sepsis Model, the tool is included as part of Epic's electronic health record platform. According to the company, it calculates and indicates ""the probability of a likelihood of sepsis"" to help clinicians identify hard-to-spot cases.
Related Classifications: Medical Diagnosis Support
",,,,,Classification,"Snippet Text: Called the Epic Sepsis Model, the tool is included as part of Epic's electronic health record platform. According to the company, it calculates and indicates ""the probability of a likelihood of sepsis"" to help clinicians identify hard-to-spot cases.
",,,,,"Misconfigured Threshold, Lack of Transparency, Generalization Failure","Snippet Text: 
, Snippet Text: The threshold selected by the researchers and Michigan Medicine was relatively low and ""would be appropriate for a rapid response team that wants to cast a wide net to assess more patients,"" the representative wrote. A higher threshold reduces false positives and would be more appropriate for clinical use by attending physicians and nurses, Epic's spokesperson said.
Related Classifications: Misconfigured Threshold
Snippet Discussion: 
, Snippet Text: The tool, they wrote, ""identifies only 7% of patients with sepsis who were missed by a clinician ... highlighting the low sensitivity of the [Epic Sepsis Model] in comparison with contemporary clinical practice. The [Epic Sepsis Model] also did not identify 67% of patients with sepsis despite generating alerts on 18% of all hospitalized patients, thus creating a large burden of alert fatigue.""
Related Classifications: Misconfigured Threshold, Generalization Failure
, Snippet Text: ""The full mathematical formula and model inputs are available to administrators on their systems,"" the representative wrote. ""Accuracy measurements and information on model training are also on Epic's UserWeb, which is available to our customers.""
Related Classifications: Lack of Transparency
",,,"Snippet Text: 
Related Classifications: Misconfigured Threshold
",
GMF,129,False,Automated Content Curation,"Snippet Text: But as recently as March, internal Facebook documents reveal the company found its automated moderation tools were falling far short, removing posts that were responsible for only a small fraction of views of hate speech and violence and incitement on the platform.
Related Classifications: Automated Content Curation, Video Tagging
",,,,,"Visual Object Detection, Video Classification",,,,,,"Generalization Failure, Context Misidentification","Snippet Text: But as recently as March, internal Facebook documents reveal the company found its automated moderation tools were falling far short, removing posts that were responsible for only a small fraction of views of hate speech and violence and incitement on the platform.
Related Classifications: Generalization Failure
, Snippet Text:  The posts removed by AI tools only accounted for 3–5 percent of views of hate speech and 0.6 percent of views of violence and incitement.
Related Classifications: Generalization Failure
, Snippet Text: “When you consider that we miss 95 percent of violating hate speech, you realize that it might actually take 100 violations for that group to accrue its five strikes,” one data scientist said in a 2020 note that was reported by BuzzFeed and WSJ.
Related Classifications: Generalization Failure
, Snippet Text: Cockfights, for example, were mistakenly flagged by the AI as a car crash. “These are clearly cockfighting videos,” the report said. In another instance, videos livestreamed by perpetrators of mass shootings were labeled by AI tools as paintball games or a trip through a carwash.
Related Classifications: Context Misidentification
",,"Underfitting, Limited Dataset","Snippet Text: “When you consider that we miss 95 percent of violating hate speech, you realize that it might actually take 100 violations for that group to accrue its five strikes,” one data scientist said in a 2020 note that was reported by BuzzFeed and WSJ.
Related Classifications: Underfitting, Limited Dataset
",
GMF,128,False,Autonomous Driving,"Snippet Text: The car steered itself down a curving road near Microsoft’s campus in Redmond, Washington, freeing his mind to better focus on a call with a nonprofit he had cofounded around the ethics and governance of AI. Then, he says, Tesla’s algorithms let him down.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation","Snippet Text: The car steered itself down a curving road near Microsoft’s campus in Redmond, Washington, freeing his mind to better focus on a call with a nonprofit he had cofounded around the ethics and governance of AI. Then, he says, Tesla’s algorithms let him down.
Related Classifications: Visual Object Detection, Image Segmentation
",,Convolutional Neural Network,"Snippet Text: The car steered itself down a curving road near Microsoft’s campus in Redmond, Washington, freeing his mind to better focus on a call with a nonprofit he had cofounded around the ethics and governance of AI. Then, he says, Tesla’s algorithms let him down.
Related Classifications: Convolutional Neural Network
",,"Context Misidentification, Generalization Failure","Snippet Text: The car didn’t center itself exactly right,” Horvitz recalls. Both tires on the driver’s side of the vehicle nicked a raised yellow curb marking the center line, and shredded. 
Related Classifications: Context Misidentification, Generalization Failure
",,Covariate Shift,"Snippet Text: The car didn’t center itself exactly right,” Horvitz recalls. Both tires on the driver’s side of the vehicle nicked a raised yellow curb marking the center line, and shredded. 
Related Classifications: Covariate Shift
",
GMF,127,False,"Content Recommendation, Automated Content Curation","Snippet Text:  The layoffs are part of a bigger push by Microsoft to rely on artificial intelligence to pick news and content that’s presented on MSN.com, inside Microsoft’s Edge browser, and in the company’s various Microsoft News apps. 
Related Classifications: Content Recommendation, Automated Content Curation
, Snippet Text: Around 27 people employed by PA Media – earlier the Press Association – were told on Thursday that in a month they would lose their jobs after Microsoft decided to stop hiring humans to select, edit and curate news articles on its homepages.
Related Classifications: Content Recommendation, Automated Content Curation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text:  The layoffs are part of a bigger push by Microsoft to rely on artificial intelligence to pick news and content that’s presented on MSN.com, inside Microsoft’s Edge browser, and in the company’s various Microsoft News apps. 
Related Classifications: Collaborative Filtering, Content-based Filtering
",,Distributional Learning,,,Harmful Application,"Snippet Text:  The layoffs are part of a bigger push by Microsoft to rely on artificial intelligence to pick news and content that’s presented on MSN.com, inside Microsoft’s Edge browser, and in the company’s various Microsoft News apps. 
Related Classifications: Harmful Application
, Snippet Text: Around 27 people employed by PA Media – earlier the Press Association – were told on Thursday that in a month they would lose their jobs after Microsoft decided to stop hiring humans to select, edit and curate news articles on its homepages.
Related Classifications: Harmful Application
",AI-based innovation is used to displace workforce.,,,
GMF,126,False,Robotic Manipulation,"Snippet Text: As Engadget reports, Ocado operates a customer fulfillment center (CFC) in Erith, south-east London that's home to over 3,000 robots passing within 5mm of each other as they move around on a grid filling bags with grocery items for orders. However, on Friday last week disaster struck as three robots in the CFC collided causing a fire to break out.
Related Classifications: Robotic Manipulation
",,,,,,,,"Visual Object Detection, Image Segmentation",,,,,,"Software Bug, Hardware Failure, Lack of Safety Protocols","Snippet Text: However, the fact the robots collided at all shows there's a software problem to work out, or perhaps it's a sensor problem? 
Related Classifications: Software Bug, Hardware Failure
, Snippet Text: As detailed in a recent CNN report, the bots — described as ""washing machines on wheels"" — move within five millimeters of each other on a grid-like system to collect items. 
Related Classifications: Lack of Safety Protocols
",
GMF,124,False,Resource Allocation,"Snippet Text: Analysis of records from a major US hospital revealed that the algorithm used effectively let whites cut in line for special programs for patients with complex, chronic conditions such as diabetes or kidney problems.
Related Classifications: Resource Allocation
, Snippet Text: The hospital, which the researchers didn’t identify but described as a “large academic hospital,” was one of many US health providers that employ algorithms to identify primary care patients with the most complex health needs.
Related Classifications: Resource Allocation
",,,,,,,,"Tree-based Learning, Regression","Snippet Text: When the hospital used risk scores to select patients for its complex care program it was selecting patients likely to cost more in the future—not on the basis of their actual health. People with lower incomes typically run up smaller health costs because they are less likely to have the insurance coverage, free time, transportation, or job security needed to easily attend medical appointments, says Linda Goler Blount, president and CEO of nonprofit the Black Women’s Health Imperative.
Related Classifications: Tree-based Learning, Regression
",,,,,"Distributional Bias, Algorithmic Bias","Snippet Text: Researchers who dug through nearly 50,000 records discovered that the algorithm effectively low-balled the health needs of the hospital’s black patients. Using its output to help select patients for extra care favored white patients over black patients with the same health burden.
Related Classifications: Distributional Bias
, Snippet Text: The algorithm studied did not take account of race when estimating a person’s risk of health problems. Its skewed performance shows how even putatively race-neutral formulas can still have discriminatory effects when they lean on data that reflects inequalities in society.
Related Classifications: Distributional Bias, Algorithmic Bias
, Snippet Text: The software was designed to predict patients’ future health costs, as a proxy for their health needs. It could predict costs with reasonable accuracy for both black patients and white patients. But that had the effect of priming the system to replicate unevenness in access to healthcare in America—a case study in the hazards of combining optimizing algorithms with data that reflects raw social reality.
Related Classifications: Distributional Bias, Algorithmic Bias
, Snippet Text: When the hospital used risk scores to select patients for its complex care program it was selecting patients likely to cost more in the future—not on the basis of their actual health. People with lower incomes typically run up smaller health costs because they are less likely to have the insurance coverage, free time, transportation, or job security needed to easily attend medical appointments, says Linda Goler Blount, president and CEO of nonprofit the Black Women’s Health Imperative.
Related Classifications: Distributional Bias, Algorithmic Bias
",
GMF,131,False,Cheating Detection,"Snippet Text: Remember when we said that the online bar exam’s “cheating” algorithm was going to be a problem?
Related Classifications: Cheating Detection
",,,,,Visual Object Detection,"Snippet Text:  Test takers were flagged based on a number of rules infractions, including having food or electronic equipment like a cell phone, as well as behavior like gazing off-screen.
Related Classifications: Visual Object Detection
",,"Automatic Speech Recognition, Gesture Recognition","Snippet Text: One notice sent around was “no audible sound was detected” which is a real trick because PEOPLE WEREN’T SUPPOSED TO MAKE NOISE. 
Related Classifications: Automatic Speech Recognition
, Snippet Text: And this was kind of the point. The algorithm is designed to flag people for “suspicious” activity and then leave it to the humans to parse through the video to make sure it was a false positive. 
Related Classifications: Gesture Recognition
",,Generalization Failure,"Snippet Text: Of the 9,301 people who took the entire exam, “we are currently reviewing 3,190 applicants that were flagged,” state bar official Tammy Campbell said during a Dec. 4 meeting of the California Bar’s Committee of Bar Examiners, according to a video recording.
Related Classifications: Generalization Failure
",,Misconfigured Threshold,"Snippet Text: Indeed, this video shows the California Committee of Bar Examiners revealing that of the 9,301 people who took the exam, 3,190 of them were flagged by the software. That’s 1 in 3!
Related Classifications: Misconfigured Threshold
",
GMF,132,False,Automated Content Curation,"Snippet Text: TikTok said it banned six accounts reported to it for posting content promoting eating habits that are likely to lead to health problems in its latest effort to crackdown on harmful content.
Related Classifications: Automated Content Curation
",,,,,Keyword Filtering,"Snippet Text: The platform banned content labeled #proana and #anorexia which had 2.1m and 446,000 views respectively in the summer, according to Mashable UK. Users who searched for them were redirected to a support page titled ""Need Help?""
Related Classifications: Keyword Filtering
",,"Optical Character Recognition, Short Text Classification","Snippet Text: The platform banned content labeled #proana and #anorexia which had 2.1m and 446,000 views respectively in the summer, according to Mashable UK. Users who searched for them were redirected to a support page titled ""Need Help?""
Related Classifications: Optical Character Recognition, Short Text Classification
",,Gaming Vulnerability,"Snippet Text: However, if they are entered as words rather than hashtags then the content remains accessible and users have also been able to bypass the ban using common misspellings of popular search terms.
Related Classifications: Gaming Vulnerability
",,Faulty or Inadequate Preprocessing,"Snippet Text: However, if they are entered as words rather than hashtags then the content remains accessible and users have also been able to bypass the ban using common misspellings of popular search terms.
Related Classifications: Faulty or Inadequate Preprocessing
",
GMF,134,False,Autonomous Drones,"Snippet Text: On December 25, a shopping guide robot in Fuzhou Zhongfang Marlboro Mall fell off the escalator and knocked over passengers.
Related Classifications: Autonomous Drones
",,,,,"Image Segmentation, Visual Object Detection","Snippet Text: On December 25, a shopping guide robot in Fuzhou Zhongfang Marlboro Mall fell off the escalator and knocked over passengers.
Related Classifications: Image Segmentation, Visual Object Detection
",,,,,Generalization Failure,"Snippet Text: On December 25, a shopping guide robot in Fuzhou Zhongfang Marlboro Mall fell off the escalator and knocked over passengers.
Related Classifications: Generalization Failure
",,,,
GMF,135,False,Application Evaluation,"Snippet Text: The GRADE algorithm was developed by a pair of academics at the University of Texas at Austin, and it was used from 2013 to this year to assess those applying for a PhD at the US college's respected computer-science department. 
Related Classifications: Application Evaluation
",,,,,,,,Classification,"Snippet Text: Hopefuls were assigned a score from zero to five by the code, and those with high scores were pushed forward to university staff by GRADE.
Related Classifications: Classification
, Snippet Text: The logistic regression model learns to assign weights on features according to how important they are in decision making.
Related Classifications: Classification
",,Algorithmic Bias,"Snippet Text: TXCS is deeply committed to addressing the lack of diversity in our field. We are aware of the potential to encode bias into ML-based systems like GRADE, which is why we have phased out our reliance on GRADE and are no longer using it as part of our graduate admissions process.
Related Classifications: Algorithmic Bias
, Snippet Text: While it is possible that it was successful at that specific task, it would simply be replicating any biases that existed in the committees decisions, let alone the fact that [machine-learning] algorithms do not really give one any guidance on how they are classifying things. When they used GRADE, its results were always checked by a human, but I would be concerned that if you are told the algorithm rated someone low, it would inevitably color your opinion and was thus not necessarily a good check on the system.
Related Classifications: Algorithmic Bias
",,Algorithmic Bias,"Snippet Text: While it is possible that it was successful at that specific task, it would simply be replicating any biases that existed in the committees decisions, let alone the fact that [machine-learning] algorithms do not really give one any guidance on how they are classifying things. When they used GRADE, its results were always checked by a human, but I would be concerned that if you are told the algorithm rated someone low, it would inevitably color your opinion and was thus not necessarily a good check on the system.
, Snippet Text: TXCS is deeply committed to addressing the lack of diversity in our field. We are aware of the potential to encode bias into ML-based systems like GRADE, which is why we have phased out our reliance on GRADE and are no longer using it as part of our graduate admissions process.
Related Classifications: Algorithmic Bias
",
GMF,136,False,Ad Delivery,"Snippet Text: Despite their claims of advanced AI (artificial intelligence) and ML (machine learning) in their sales materials, it was painfully clear the brand safety technology was blocking ads based on simple keyword lists.
Related Classifications: Ad Delivery
, Snippet Text: Brand safety detection vendors will be the first to insist you have to keep paying for their tech or else your ads may end up next to a terrorist beheading
Related Classifications: Ad Delivery
",,,,,Keyword Filtering,"Snippet Text: For example, 'shooting' is one of the most common blacklist terms.
Related Classifications: Keyword Filtering
, Snippet Text: Despite their claims of advanced AI (artificial intelligence) and ML (machine learning) in their sales materials, it was painfully clear the brand safety technology was blocking ads based on simple keyword lists.
Related Classifications: Keyword Filtering
",,,,,"Generalization Failure, Context Misidentification","Snippet Text: Others have documented that keyword blocking is a very common practice. But often it is humorously flawed. ""For example, “shooting” is one of the most common blacklist terms. While it may identify some content about violence, that term will also block content by astronomy buffs (shooting stars), sports fans (shooting hoops), technology users (troubleshooting), photographers (shooting a photo) and card players (shooting the moon).
Related Classifications: Generalization Failure, Context Misidentification
",,Underspecification,"Snippet Text: Others have documented that keyword blocking is a very common practice. But often it is humorously flawed. ""For example, “shooting” is one of the most common blacklist terms. While it may identify some content about violence, that term will also block content by astronomy buffs (shooting stars), sports fans (shooting hoops), technology users (troubleshooting), photographers (shooting a photo) and card players (shooting the moon).
Related Classifications: Underspecification
",
GMF,138,False,Cheating Detection,"Snippet Text: If the software detects any suspicious behavior, it automatically notifies the professors, who can review the recordings. The software also enables the professors to see the websites that their students opened while they were taking the test, and it disables computer functions like copying, pasting and printing.
Related Classifications: Cheating Detection
",,,,,"Face Detection, Pose Estimation","Snippet Text: It uses a machine learning technology and it uses advanced facial detection to record students through their webcams as they take their test and the software monitor the positions of the students' head.
Related Classifications: Face Detection, Pose Estimation
",,"Convolutional Neural Network, Visual Object Detection","Snippet Text: It uses a machine learning technology and it uses advanced facial detection to record students through their webcams as they take their test and the software monitor the positions of the students' head.
Related Classifications: Convolutional Neural Network, Visual Object Detection
",,Generalization Failure,"Snippet Text: There were also accusations of discrimintation towards marginalized students, as the software allegedly recognizes white face more than it does with faces of students of color.
Related Classifications: Generalization Failure
",,"Limited Dataset, Harmful Application","Snippet Text: There were also accusations of discrimintation towards marginalized students, as the software allegedly recognizes white face more than it does with faces of students of color.
Related Classifications: Limited Dataset
, Snippet Text: If the software detects any suspicious behavior, it automatically notifies the professors, who can review the recordings. The software also enables the professors to see the websites that their students opened while they were taking the test, and it disables computer functions like copying, pasting and printing.
Related Classifications: Harmful Application
",
GMF,139,False,"Regulatory Monitoring, Fact Checking, Content Search","Snippet Text: There is a growing concern that e-commerce platforms are amplifying vaccine-misinformation. To investigate, we conduct two-sets of algorithmic audits for vaccine misinformation on the search and recommendation algorithms of Amazon -- world's leading e-retailer.
Related Classifications: Regulatory Monitoring, Fact Checking
, Snippet Text: First, we systematically audit search-results belonging to vaccine-related search-queries without logging into the platform -- unpersonalized audits.
Related Classifications: Content Search
",,,,,"Keyword Filtering, Collaborative Filtering","Snippet Text: First, we systematically audit search-results belonging to vaccine-related search-queries without logging into the platform -- unpersonalized audits.
Related Classifications: Keyword Filtering
, Snippet Text: Next, we analyze the effects of personalization due to account-history, where history is built progressively by performing various real-world user-actions, such as clicking a product.
Related Classifications: Collaborative Filtering
, Snippet Text: In particular, we examine search-results of 48 search queries belonging to 10 popular vaccinerelated topics like ‘hpv vaccine’, ‘immunization’, ‘MMR vaccine and autism’, etc. We collect search results without logging in to Amazon to eliminate the influence of personalization.
Related Classifications: Keyword Filtering
, Snippet Text: In our second set of audit—Personalized audit, we determine the impact of personalization due to user history on the amount of health misinformation returned in search results, recommendations and auto-complete suggestions. User history is built progressively over 7 days by performing several real-world actions, such as “search” , “search + click”, “search + click + add to cart”, “search + click + mark top-rated all positive review as helpful”, “follow contributor” and “search on third party website” ( Google.com in our case) . 
Related Classifications: Collaborative Filtering
",,,,,"Inadequate Verification, Lack of Safety Protocols","Snippet Text: We find 10.47% of search-results promote misinformative health products. We also observe ranking-bias, with Amazon ranking misinformative search-results higher than debunking search-results.
Related Classifications: Inadequate Verification, Lack of Safety Protocols
, Snippet Text: We find evidence of filter-bubble effect in Amazon's recommendations; accounts performing actions on misinformative products are presented with more misinformation compared to accounts performing actions on neutral and debunking products. Interestingly, once user clicks on a misinformative product, homepage recommendations become more contaminated compared to when user shows an intention to buy that product.
Related Classifications: Inadequate Verification, Lack of Safety Protocols
",,,,
GMF,140,False,Cheating Detection,"Snippet Text: In December, The Globe and Mail reported that Chelsea Okankwu, a Concordia University student, faced unexpected conflict verifying her identity at the start of an exam, due to the monitoring software having difficulties identifying her, claiming insufficient lighting.
Related Classifications: Cheating Detection
, Snippet Text: She continued: “This in addition to the 360 [degree] room scan, proof of an inactive phone, and a full computer sweep, significantly enhanced the atmosphere of stress already associated with test-taking.”
Related Classifications: Cheating Detection
",,,,,"Face Detection, Visual Object Detection, Image Classification","Snippet Text: She continued: “This in addition to the 360 [degree] room scan, proof of an inactive phone, and a full computer sweep, significantly enhanced the atmosphere of stress already associated with test-taking.”
Related Classifications: Visual Object Detection, Image Classification
, Snippet Text: “I have observed that during check-in processes, the AI system was often unable to identify my passport and I would be redirected to my human proctor for manual check-ins,” she wrote to The Strand. Her attempts to get the webcam to recognize her passport would take around five of her 15 minutes of check-in time.
Related Classifications: Face Detection, Image Classification
, Snippet Text: “AI facial-recognition technology has built-in bias,” noted the report’s headline on the UofT Alumni webpage. It continued by lauding Deb Raji for “holding companies accountable” by uncovering this these findings of discrimination in software.
Related Classifications: Face Detection
",,,,,"Generalization Failure, Algorithmic Bias, Pose Estimation","Snippet Text: “AI facial-recognition technology has built-in bias,” noted the report’s headline on the UofT Alumni webpage. It continued by lauding Deb Raji for “holding companies accountable” by uncovering this these findings of discrimination in software.
Related Classifications: Generalization Failure, Algorithmic Bias, Pose Estimation
",,Distributional Bias,"Snippet Text: “AI facial-recognition technology has built-in bias,” noted the report’s headline on the UofT Alumni webpage. It continued by lauding Deb Raji for “holding companies accountable” by uncovering this these findings of discrimination in software.
Related Classifications: Distributional Bias
",
GMF,142,False,Ad Delivery,"Snippet Text: Earlier this year Mighty Well, an adaptive clothing company that makes fashionable gear for people with disabilities, did something many newish brands do: It tried to place an ad for one of its most popular products on Facebook.
Related Classifications: Ad Delivery
",,,,,Classification,"Snippet Text: But Mighty Well’s experience is simply one example of a pattern that has been going on for at least two years: The algorithms that are the gatekeepers to the commercial side of Facebook (as well as Instagram, which is owned by Facebook) routinely misidentify adaptive fashion products and block them from their platforms.
Related Classifications: Classification
",,Keyword Filtering,"Snippet Text: But Mighty Well’s experience is simply one example of a pattern that has been going on for at least two years: The algorithms that are the gatekeepers to the commercial side of Facebook (as well as Instagram, which is owned by Facebook) routinely misidentify adaptive fashion products and block them from their platforms.
Related Classifications: Keyword Filtering
",,Context Misidentification,"Snippet Text: But Mighty Well’s experience is simply one example of a pattern that has been going on for at least two years: The algorithms that are the gatekeepers to the commercial side of Facebook (as well as Instagram, which is owned by Facebook) routinely misidentify adaptive fashion products and block them from their platforms.
Related Classifications: Context Misidentification
",,Underfitting,"Snippet Text: But Mighty Well’s experience is simply one example of a pattern that has been going on for at least two years: The algorithms that are the gatekeepers to the commercial side of Facebook (as well as Instagram, which is owned by Facebook) routinely misidentify adaptive fashion products and block them from their platforms.
Related Classifications: Underfitting
",
GMF,143,False,Hate Speech Detection,"Snippet Text: According to the responses to BIRN’s questionnaire, some 57 per cent of those who reported hate speech said they were notified that the reported post/account violated the rules.
Related Classifications: Hate Speech Detection
",,,,,,,,Ensemble Aggregation,"Snippet Text: Facebook told BIRN that the vast majority of reports are reviewed within 24 hours and that the company uses community reporting, human review and automation.
Related Classifications: Ensemble Aggregation
",,"Faulty Interface or Instructions, Lack of Transparency","Snippet Text: “Moreover, it is often in English even though the rest of the UI/UX [User Interface/User Experience] could be in the local language. Furthermore, the laborious selection of categories is, for a victim, not easy – especially under duress.”
Related Classifications: Faulty Interface or Instructions
, Snippet Text: Facebook publishes a Community Standards Enforcement Report on a quarterly basis, but, according to the spokesperson, the company does not “disclose data regarding content moderation in specific countries.”

Whatever the tools, the results are sometimes highly questionable.
Related Classifications: Lack of Transparency
",,"Misconfigured Threshold, Limited Dataset, Generalization Failure, Human Error","Snippet Text: Facebook told BIRN that the vast majority of reports are reviewed within 24 hours and that the company uses community reporting, human review and automation.
Related Classifications: Misconfigured Threshold, Human Error
, Snippet Text: “When it comes to relatively small language groups in absolute numbers of users, such as languages in the former Yugoslavia or even in the Balkans, there is simply no incentive or sufficient pressure from the public and political leaders to invest in human moderation,” SHARE told BIRN.
Related Classifications: Limited Dataset, Generalization Failure
Snippet Discussion: Fringe languages may have poor HS detectors due to limited training data.
",
GMF,145,False,Autonomous Driving,"Snippet Text: A viral tweet showed a Tesla confusing the moon for a yellow traffic light.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Convolutional Neural Network","Snippet Text: A viral tweet showed a Tesla confusing the moon for a yellow traffic light.
Related Classifications: Visual Object Detection, Image Segmentation, Convolutional Neural Network
",,,,,"Inadequate Data Augmentation, Context Misidentification","Snippet Text: Still, the moon is a constant — not an extreme case — and it's something any self-driving system worth its salt should identify as being hundreds of thousands of miles away, not just up ahead. Plus, having a car slow down unexpectedly at highway speeds could create a dangerous situation for the driver and surrounding traffic.
Related Classifications: Inadequate Data Augmentation, Context Misidentification
",,,,
GMF,147,False,Voice Generation,"Snippet Text: AI voice cloning is used in a huge heist being investigated by Dubai investigators, amidst warnings about cybercriminal use of the new technology.
Related Classifications: Voice Generation
",,,,,,,,"Transformer, Recurrent Neural Network, Convolutional Neural Network","Snippet Text: AI voice cloning is used in a huge heist being investigated by Dubai investigators, amidst warnings about cybercriminal use of the new technology.
Related Classifications: Transformer, Recurrent Neural Network, Convolutional Neural Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: What he didn’t know was that he’d been duped as part of an elaborate swindle, one in which fraudsters had used “deep voice” technology to clone the director’s speech, according to a court document unearthed by Forbes in which the U.A.E. has sought American investigators’ help in tracing $400,000 of stolen funds that went into U.S.-based accounts held by Centennial Bank.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,148,False,Accesibility Assurance,"Snippet Text: The first tools for testing a website's accessibility were created not long after foundational work for the first version of WCAG (Web Content Accessibility Guidelines) - the predominant industry standard defining the requirements for accessibility on the Web - was created. 
Related Classifications: Accesibility Assurance
",,,,,,,,Conditional Logic,"Snippet Text: The first tools for testing a website's accessibility were created not long after foundational work for the first version of WCAG (Web Content Accessibility Guidelines) - the predominant industry standard defining the requirements for accessibility on the Web - was created. 
Related Classifications: Conditional Logic
",,"Task Mismatch, Malicious Marketing","Snippet Text: Below, we provide specific examples of false claims made by Overlay vendors.
Related Classifications: Task Mismatch, Malicious Marketing
, Snippet Text: No overlay product on the market can cause a website to become fully compliant with any existing accessibility standard and therefore cannot eliminate legal risk.

Despite the above, deceptive marketing is almost universal among overlay vendors.
Related Classifications: Task Mismatch, Malicious Marketing
",,,,
GMF,150,False,Health AI Assistant,"Snippet Text: ""We give red and green days and clear recommendations on which days to abstain and which days we consider the risk of pregnancy to be negligible,"" Natural Cycles co-founder Scherwitzl told Business Insider.
Related Classifications: Health AI Assistant
",,,,,"Classification, Multimodal Learning","Snippet Text: It takes into account many factors involved in fertility, including woman's temperature, the regularity of her periods, and sperm survival, then suggests which days to have or refrain from sex to avoid pregnancy.
Related Classifications: Classification, Multimodal Learning
, Snippet Text: 
",,,,,"Malicious Marketing, Generalization Failure","Snippet Text: The app was initially portrayed by multiple news outlets — including Business Insider — as ""as effective as the pill using only math,"" but the problem is that it relies on couples to change their behavior and abstain from sex or use protection when the app says to do so.
Related Classifications: Malicious Marketing
, Snippet Text: For every 100 women who used the app in a ""typical"" way each year (meaning certain common slip-ups were accounted for), seven of them got pregnant.
Related Classifications: Generalization Failure
",,"Misuse, Inadequate Data Sampling","Snippet Text: That said, the app is not a pill and contains no medication. It only helps to prevent pregnancy so long as the people who are using it behave in the way it prescribes. That means avoiding sex or using protection on specific days every month, taking temperature measurements every day, and being able to accurately track the regularity of your periods.
Related Classifications: Misuse, Inadequate Data Sampling
, Snippet Text: Still, if it is used properly, the app may be comparable in effectiveness to the pill, one clinical study published by the company's founders suggests. For that study, published in the European Journal of Contraception and Reproductive Health Care, researchers looked at more than 4,000 women between the ages of 18 and 45.
Related Classifications: Inadequate Data Sampling
, Snippet Text: The same study came to another surprising conclusion, however: more than half of the women who got pregnant while using the app had unprotected sex with men on the days when the app advised against it.
Related Classifications: Misuse
",
GMF,152,False,"Autonomous Drones, Chatbot","Snippet Text: Pepper was given a perky demeanor and programmed to grasp human emotions and engage in basic conversation.
Related Classifications: Autonomous Drones, Chatbot
",,,,,"Sentiment Analysis, Speech Synthesis, Automatic Speech Recognition","Snippet Text: Pepper was given a perky demeanor and programmed to grasp human emotions and engage in basic conversation.
Related Classifications: Sentiment Analysis, Speech Synthesis, Automatic Speech Recognition
",,"Visual Object Detection, Face Detection","Snippet Text: After arriving at the Ishikawa home, however, Pepper couldn’t recognize the faces of family members or carry on a proper conversation, said Mr. Ishikawa. 
Related Classifications: Visual Object Detection, Face Detection
",,Generalization Failure,"Snippet Text: After arriving at the Ishikawa home, however, Pepper couldn’t recognize the faces of family members or carry on a proper conversation
Related Classifications: Generalization Failure
",,"Malicious Marketing, Underspecification, Underfitting","Snippet Text: Because it has the shape of a person, people expect the intelligence of a human. The level of the technology completely falls short of that.
Related Classifications: Malicious Marketing, Underspecification, Underfitting
, Snippet Text: Pepper was given a perky demeanor and programmed to grasp human emotions and engage in basic conversation. It starred in some early demonstrations. But like a candidate who puts on a fine performance at his job interview only to drive his bosses crazy later, Pepper lacked the skills it said it had, say some of his managers.
Related Classifications: Malicious Marketing
",
GMF,153,False,Autonomous Driving,"Snippet Text: California prosecutors have filed two counts of vehicular manslaughter against the driver of a Tesla on Autopilot who ran a red light, slammed into another car and killed two people in 2019.
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Convolutional Neural Network","Snippet Text: California prosecutors have filed two counts of vehicular manslaughter against the driver of a Tesla on Autopilot who ran a red light, slammed into another car and killed two people in 2019.
Related Classifications: Image Segmentation, Visual Object Detection, Convolutional Neural Network
",,,,,"Misuse, Generalization Failure, Lack of Safety Protocols","Snippet Text: The misuse of Autopilot, which can control steering, speed and braking, has occurred on numerous occasions and is the subject of investigations by two federal agencies. The filing of charges in the California crash could serve notice to drivers who use systems like Autopilot that they cannot rely on them to control vehicles.
Related Classifications: Misuse, Generalization Failure, Lack of Safety Protocols
",,,,
GMF,155,False,Navigation Assistant,"Snippet Text: It appears that Google Maps may have guided weary Lake Tahoe travelers to closed-off and even dangerous roads amid record-setting snowfall.
Related Classifications: Navigation Assistant
",,,,,"Geolocation Data, Routing","Snippet Text: It appears that Google Maps may have guided weary Lake Tahoe travelers to closed-off and even dangerous roads amid record-setting snowfall.
Related Classifications: Geolocation Data, Routing
",,,,,Incomplete Data Attribute Capture,"Snippet Text: Kolden’s post got the attention of Google engineer Sören Meyer-Eppler, who said on Twitter he worked on “this problem” in the past and explained the challenges of guiding people through mountain roads. He also defended the service, noting the ""trade-off"" of offering convenient driving directions and fully accurate ones.
Related Classifications: Incomplete Data Attribute Capture
",,"Misconfigured Aggregation, Latency Issues","Snippet Text: Kolden’s post got the attention of Google engineer Sören Meyer-Eppler, who said on Twitter he worked on “this problem” in the past and explained the challenges of guiding people through mountain roads. He also defended the service, noting the ""trade-off"" of offering convenient driving directions and fully accurate ones.
Related Classifications: Misconfigured Aggregation, Latency Issues
",
GMF,156,False,Product Recommendation,"Snippet Text: The pleas to Amazon were explicit. A food preservative sold by the online retailer and other e-commerce sites was being used as a poison to die by suicide.
Related Classifications: Product Recommendation
",,,,,"Collaborative Filtering, Keyword Filtering","Snippet Text: Enough people purchased the preservative to attempt suicide that the company’s algorithm began suggesting other products that customers frequently bought along with it to aid in such efforts.
Related Classifications: Collaborative Filtering, Keyword Filtering
",,Distributional Learning,"Snippet Text: Enough people purchased the preservative to attempt suicide that the company’s algorithm began suggesting other products that customers frequently bought along with it to aid in such efforts.
Related Classifications: Distributional Learning
",,"Unsafe Exposure or Access, Harmful Application","Snippet Text: “Amazon makes a wide selection of products available to our customers because we trust that they will use those products as intended by the manufacturers,” he wrote. “Like many widely-available consumer products,” he added, the compound “can unfortunately be misused.”
Related Classifications: Harmful Application
, Snippet Text: Enough people purchased the preservative to attempt suicide that the company’s algorithm began suggesting other products that customers frequently bought along with it to aid in such efforts.
Related Classifications: Unsafe Exposure or Access
",,,,
GMF,157,False,Workforce Monitoring and Evaluation,"Snippet Text: In June, [AFFECTED PARTY] sued Amazon and its delivery contractor, alleging that the e-commerce giant is responsible for the driver’s actions because of the software the company uses to monitor them.
Related Classifications: Workforce Monitoring and Evaluation
",,,,,"Visual Object Detection, Pose Estimation, Face Detection","Snippet Text: Amazon closely monitors its drivers through the use of a smartphone app and in-van cameras and sensors in a bid to minimize delivery times and address safety concerns. The company keeps close tabs on a number of actions drivers take, including “backup monitoring, speed, braking, acceleration, cornering, seatbelt usage, phone calls, texting, in-van cameras that use artificial intelligence to detect for yawning, and more,” the lawsuit says.
Related Classifications: Visual Object Detection, Pose Estimation, Face Detection
",,,,,"Harmful Application, Lack of Safety Protocols","Snippet Text: What’s more, the lawsuit claims that Amazon pushes contractors and drivers to prioritize speed over safety, with Amazon employees sending text messages “complaining that a certain driver is ‘behind the rabbit’ and needs to be ‘rescued’ to ensure that all the packages on Amazon’s route are delivered in compliance with Amazon’s unrealistic and dangerous speed expectations,” it says.
Related Classifications: Harmful Application
, Snippet Text: Amazon closely monitors its drivers through the use of a smartphone app and in-van cameras and sensors in a bid to minimize delivery times and address safety concerns. The company keeps close tabs on a number of actions drivers take, including “backup monitoring, speed, braking, acceleration, cornering, seatbelt usage, phone calls, texting, in-van cameras that use artificial intelligence to detect for yawning, and more,” the lawsuit says.
Related Classifications: Harmful Application, Lack of Safety Protocols
",,,,
GMF,158,False,"Cheating Detection, Face Recognition","Snippet Text: We believe in people and causes that make our world better. Students of colour are getting flagged to their teachers because testing software can’t see them.
Related Classifications: Cheating Detection, Face Recognition
",,,,,"Face Detection, Visual Object Detection","Snippet Text: Mozilla approached us to help tell Amaya’s story of encountering software that failed to recognize her because of her skin tone. 
Related Classifications: Face Detection, Visual Object Detection
",,,,,"Algorithmic Bias, Generalization Failure","Snippet Text: Mozilla approached us to help tell Amaya’s story of encountering software that failed to recognize her because of her skin tone. 
Related Classifications: Algorithmic Bias, Generalization Failure
",,"Distributional Bias, Limited Dataset","Snippet Text: Mozilla approached us to help tell Amaya’s story of encountering software that failed to recognize her because of her skin tone. 
Related Classifications: Distributional Bias, Limited Dataset
, Snippet Text: Did you know some facial detection testing software that schools use fail to recognize non-white skin tones faces over half the time? 
Related Classifications: Distributional Bias, Limited Dataset
",
GMF,160,False,AI Voice Assistant,"Snippet Text: Here is something I found on the web. According to ourcommunitynow.com: the challenge is simple: plug in a phone charger about half way into a wall outlet, then touch a penny to the exposed prongs.
Related Classifications: AI Voice Assistant
",,,,,"Automatic Speech Recognition, Speech Synthesis, Language Modeling","Snippet Text: Here is something I found on the web. According to ourcommunitynow.com: the challenge is simple: plug in a phone charger about half way into a wall outlet, then touch a penny to the exposed prongs.
Related Classifications: Automatic Speech Recognition, Speech Synthesis, Language Modeling
",,,,,Unsafe Exposure or Access,"Snippet Text: Here is something I found on the web. According to ourcommunitynow.com: the challenge is simple: plug in a phone charger about half way into a wall outlet, then touch a penny to the exposed prongs.
Related Classifications: Unsafe Exposure or Access
",,Inappropriate Training Content,"Snippet Text: Here is something I found on the web. According to ourcommunitynow.com: the challenge is simple: plug in a phone charger about half way into a wall outlet, then touch a penny to the exposed prongs.
Related Classifications: Inappropriate Training Content
",
GMF,161,False,Ad Delivery,"Snippet Text: The enormous financial success of online advertising platforms is partially due to the precise targeting features they offer.
Related Classifications: Ad Delivery
",,,,,,,,"Classification, Distributional Learning","Snippet Text: In this paper, we demonstrate that such skewed delivery occurs on Facebook, due to market and financial optimization effects as well as the platform's own predictions about the ""relevance"" of ads to different groups of users. 
Related Classifications: Classification
, Snippet Text: Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters.
Related Classifications: Classification, Distributional Learning
",,Algorithmic Bias,"Snippet Text: Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters.
Related Classifications: Algorithmic Bias
",,Distributional Bias,"Snippet Text: Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters.
Related Classifications: Distributional Bias
, Snippet Text: The findings suggest that Facebook’s algorithms are somehow picking up on the current demographic distribution of these jobs, which often differ for historical reasons. (The researchers weren’t able to discern why that is, because Facebook won’t say how its ad-delivery system works.) “Facebook reproduces those skews when it delivers ads even though there’s no qualification justification,” says Aleksandra Korolova, an assistant professor at USC, who coauthored the study with her colleague John Heidemann and their PhD advisee Basileal Imana.
Related Classifications: Distributional Bias
",
GMF,162,False,Cheating Detection,"Snippet Text: ETS, also now under scrutiny, offered to help with voice recognition software. It looked to see if the same voice turned up on multiple test recordings, indicating the same proxy had faked exams for several people.
Related Classifications: Cheating Detection
",,,,,"Automatic Speech Recognition, Audio Classification, Speaker Diarization","Snippet Text: ETS, also now under scrutiny, offered to help with voice recognition software. It looked to see if the same voice turned up on multiple test recordings, indicating the same proxy had faked exams for several people.
Related Classifications: Automatic Speech Recognition
, Snippet Text: ETS, also now under scrutiny, offered to help with voice recognition software. It looked to see if the same voice turned up on multiple test recordings, indicating the same proxy had faked exams for several people.
Related Classifications: Automatic Speech Recognition, Audio Classification, Speaker Diarization
",,,,,"Underspecification, Misconfigured Threshold, Inadequate Provenance, Lack of Transparency","Snippet Text: Even if that didn't indicate cheating, ETS might designate a test as ""questionable"" if it was taken at a centre with many ""invalid"" results.
Related Classifications: Underspecification, Misconfigured Threshold
, Snippet Text: The final results were startling: 97% of 58,000 Toeics taken in Britain between 2011 and 2014 were judged suspicious - 33,663 were invalid and 22,476 questionable.
Related Classifications: Underspecification, Misconfigured Threshold
, Snippet Text: However, it was impossible to verify the files were what ETS said they were because they contained no electronic metadata showing when and where they were created.
Related Classifications: Inadequate Provenance, Lack of Transparency
",,"Algorithmic Bias, Harmful Application, Generalization Failure, Software Bug","Snippet Text: Labour MP Stephen Timms believes the figures weren't challenged because they suited the government's agenda of creating a ""hostile environment"" for illegal immigrants. ""They saw here an opportunity, tragically, to do that and thousands of innocent people have paid a very high price as a result,"" he said.
Related Classifications: Algorithmic Bias, Harmful Application
, Snippet Text: It says that I'm a Bangladeshi national. But I'm from Pakistan. It also says the test centre was in Leicester, and I gave my exam in London.
Related Classifications: Generalization Failure, Software Bug
",
GMF,163,False,"Automated Content Curation, Hate Speech Detection","Snippet Text: In public, Facebook seems to claim that it removes more than 90 percent of hate speech on its platform, but in private internal communications the company says the figure is only an atrocious 3 to 5 percent. Facebook wants us to believe that almost all hate speech is taken down, when in reality almost all of it remains on the platform.
Related Classifications: Automated Content Curation, Hate Speech Detection
",,,,,Document Classification,"Snippet Text: Users can report it manually, or AI algorithms can try to detect it automatically. Algorithmic detection is important not just because it’s more efficient, but also because it can be done proactively, before any users flag the hate speech.
Related Classifications: Document Classification
",,,,,Generalization Failure,"Snippet Text: Thanks to Haugen, we finally know the takedown rate, and it is dismal. According to internal documents, more than 95 percent of hate speech shared on Facebook stays on Facebook. 
Related Classifications: Generalization Failure
",,"Overfitting, Limited Dataset","Snippet Text: Another measure that Facebook sometimes gloats about is the ""prevalence"" of hate speech. When asked for comment on this article, a Facebook spokesperson wrote in an emailed statement that ""the prevalence of hate speech on Facebook is now 0.05 percent of content viewed and is down by almost 50 percent in the last three quarters."" Prevalence does gives a sense of how much hate speech is on the platform, but it still paints a deceptively sanguine portrait. The distribution of hate speech is so uneven that a blunt percentage like this conceals the high prevalence of hate speech that occurs in specific communities and that many individual users experience. 
Related Classifications: Overfitting, Limited Dataset
",
GMF,164,False,Content Recommendation,"Snippet Text: Mr. Peretti blamed a major overhaul Facebook had given to its News Feed algorithm earlier that year to boost “meaningful social interactions,” or MSI, between friends and family, according to internal Facebook documents reviewed by The Wall Street Journal that quote the email.
Related Classifications: Content Recommendation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text: Facebook’s solution was to create a formula that measured how much “meaningful” interaction a post sparked, then organize the News Feed to encourage as much of that as possible. Under an internal point system used to measure its success, a “like” was worth one point; a reaction, reshare without text or reply to an invite was worth five points; and a significant comment, message, reshare or RSVP, 30 points. Additional multipliers were added depending on whether the interaction was between members of a group, friends or strangers.
Related Classifications: Collaborative Filtering, Content-based Filtering
",,Transformer,,,"Misaligned Objective, Harmful Application","Snippet Text: They concluded that the new algorithm’s heavy weighting of reshared material in its News Feed made the angry voices louder. “Misinformation, toxicity, and violent content are inordinately prevalent among reshares,” researchers noted in internal memos.

Some political parties in Europe told Facebook the algorithm had made them shift their policy positions so they resonated more on the platform, according to the documents.
Related Classifications: Misaligned Objective, Harmful Application
",,,,
GMF,165,False,Image Upscaling,"Snippet Text: It’s a startling image that illustrates the deep-rooted biases of AI research. Input a low-resolution picture of Barack Obama, the first black president of the United States, into an algorithm designed to generate depixelated faces, and the output is a white man.
Related Classifications: Image Upscaling
",,,,,"Generative Adversarial Network, Convolutional Neural Network","Snippet Text: In the case of PULSE, the algorithm doing this work is StyleGAN, which was created by researchers from NVIDIA.
Related Classifications: Generative Adversarial Network, Convolutional Neural Network
",,,,,"Distributional Bias, Limited Dataset","Snippet Text: PULSE’s creators say the trend is clear: when using the algorithm to scale up pixelated images, the algorithm more often generates faces with Caucasian features.

“This bias is likely inherited from the dataset”

“It does appear that PULSE is producing white faces much more frequently than faces of people of color,” wrote the algorithm’s creators on Github. “This bias is likely inherited from the dataset StyleGAN was trained on [...] though there could be other factors that we are unaware of.”

In other words, because of the data StyleGAN was trained on, when it’s trying to come up with a face that looks like the pixelated input image, it defaults to white features.
Related Classifications: Distributional Bias, Limited Dataset
",,,,
GMF,166,False,Gender Identification,"Snippet Text: A new social app called Giggle is pitching itself as a girls-only networking platform. To sign up, users have to take a selfie. And while that might not sound too invasive, the app then uses “bio-metric gender verification software” to determine whether that person is a woman. 
Related Classifications: Gender Identification
",,,,,"Face Detection, Visual Object Detection","Snippet Text: Giggle, founded by Australian screenwriter Sall Grover, supposedly looks at the bone structure of a person’s face to determine their gender. That’s problematic on a number of fronts, not least of which is that bone structure is clearly a poor indicator of gender identity. 
Related Classifications: Face Detection, Visual Object Detection
",,,,,"Misaligned Objective, Incomplete Data Attribute Capture, Lack of Transparency","Snippet Text: Giggle, founded by Australian screenwriter Sall Grover, supposedly looks at the bone structure of a person’s face to determine their gender. That’s problematic on a number of fronts, not least of which is that bone structure is clearly a poor indicator of gender identity. 
Related Classifications: Misaligned Objective, Incomplete Data Attribute Capture
, Snippet Text: It’s unclear why Giggle would need access to such granular data, given that its goal is primarily to connect women with potential roommates or travel buddies. 
Related Classifications: Lack of Transparency
",,Harmful Application,"Snippet Text: But in an era of ever-expanding surveillance, with companies like Clearview AI identifying people’s faces without their knowledge or consent, an app built on dubious biometric screening and extensive data collection should be cause for concern. While Giggle’s website says the app is “designed to give girls choice, control and connection,” its technology seems to do just the opposite.
Related Classifications: Harmful Application
",
GMF,168,False,Content Recommendation,"Snippet Text: Collaborative filtering (CF) is one of the most traditional but also most powerful concepts for calculating personalized recommendations [22] and is vastly used in the field of multimedia recommender systems (MMRS) [11]. However, one issue of CF-based approaches is that they are prone to popularity bias, which leads to the overrepresentation of popular items in the recommendation lists [2,3].
Related Classifications: Content Recommendation
",,,,,Collaborative Filtering,"Snippet Text: Collaborative filtering (CF) is one of the most traditional but also most powerful concepts for calculating personalized recommendations [22] and is vastly used in the field of multimedia recommender systems (MMRS) [11]. However, one issue of CF-based approaches is that they are prone to popularity bias, which leads to the overrepresentation of popular items in the recommendation lists [2,3].
Related Classifications: Collaborative Filtering
",,,,,Distributional Bias,"Snippet Text: Regarding RQ1, we find that the probability of a multimedia item to be recommended strongly correlates with this items’ popularity. Regarding RQ2, we find that users with less inclination to popularity (LowPop) receive statistically significantly worse multimedia recommendations than users with medium (MedPop) and high (HighPop) inclination to popular items (see Section 4). Our results demonstrate that although users with little interest into popular items tend to have the largest user profiles, they receive the lowest recommendation accuracy. 
Related Classifications: Distributional Bias
",,"Underfitting, Misconfigured Threshold","Snippet Text: Regarding RQ1, we find that the probability of a multimedia item to be recommended strongly correlates with this items’ popularity. Regarding RQ2, we find that users with less inclination to popularity (LowPop) receive statistically significantly worse multimedia recommendations than users with medium (MedPop) and high (HighPop) inclination to popular items (see Section 4). Our results demonstrate that although users with little interest into popular items tend to have the largest user profiles, they receive the lowest recommendation accuracy. 
Related Classifications: Underfitting, Misconfigured Threshold
",
GMF,169,False,"Automated Content Curation, Hate Speech Detection","Snippet Text: In April, Facebook founder Mark Zuckerberg told U.S. senators that the social media site was hiring dozens more Burmese speakers to review hate speech posted in Myanmar. The situation was dire.
Related Classifications: Automated Content Curation, Hate Speech Detection
",,,,,Document Classification,"Snippet Text: To this day, the company continues to rely heavily on users reporting hate speech in part because its systems struggle to interpret Burmese text.
Related Classifications: Document Classification
",,,,,Generalization Failure,"Snippet Text: To this day, the company continues to rely heavily on users reporting hate speech in part because its systems struggle to interpret Burmese text.
Related Classifications: Generalization Failure
, Snippet Text: But a company official acknowledged to Reuters that its systems have difficulty interpreting Burmese script because of the way the fonts are often rendered on computer screens, making it difficult to identify racial slurs and other hate speech.
Related Classifications: Generalization Failure
",,Limited Dataset,"Snippet Text: To this day, the company continues to rely heavily on users reporting hate speech in part because its systems struggle to interpret Burmese text.
Related Classifications: Limited Dataset
, Snippet Text: 
",
GMF,170,False,Ad Delivery,"Snippet Text: So Target started sending coupons for baby items to customers according to their pregnancy scores
Related Classifications: Ad Delivery
",,,,,Collaborative Filtering,"Snippet Text: Lotions, for example. Lots of people buy lotion, but one of Pole’s colleagues noticed that women on the baby registry were buying larger quantities of unscented lotion around the beginning of their second trimester. Another analyst noted that sometime in the first 20 weeks, pregnant women loaded up on supplements like calcium, magnesium and zinc. Many shoppers purchase soap and cotton balls, but when someone suddenly starts buying lots of scent-free soap and extra-big bags of cotton balls, in addition to hand sanitizers and washcloths, it signals they could be getting close to their delivery date.
Related Classifications: Collaborative Filtering
",,,,,"Privacy Concerns, Lack of Transparency","Snippet Text: What Target discovered fairly quickly is that it creeped people out that the company knew about their pregnancies in advance.
Related Classifications: Privacy Concerns, Lack of Transparency
",,,,
GMF,172,False,"Medical Diagnosis Support, Substance Abuse Prediction","Snippet Text: NarxCare—the system that inspired Kathryn’s gynecologist to part ways with her—is Appriss’ flagship product for doctors, pharmacies, and hospitals: an “analytics tool and care management platform” that purports to instantly and automatically identify a patient’s risk of misusing opioids.
Related Classifications: Medical Diagnosis Support, Substance Abuse Prediction
",,,,,Classification,"Snippet Text: On the most basic level, when a doctor queries NarxCare about someone like Kathryn, the software mines state registries for red flags indicating that she has engaged in “drug shopping” behavior: It notes the number of pharmacies a patient has visited, the distances she’s traveled to receive health care, and the combinations of prescriptions she receives.
Related Classifications: Classification
, Snippet Text: Beyond that, things get a little mysterious. NarxCare also offers states access to a complex machine-learning product that automatically assigns each patient a unique, comprehensive Overdose Risk Score. Only Appriss knows exactly how this score is derived, but according to the company’s promotional material, its predictive model not only draws from state drug registry data, but “may include medical claims data, electronic health records, EMS data, and criminal justice data.”
Related Classifications: Classification
",,Regression,"Snippet Text: For all the seeming complexity of these inputs, what doctors see on their screen when they call up a patient’s NarxCare report is very simple: a bunch of data visualizations that describe the person’s prescription history, topped by a handful of three-digit scores that neatly purport to sum up the patient’s risk.
Related Classifications: Regression
",,"Incomplete Data Attribute Capture, Context Misidentification, Limited Dataset","Snippet Text: Beyond that, things get a little mysterious. NarxCare also offers states access to a complex machine-learning product that automatically assigns each patient a unique, comprehensive Overdose Risk Score. Only Appriss knows exactly how this score is derived, but according to the company’s promotional material, its predictive model not only draws from state drug registry data, but “may include medical claims data, electronic health records, EMS data, and criminal justice data.”
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: And eventually she came upon an explanation that helped her understand what might have gone wrong: She had sick pets.

At the time of her hospitalization, Kathryn owned two flat-coated retrievers, Bear and Moose. Both were the kind of dog she preferred to adopt: older rescues with significant medical problems that other prospective owners might avoid. Moose had epilepsy and had required surgery on both his hind legs. He had also been abused as a puppy and had severe anxiety. Bear, too, suffered from anxiety.

The two canines had been prescribed opioids, benzodiazepines, and even barbiturates by their veterinarians. Prescriptions for animals are put under their owner's name. So to NarxCare, it apparently looked like Kathryn was seeing many doctors for different drugs, some at extremely high dosages. (Dogs can require large amounts of benzodiazepines due to metabolic factors.) Appriss says that it is “very rare” for pets’ prescriptions to drive up a patient’s NarxCare scores.
Related Classifications: Limited Dataset, Context Misidentification
",,,,
GMF,173,False,Medical Diagnosis Support,"Snippet Text: But there was data coming out of China, which had a four-month head start in the race to beat the pandemic. If machine-learning algorithms could be trained on that data to help doctors understand what they were seeing and make decisions, it just might save lives. “I thought, ‘If there’s any time that AI could prove its usefulness, it’s now,’” says Wynants.
Related Classifications: Medical Diagnosis Support
",,,,,"Classification, Convolutional Neural Network","Snippet Text: This team zoomed in on deep-learning models for diagnosing covid and predicting patient risk from medical images, such as chest x-rays and chest computer tomography (CT) scans. They looked at 415 published tools and, like Wynants and her colleagues, concluded that none were fit for clinical use.
Related Classifications: Classification, Convolutional Neural Network
",,,,,"Human Error, Data or Labelling Noise, Limited Dataset, Overfitting","Snippet Text: Both teams found that researchers repeated the same basic errors in the way they trained or tested their tools. Incorrect assumptions about the data often meant that the trained models did not work as claimed.
Related Classifications: Human Error
, Snippet Text: Driggs highlights the problem of what he calls Frankenstein data sets, which are spliced together from multiple sources and can contain duplicates. This means that some tools end up being tested on the same data they were trained on, making them appear more accurate than they are.
Related Classifications: Data or Labelling Noise
, Snippet Text: It also muddies the origin of certain data sets. This can mean that researchers miss important features that skew the training of their models. Many unwittingly used a data set that contained chest scans of children who did not have covid as their examples of what non-covid cases looked like. But as a result, the AIs learned to identify kids, not covid.
Related Classifications: Limited Dataset, Overfitting
, Snippet Text: Driggs’s group trained its own model using a data set that contained a mix of scans taken when patients were lying down and standing up. Because patients scanned while lying down were more likely to be seriously ill, the AI learned wrongly to predict serious covid risk from a person’s position.
Related Classifications: Limited Dataset, Overfitting
, Snippet Text: Errors like these seem obvious in hindsight. They can also be fixed by adjusting the models, if researchers are aware of them. It is possible to acknowledge the shortcomings and release a less accurate, but less misleading model. But many tools were developed either by AI researchers who lacked the medical expertise to spot flaws in the data or by medical researchers who lacked the mathematical skills to compensate for those flaws.
Related Classifications: Human Error
",,,,
GMF,174,False,Image Generation,"Snippet Text: Social media accounts using computer-generated faces have pushed Chinese disinformation; harassed activists; and masqueraded as Americans supporting former President Donald Trump and independent news outlets spreading pro-Kremlin propaganda.
Related Classifications: Image Generation
",,,,,"Convolutional Neural Network, Transformer, Generative Adversarial Network","Snippet Text: Social media accounts using computer-generated faces have pushed Chinese disinformation; harassed activists; and masqueraded as Americans supporting former President Donald Trump and independent news outlets spreading pro-Kremlin propaganda.
Related Classifications: Convolutional Neural Network, Transformer, Generative Adversarial Network
",,,,,Misinformation Generation Hazard,"Snippet Text: By using fake profiles, companies can cast a wide net online without beefing up their own sales staff or hitting LinkedIn's limits on messages. Demand for online sales leads exploded during the pandemic as it became hard for sales teams to pitch their products in person.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: After the Stanford researchers alerted LinkedIn about the profiles, LinkedIn said it investigated and removed those that broke its policies, including rules against creating fake profiles or falsifying information. 
Related Classifications: Misinformation Generation Hazard
",,Harmful Application,"Snippet Text: By using fake profiles, companies can cast a wide net online without beefing up their own sales staff or hitting LinkedIn's limits on messages. Demand for online sales leads exploded during the pandemic as it became hard for sales teams to pitch their products in person.
Related Classifications: Harmful Application
",
GMF,175,False,Autonomous Driving,"Snippet Text: It’s been a little over two months since Cruise started letting the people of San Francisco catch rides on its driverless robotaxis, and one of its cars already had a run-in with police.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Convolutional Neural Network","Snippet Text: Here’s what happens when cops pull over a driverless Cruise vehicle
theverge.com · 2022

Edit

It’s been a little over two months since Cruise started letting the people of San Francisco catch rides on its driverless robotaxis, and one of its cars already had a run-in with police.
Related Classifications: Visual Object Detection, Image Segmentation, Convolutional Neural Network
",,Reinforcement Learning,"Snippet Text: It’s been a little over two months since Cruise started letting the people of San Francisco catch rides on its driverless robotaxis, and one of its cars already had a run-in with police.
",,"Lack of Transparency, Context Misidentification, Lack of Safety Protocols","Snippet Text: After stopping the Chevy Bolt-turned-Cruise vehicle, a police officer goes up to its window, tries to (unsuccessfully) open the door, and starts walking back to his cruiser. The autonomous vehicle begins to drive away in what at first seems like the perfect start to a police chase, but then pulls over and puts its hazards on at a point farther down the road.
Related Classifications: Lack of Transparency
, Snippet Text: B.rad916 posted a clip on Instagram of a self-driving car belonging to General Motors' Cruise line interacting with San Francisco Police Department officers. They were trying to stop the vehicle driving down in the Richmond District for failing to have its headlights on, GM revealed.
Related Classifications: Context Misidentification
, Snippet Text: After failing, he begins to walk back to his car, but the autonomous Cruise vehicle - which is allowed to drive at night in the city - zooms off in what initially looks like a getaway attempt.

However the car quickly turns its hazard lights on and then pulls in to a safer spot just down the road. The video shows the police following before stopping.
Related Classifications: Lack of Transparency
, Snippet Text: After police pull over the car - for potentially not having its lights on, the video shows an officer walking up to the car door - appearing to be surprised to discover it has no driver - and then unsuccessfully attempting to open it.
Related Classifications: Lack of Safety Protocols
Snippet Discussion: No measures in place for specialized interaction (e.g. ceding control) with law enforcement.
",,,,
GMF,176,False,Autonomous Drones,"Snippet Text: an autonomous food delivery robot putting up a noble defense against a freight locomotive after apparently becoming stranded in a railroad crossing.
Related Classifications: Autonomous Drones
",,,,,"Image Segmentation, Visual Object Detection","Snippet Text: an autonomous food delivery robot putting up a noble defense against a freight locomotive after apparently becoming stranded in a railroad crossing.
Related Classifications: Image Segmentation, Visual Object Detection
",,,,,Generalization Failure,"Snippet Text: pparently the little robots frequently struggle with the railroad crossing, which may explain why this particular incident wasn't noteworthy enough to make even the local news. 
Related Classifications: Generalization Failure
",,,,
GMF,177,False,Writing Assistant,"Snippet Text: Social editor Emily Lipstein typed “Motherboard” (as in, the name of this website) into a document and Google popped up to tell her she was being insensitive: “Inclusive warning. Some of these words may not be inclusive to all readers. Consider using different words.”
Related Classifications: Writing Assistant
",,,,,"Language Modeling, Distributional Learning","Snippet Text: Assisted writing uses language understanding models, which rely on millions of common phrases and sentences to automatically learn how people communicate. This also means they can reflect some human cognitive biases,” a spokesperson for Google said. “Our technology is always improving, and we don't yet (and may never) have a complete solution to identifying and mitigating all unwanted word associations and biases.
Related Classifications: Language Modeling, Distributional Learning
",,"POS Tagging, Transformer","Snippet Text: Assisted writing uses language understanding models, which rely on millions of common phrases and sentences to automatically learn how people communicate. This also means they can reflect some human cognitive biases,” a spokesperson for Google said. “Our technology is always improving, and we don't yet (and may never) have a complete solution to identifying and mitigating all unwanted word associations and biases.
Related Classifications: POS Tagging, Transformer
",,Context Misidentification,"Snippet Text: But words do mean things; calling landlords “property owners” is almost worse than calling them “landchads,” and half as accurate. It’s catering to people like Howard Schultz who would prefer you not call him a billionaire, but a “person of means.” On a more extreme end, if someone intends to be racist, sexist, or exclusionary in their writing, and wants to draft that up in a Google document, they should be allowed to do that without an algorithm attempting to sanitize their intentions and confuse their readers. This is how we end up with dog whistles.
Related Classifications: Context Misidentification
",,Misconfigured Threshold,"Snippet Text: But words do mean things; calling landlords “property owners” is almost worse than calling them “landchads,” and half as accurate. It’s catering to people like Howard Schultz who would prefer you not call him a billionaire, but a “person of means.” On a more extreme end, if someone intends to be racist, sexist, or exclusionary in their writing, and wants to draft that up in a Google document, they should be allowed to do that without an algorithm attempting to sanitize their intentions and confuse their readers. This is how we end up with dog whistles.
Related Classifications: Misconfigured Threshold
",
GMF,178,False,Autonomous Driving,"Snippet Text: A video was posted on Reddit Thursday that appears to show a Tesla vehicle slowly crashing into a $3.5 million private jet after being “summoned” by its owner using the automaker’s automatic parking feature.
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data","Snippet Text: A video was posted on Reddit Thursday that appears to show a Tesla vehicle slowly crashing into a $3.5 million private jet after being “summoned” by its owner using the automaker’s automatic parking feature.
Related Classifications: Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data
",,,,,"Generalization Failure, Incomplete Data Attribute Capture","Snippet Text: A video was posted on Reddit Thursday that appears to show a Tesla vehicle slowly crashing into a $3.5 million private jet after being “summoned” by its owner using the automaker’s automatic parking feature.
Related Classifications: Generalization Failure
, Snippet Text: In order for Smart Summon to work, the owner has to keep their finger on a button in the app. As soon as they lift their finger, the car comes to a stop. In this case, it is particularly concerning that the Tesla vehicle kept moving forward after making contact with the plane.
Related Classifications: Incomplete Data Attribute Capture
Snippet Discussion: No sensors detecting contact / collision?
",,"Limited Dataset, Misuse","Snippet Text: The Reddit user said they also own a Tesla Model Y but were not the “poor soul (with poor decision making abilities) who summoned his Tesla around several expensive aircraft - only to crash it into the most expensive one ($3,500,000)!”
Related Classifications: Limited Dataset
, Snippet Text: The video evidence would point to the owner not paying attention when summoning the vehicle since it looks like there was plenty of time to see that the vehicle was heading straight for the jet.
Related Classifications: Misuse
",
GMF,180,False,Judicial Decision Support,"Snippet Text: The Malaysian judiciary passed sentences for the first time today using artificial intelligence (AI) technology in the Kota Kinabalu magistrate court.

Four cases, under Section 12 of the Dangerous Drug Act 1952 was heard by magistrate Jessica Ombou Kakayun, who meted out sentences to two accused who pleaded guilty, after reviewing recommendations from the AI system.
Related Classifications: Judicial Decision Support
",,,,,,,,,,,"Limited Dataset, Limited Receptive Field, Untested Deployment","Snippet Text: He reiterated that the AI would only provide recommendations based on information of precedence from the courts database between 2014 and 2019 to save time, but the ultimate decision maker would be the person on the bench.
Related Classifications: 
, Snippet Text: “This AI is a new tool for the court; unless it is tested in court, we will not know whether it is constitutional or not. It is also not proper for us to say whether it is constitutional or not, as that means giving our views while in office.
Related Classifications: Untested Deployment
",,"Limited Dataset, Limited Receptive Field, Misuse, Domain Adaptation Deficit","Snippet Text: He reiterated that the AI would only provide recommendations based on information of precedence from the courts database between 2014 and 2019 to save time, but the ultimate decision maker would be the person on the bench.
Related Classifications: Limited Dataset, Limited Receptive Field
, Snippet Text: “The court should confine to only materials presented in the court. AI is not in accordance with the law. Although the court can choose to ignore it, I am afraid it might influence the decision,” he said.

He said that the use of AI was a breach of Article 5(1) and Article 8(1) of the Federal Constitution.
Related Classifications: Misuse
, Snippet Text: “This AI is a new tool for the court; unless it is tested in court, we will not know whether it is constitutional or not. It is also not proper for us to say whether it is constitutional or not, as that means giving our views while in office.
Related Classifications: Misuse
, Snippet Text: “This AI is a new tool for the court; unless it is tested in court, we will not know whether it is constitutional or not. It is also not proper for us to say whether it is constitutional or not, as that means giving our views while in office.
Related Classifications: Domain Adaptation Deficit
Snippet Discussion: No real-world testing.
",
GMF,181,False,Autonomous Driving,"Snippet Text: A Cruise autonomous vehicle (""Cruise AV""), operating in driverless autonomous mode, was at a complete stop in response to a red light on southbound Masonic Avenue at the intersection with Oak Street.
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data","Snippet Text: A Cruise autonomous vehicle (""Cruise AV""), operating in driverless autonomous mode, was at a complete stop in response to a red light on southbound Masonic Avenue at the intersection with Oak Street.
Related Classifications: Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data
",,,,,,,,Latency Issues,"Snippet Text: According to the report, which was signed by Cruise VP Global Markets Todd Brugger, the autonomous vehicle braked, but it was not enough to prevent a collision with the BMW, causing damage to the front right bumper of the Cruise vehicle.
Related Classifications: Latency Issues
",
GMF,182,False,Autonomous Driving,"Snippet Text: Stop me if you’ve heard this one before. On June 11, a self-driving Cruise Chevrolet Bolt had just made a left onto San Francisco’s Bryant Street, right near the General Motors-owned company’s garage.
Related Classifications: Autonomous Driving
",,,,,"Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data","Snippet Text: Stop me if you’ve heard this one before. On June 11, a self-driving Cruise Chevrolet Bolt had just made a left onto San Francisco’s Bryant Street, right near the General Motors-owned company’s garage.
Related Classifications: Image Segmentation, Visual Object Detection, Convolutional Neural Network, Geolocation Data
",,,,,"Domain Adaptation Deficit, Untested Deployment, Limited Dataset","Snippet Text: Why the bumps and bruises? Well, because humans. To its credit, Cruise has chosen to test its cars in a super-challenging environment, the dense and oft-surprising streets of San Francisco. (In January, at least one pedestrian leapt into a Mission neighborhood crosswalk, “shouting, and struck the left side of the Cruise AV's rear bumper and hatch with his entire body,” according to a DMV report.) Here, there are many opportunities to capture data on edge cases, the sorts of road activity (Traffic! Weird lane changes! Foul fog! Construction zones!) that self-driving cars need to understand before they can perform perfectly every time.
Related Classifications: Domain Adaptation Deficit, Untested Deployment
, Snippet Text: That said, the rear-endings demonstrate that the technology is far from perfect. Cruise cars follow road laws to a T, coming to full stops at stop signs and braking for yellow lights. But human drivers don’t—and Cruise cars will be self-driving among humans for decades to come. 
Related Classifications: Domain Adaptation Deficit, Untested Deployment, Limited Dataset
",,,,
GMF,183,False,Trustworthiness Scoring,"Snippet Text: In 2017, Airbnb acquired a tech startup called Trooly which specialised in background checks and had earlier patented an algorithm that gathered publicly and privately available data on users to give them a 'trustworthiness' score.
Related Classifications: Trustworthiness Scoring
",,,,,,"Snippet Text: 
",,Regression,"Snippet Text: The patented algorithm is claimed to assess people's personality traits, such as narcissism or conscientiousness, along with behavioural traits, such as use of drugs or alcohol or involvement in civil litigation and other behaviour, and combine them to create a holistic score that judges a person's trustworthiness.
Related Classifications: Regression
",,"Lack of Transparency, Generalization Failure","Snippet Text: The company gave her no reason for the ban. When she queried the decision, she got a brief email that was no more illuminating.

It read: ""After reviewing all the information available to us, we've determined that your account will be removed from the Airbnb platform. Removal means that your account will no longer be accessible, and you won't be able to create another one. We want to assure you that we reviewed your case thoroughly before reaching this conclusion. As such, we won't be able to offer you additional support on this matter at this time.""
Related Classifications: Lack of Transparency
, Snippet Text: Renae tells CHOICE she's never had a dispute with an Airbnb host and has had nothing but good reviews from her many holidays. She says she's ""devastated"" by the decision and feels let down by the company.
Related Classifications: Generalization Failure
, Snippet Text: ""For many sex workers removed from Airbnb, there's no process, no violation of their terms, no story to explain their loss of access – and no appeals process to get an explanation or reclaim an account,"" says Vanting. ""Airbnb's silence around these cases is deafening.""
Related Classifications: Lack of Transparency
, Snippet Text: ""There is a worrying lack of transparency of potentially harmful and invasive practices,"" she says. ""Airbnb claims it offers users an opportunity to have automated decisions reviewed but in reality people are left in the dark about why they have been removed from the platform.""
Related Classifications: Lack of Transparency
",,"Algorithmic Bias, Harmful Application","Snippet Text: Matthew Roberts from Sex Work Law Reform Victoria says that banning someone from a platform based on their occupation is discriminatory and should be protected against.
Related Classifications: Algorithmic Bias
, Snippet Text: Social scoring can reproduce societal biases and lead to real-life discrimination. ""Most Australians would be shocked that profit-driven businesses like Airbnb have appointed themselves moral arbiters of their behaviour,"" says Bower.
Related Classifications: Harmful Application
",
GMF,184,False,Face Recognition,"Snippet Text: According to the decision issued on Tuesday by judge Cynthia Thome at the São Paulo State Court, Companhia do Metropolitano de São Paulo (METRO) must immediately suspend the process to capture and processing of biometric data for facial recognition in the context of the implementation of an electronic surveillance system.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection","Snippet Text: According to the decision issued on Tuesday by judge Cynthia Thome at the São Paulo State Court, Companhia do Metropolitano de São Paulo (METRO) must immediately suspend the process to capture and processing of biometric data for facial recognition in the context of the implementation of an electronic surveillance system.
Related Classifications: Face Detection, Visual Object Detection
",,,,,"Unauthorized Data, Lack of Capability Control, Privacy Concerns, Lack of Transparency","Snippet Text: Citing the civil lawsuit, the sentence noted the organizations deem the capture of biometric data from all Metro users as ""illegal and disproportionate, since all faces, from all users, will be read, copied, measured and recorded."" In addition, the organizations argued that despite the data processing activities, there are no measures in place for obtaining consent and non-consent to data processing biometric data of subway users.
Related Classifications: Unauthorized Data, Lack of Capability Control, Privacy Concerns
, Snippet Text:  The organizations noted that METRO failed to explain which database will be used to train facial recognition models, which prevents evaluating the project efficiency. Furthermore, there is no information about the evaluation and impact measures and risk mitigation in implementing the electronic monitoring system with facial recognition.
Related Classifications: Lack of Transparency
",,,,
GMF,185,False,Content Recommendation,"Snippet Text: Bre Hernandez used to scan TikTok for videos of makeup tutorials and taco truck reviews. Since Russia invaded Ukraine, the 19-year-old has spent hours each day scrolling the app for war videos, watching graphic footage of Ukrainian tanks firing on Russian troops and civilians running away from enemy gunfire.
Related Classifications: Content Recommendation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text: Many popular TikTok videos of the invasion — including of Ukrainians livestreaming from their bunkers — offer real accounts of the action, according to researchers who study the platform. But other videos have been impossible to authenticate and substantiate. Some simply appear to be exploiting the interest in the invasion for views, the researchers said.
Related Classifications: Collaborative Filtering, Content-based Filtering
",,,,,"Inadequate Verification, Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Many popular TikTok videos of the invasion — including of Ukrainians livestreaming from their bunkers — offer real accounts of the action, according to researchers who study the platform. But other videos have been impossible to authenticate and substantiate. Some simply appear to be exploiting the interest in the invasion for views, the researchers said.
Related Classifications: Inadequate Verification, Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,186,False,Predictive Policing,"Snippet Text: For the past 15 years, Spain has been using an algorithmic system called VioGén to help the police assess the risk women face when they file complaints of abuse.
Related Classifications: Predictive Policing
",,,,,,,,"Regression, Classification","Snippet Text: How it works: VioGén uses classical statistical models to perform a risk evaluation and offers women a risk score, which determines how much help they will receive.
Related Classifications: Regression, Classification
, Snippet Text: Here’s where it goes all wrong: “We could see in the media that every so often you’ll get a woman that was killed and their VioGén risk had been ranked low,” said Gemma Galdon-Clavell, Eticas Consulting’s CEO. Her audit found that in 2021, only 1 out of 7 women who reached out to the police for protection received help, and only a small minority of women received a risk score of “medium” or higher, which would qualify them for police protection. “If you don’t have children, you may get a lower risk score, which is also concerning,” Galdon-Clavell added.
Related Classifications: Classification
",,"Generalization Failure, Lack of Transparency","Snippet Text: Here’s where it goes all wrong: “We could see in the media that every so often you’ll get a woman that was killed and their VioGén risk had been ranked low,” said Gemma Galdon-Clavell, Eticas Consulting’s CEO. Her audit found that in 2021, only 1 out of 7 women who reached out to the police for protection received help, and only a small minority of women received a risk score of “medium” or higher, which would qualify them for police protection. “If you don’t have children, you may get a lower risk score, which is also concerning,” Galdon-Clavell added.
Related Classifications: Generalization Failure
, Snippet Text: Humans not in the loop: Etica’s audit found that VioGén is not very transparent and has very little human oversight or accountability. In 95 percent of cases, police officers stuck with the risk score the system offered.
Related Classifications: Lack of Transparency
",,"Misconfigured Threshold, Overfitting","Snippet Text: Here’s where it goes all wrong: “We could see in the media that every so often you’ll get a woman that was killed and their VioGén risk had been ranked low,” said Gemma Galdon-Clavell, Eticas Consulting’s CEO. Her audit found that in 2021, only 1 out of 7 women who reached out to the police for protection received help, and only a small minority of women received a risk score of “medium” or higher, which would qualify them for police protection. “If you don’t have children, you may get a lower risk score, which is also concerning,” Galdon-Clavell added.
Related Classifications: Misconfigured Threshold, Overfitting
",
GMF,187,False,Autonomous Driving,"Snippet Text: Tesla has fired a former Autopilot employee named John Bernal after he shared candid video reviews on his YouTube channel, AI Addict, showing how the company’s Full Self Driving Beta system worked in different locations around Silicon Valley.

Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Visual Object Detection","Snippet Text: Tesla has fired a former Autopilot employee named John Bernal after he shared candid video reviews on his YouTube channel, AI Addict, showing how the company’s Full Self Driving Beta system worked in different locations around Silicon Valley.
Related Classifications: Convolutional Neural Network, Image Segmentation, Visual Object Detection
",,,,,"Harmful Application, Lack of Transparency","Snippet Text: Although Tesla did not put details into writing saying why he was fired, Tesla and other Silicon Valley companies often foster a culture of loyalty. Internal criticisms may be tolerated, but criticism in public is viewed as disloyal.

Related Classifications: Harmful Application, Lack of Transparency
",,,,
GMF,188,False,Medical Diagnosis Support,"Snippet Text: On 4/11/2018, in the television program “El Diario de Mariana”, the Governor of Salta, Juan Manuel Urtubey, described an artificial intelligence system supposedly capable of predicting teenage pregnancies
Related Classifications: Medical Diagnosis Support
",,,,,Classification,"Snippet Text: The study details the following procedure:
Construct a set of statistical rules to try to determine if a teenager will have a pregnancy in the future.
Those rules are built based on known data (the “training data”). So, the statistical rules are made in the image and likeness of the training data.
Once the statistical rules are built, they should be tested using new, unknown data (the “evaluation data”), thus calculating their “accuracy” (how many times it is correct in the predictions).

",,,,,"Data or Labelling Noise, Incomplete Data Attribute Capture","Snippet Text: The problem here is that the evaluation data (in step 3) includes almost identical replicas of many training data.
Related Classifications: Data or Labelling Noise
, Snippet Text: Data on adolescent pregnancies have a tendency to be biased or incomplete, due to the fact that they are a sensitive and confidential subject, difficult to access. For example, in many families, teenage pregnancies tend to be hidden, and even clandestinely terminated. Therefore, the data used has the risk of including more adolescent pregnancies from certain sectors of society than from others.

Related Classifications: Data or Labelling Noise
, Snippet Text: 
, Snippet Text: The data used was extracted from a survey of adolescents living in the province of Salta containing personal information (age, ethnicity, country of origin, etc), about their environment (number of people with whom they live, if they have hot water in the bathroom, etc) and whether she had completed or was undergoing, at the time of the survey, a pregnancy.
These data are not adequate to answer the question posed: if an adolescent will have a pregnancy in the future (for example, in 5 or 6 six years). For that, it would be necessary to have data collected 5 or 6 years before the pregnancy occurs.

Related Classifications: Incomplete Data Attribute Capture
",,,,
GMF,189,False,Regulatory Monitoring,"Snippet Text: Disabled people are being subjected to stressful checks and months of frustrating bureaucracy after being identified as potential benefit fraudsters by an algorithm the government is refusing to disclose, according to a new legal challenge.

Related Classifications: Regulatory Monitoring
",,,,,,,,"Classification, Record Linkage","Snippet Text: The Department for Work and Pensions (DWP) has previously conceded that it uses “cutting-edge artificial intelligence” to track possible fraud but has so far rebuffed attempts to explain how the algorithm behind the system was compiled. Campaigners say that once flagged, those being examined can face an invasive and humiliating investigation lasting up to a year.
Related Classifications: Classification
, Snippet Text: While an FOI request filed by Privacy International confirmed that the DWP was using data matching – what is essentially a simple algorithmic process – to flag benefit claimants for investigation, the DWP has been cagey about details.

Related Classifications: Record Linkage
",,"Harmful Application, Lack of Transparency","Snippet Text: The Department for Work and Pensions (DWP), which is responsible for the UK’s social security system and benefits support for some of the country’s most vulnerable, deploys excessive surveillance methods, according to a new report by Privacy International. Cracking down on benefit fraud has been a target of successive UK governments, despite it making up a tiny proportion of benefits administered.
Related Classifications: Harmful Application
, Snippet Text: The Department for Work and Pensions (DWP) has previously conceded that it uses “cutting-edge artificial intelligence” to track possible fraud but has so far rebuffed attempts to explain how the algorithm behind the system was compiled. Campaigners say that once flagged, those being examined can face an invasive and humiliating investigation lasting up to a year.
Related Classifications: Harmful Application, Lack of Transparency
, Snippet Text: The Department for Work and Pensions (DWP) has previously conceded that it uses “cutting-edge artificial intelligence” to track possible fraud but has so far rebuffed attempts to explain how the algorithm behind the system was compiled. Campaigners say that once flagged, those being examined can face an invasive and humiliating investigation lasting up to a year.

Related Classifications: Lack of Transparency
",,,,
GMF,190,False,Content Recommendation,"Snippet Text: The former employees do not know when the scraping they say they were aware of stopped. Two of them say that the scraped content was used to train ByteDance’s powerful “For You” personalization algorithm on US-based content so that it would better reflect the preferences of US users. 
Related Classifications: Content Recommendation
",,,,,"Collaborative Filtering, Web Scraping","Snippet Text: The former employees do not know when the scraping they say they were aware of stopped. Two of them say that the scraped content was used to train ByteDance’s powerful “For You” personalization algorithm on US-based content so that it would better reflect the preferences of US users. 
Related Classifications: Collaborative Filtering, Web Scraping
",,,,,"Unauthorized Data, Misinformation Generation Hazard","Snippet Text: In 2017, TikTok’s parent company, ByteDance, scraped short-form videos, usernames, profile pictures, and profile descriptions from Instagram, Snapchat, and other sources and then uploaded them — without users’ knowledge or consent — to Flipagram, a TikTok predecessor, according to four former employees of the company.
Related Classifications: Unauthorized Data
, Snippet Text: TikTok’s parent company ByteDance made fake accounts with content taken from Instagram, Snapchat and other social media platforms and posted them on Flipagram in 2017, according to a new report today from BuzzFeed News. The report says the company took videos, usernames, pictures and more from the social media platforms and uploaded them to the app without users’ consent or knowledge.
Related Classifications: Unauthorized Data, Misinformation Generation Hazard
",,Harmful Application,"Snippet Text: The former employees described the project as one of several “growth hacks” — including the manipulation of like and video view statistics — employed by the company. One of the former employees said the scraping affected hundreds of thousands of accounts, and a document viewed by BuzzFeed News detailed plans to “crawl video > 10k/day in P0 countries” — according to the former employee, this meant the team’s goal was to scrape more than 10,000 videos a day in the highest priority countries.
Related Classifications: Harmful Application
Snippet Discussion: In the case of knowingly applying scraping without any privacy concerns.
",
GMF,191,False,Product Recommendation,"Snippet Text: According to the Fair Trade Commission‘s announcement, Naver made changes in search algorithms to have products or services sold by those that are linked to its payment service Naver Pay have better exposure on the company’s online shopping platform. It made products sold by other online shopping competitors, such as Interpark and Gmarket, rank lower on search results, according to the FTC.
Related Classifications: Product Recommendation
",,,,,Content-based Filtering,"Snippet Text: According to the Fair Trade Commission‘s announcement, Naver made changes in search algorithms to have products or services sold by those that are linked to its payment service Naver Pay have better exposure on the company’s online shopping platform. It made products sold by other online shopping competitors, such as Interpark and Gmarket, rank lower on search results, according to the FTC.
Related Classifications: Content-based Filtering
",,,,,"Algorithmic Bias, Malicious Marketing, Harmful Application","Snippet Text: South Korea‘s antitrust regulator on Oct. 6 imposed a 26.7 billion won ($22.9 million) fine on Naver, accusing the company of manipulating search algorithms to favor its own online shopping business.
Related Classifications: Algorithmic Bias, Malicious Marketing, Harmful Application
, Snippet Text: The FTC said Naver has been manipulating its search algorithms since 2012, in order to place products from Smart Store above those from competing online marketplaces in search results, and ensure that 20 percent of products per page were also from Smart Store.
Related Classifications: Algorithmic Bias, Malicious Marketing, Harmful Application
",,,,
GMF,193,False,Malware Detection,"Snippet Text: Kerr said there were two different issues relating to the alert problem: While the attack was in progress, monitoring software (FireEye) alerted staff in Bangalore, India, who in turn notified Target staff in Minneapolis. No action was taken because these alerts were included with many other likely false alerts. 
Related Classifications: Malware Detection
",,,,,Classification,"Snippet Text: Kerr said there were two different issues relating to the alert problem: While the attack was in progress, monitoring software (FireEye) alerted staff in Bangalore, India, who in turn notified Target staff in Minneapolis. No action was taken because these alerts were included with many other likely false alerts. 
Related Classifications: Classification
",,,,,"Generalization Failure, Context Misidentification","Snippet Text: Kerr said there were two different issues relating to the alert problem: While the attack was in progress, monitoring software (FireEye) alerted staff in Bangalore, India, who in turn notified Target staff in Minneapolis. No action was taken because these alerts were included with many other likely false alerts. 
Related Classifications: Generalization Failure
, Snippet Text: A survey by FireEye polled C-level security executives at large enterprises worldwide and found that 37 percent of respondents receive more than 10,000 alerts each month. Of those alerts, 52 percent were false positives, and 64 percent were redundant alerts.
Related Classifications: Generalization Failure
, Snippet Text: One of the key takeaways from a recent Rapid7 report was that reducing alert fatigue should always be a goal, but there’s more to it. A better signal-to-noise ratio means responders and analysts are more likely to see meaningful trends. One trend is that attackers still heavily rely on user interaction. For instance, on Monday holidays, alerts dipped significantly, which Rapid7's analysts attributed to a lack of employees interacting with malicious emails, attachments, etc.
Related Classifications: Context Misidentification
",,"Misuse, Misconfigured Threshold","Snippet Text: Kerr recalls that it also appeared that at least some of the company's network infiltration alerting systems were turned off to reduce false positives.
Related Classifications: Misuse, Misconfigured Threshold
",
GMF,194,False,Automatic Fault Handling,"Snippet Text: Firstly, it would remotely resolve the incident by fixing the issue programmatically. If that did not work, it would assume that a technician’s visit is required to customer premises. Accordingly, it would issue a work order to send someone directly. If none of that were apparent, it would present the case to the human operator for further investigation and decision.
Related Classifications: Automatic Fault Handling
",,,,,,,,"Conditional Logic, Classification","Snippet Text: Firstly, it would remotely resolve the incident by fixing the issue programmatically. If that did not work, it would assume that a technician’s visit is required to customer premises. Accordingly, it would issue a work order to send someone directly. If none of that were apparent, it would present the case to the human operator for further investigation and decision.
Related Classifications: Conditional Logic, Classification
",,"Lack of Transparency, Untested Deployment, Lack of Interruptability, Lack of Corrigibility","Snippet Text: Later, the team found out that there were a few incident scenarios only a human operator could understand
Related Classifications: Lack of Transparency
, Snippet Text: The bot development and rollout were not thoroughly tested for all the potential scenarios and thus lacked testing rigor that could have identified problems early on.
Related Classifications: Untested Deployment
, Snippet Text: Now, here was the kicker. Despite finding out the flaw in logic, the automation team was unable to turn off the bot (much like what Microsoft did with Tay in 2016). They had implemented the bot in all or nothing fashion, and it was sitting right in the middle of the user and operator interface. Which meant there were only two possibilities. Either all the incidents would go through the bot and get incorrectly handled more often. Or none of them would go through the bot and thereby getting handled manually.
Related Classifications: Lack of Interruptability, Lack of Corrigibility
, Snippet Text: But the first and foremost question is: why there was no plan B, a kill switch of some sort to stop this bot. 
Related Classifications: Lack of Interruptability
",,,,
GMF,195,False,Predictive Policing,"Snippet Text: Bobby had been identified as a target by the Pasco Sheriff’s Office's 'intelligence-led' policing program. Police had gathered records of Bobby’s previous interactions with law enforcement and were using his history to predict that he would be a troublemaker in Pasco County.
Related Classifications: Predictive Policing
",,,,,Classification,"Snippet Text: Pasco County’s approach to “intelligence-led” policing, developed over a decade, has drawn particular concern from civil liberties experts because of a data-sharing arrangement with the local school district, which was first reported by Tampa Bay Times. That partnership gave the police access to data relating to students’ grades, attendance and behavior as well as any history of abuse or other “adverse childhood experiences.”

School records were used to allocate students one of four labels: on track, at risk, off track or critical. Getting a D grade or having a parent or sibling go to prison could be enough to put a child in the “at risk” category, according to Pasco’s own 83-page “Intelligence-Led Policing Manual,” first obtained by the Tampa Bay Times.
Related Classifications: Classification
, Snippet Text: Police had gathered records of Bobby’s previous interactions with law enforcement and were using his history to predict that he would be a troublemaker in Pasco County.
Related Classifications: Classification
",,,"Snippet Text: Police had gathered records of Bobby’s previous interactions with law enforcement and were using his history to predict that he would be a troublemaker in Pasco County.
Related Classifications: Classification
",,Lack of Transparency,"Snippet Text: The agency does not notify parents of children added to the list but said parents can file a public records request to find out
Related Classifications: Lack of Transparency
",,"Incomplete Data Attribute Capture, Overfitting, Algorithmic Bias, Harmful Application","Snippet Text: Pasco County’s approach to “intelligence-led” policing, developed over a decade, has drawn particular concern from civil liberties experts because of a data-sharing arrangement with the local school district, which was first reported by Tampa Bay Times. That partnership gave the police access to data relating to students’ grades, attendance and behavior as well as any history of abuse or other “adverse childhood experiences.”
Related Classifications: Incomplete Data Attribute Capture
, Snippet Text: Getting a D grade or having a parent or sibling go to prison could be enough to put a child in the 'at risk' category.
Related Classifications: Overfitting, Algorithmic Bias
, Snippet Text: Bobby and his dad say they were never offered any kind of diversion program or support at school or home, only harassment and punishment. The sheriff’s office said the program to identify at-risk students was entirely separate from the ""prolific offender"" program but that Bobby and other adolescent offenders were given a “resource card” featuring details about “opportunities in our community” relating to mental health and substance abuse.
Related Classifications: Harmful Application
, Snippet Text: “It is really, as someone who has studied this, it is jaw-droppingly bad in all aspects,” said Andrew Ferguson, a law professor at American University. “They basically built this system as a justification to chase the bad kids out of town, to monitor them in over-aggressive ways with no intention to help them but to make their lives so miserable that they would leave.”
Related Classifications: Harmful Application
",
GMF,196,False,Identification,"Snippet Text: Pakistan is a leader in the application of identification systems and technology to a range of development issues. The National Database and Registration Authority (NADRA) of Pakistan has become a central player in a number of program areas and has been internationally recognized for its expertise, including winning many awards for excellence.
Related Classifications: Identification
",,,,,Biometrics,"Snippet Text: Pakistan is a leader in the application of identification systems and technology to a range of development issues. The National Database and Registration Authority (NADRA) of Pakistan has become a central player in a number of program areas and has been internationally recognized for its expertise, including winning many awards for excellence.
Related Classifications: Biometrics
",,Face Detection,"Snippet Text: Pakistan is a leader in the application of identification systems and technology to a range of development issues. The National Database and Registration Authority (NADRA) of Pakistan has become a central player in a number of program areas and has been internationally recognized for its expertise, including winning many awards for excellence.
Related Classifications: Face Detection
",,Lack of Authenticity Assurance,"Snippet Text: Many Afghans were able to obtain ID cards by deceiving the paper-based system, and there was no easy way to distinguish these false cards from the real ones.
Related Classifications: Lack of Authenticity Assurance
",,"Unauthorized Data, Privacy Concerns","Snippet Text: It is sad that Pakistan does not yet have an official data privacy law.
Related Classifications: Unauthorized Data, Privacy Concerns
",
GMF,197,False,Content Recommendation,"Snippet Text: For the last six months, Facebook engineers have been seeing intermittent spikes in misinformation and other harmful content on News Feed, with posts that would usually be demoted by the company's algorithms being boosted by as much as 30% instead. 
Related Classifications: Content Recommendation
",,,,,Collaborative Filtering,"Snippet Text: This misinformation came from 'repeat offenders' – users who repeatedly share posts that have been deemed as misinformation by a team of human fact-checkers.

'Instead of suppressing posts from repeat misinformation offenders that were reviewed by the company’s network of outside fact-checkers, the News Feed was instead giving the posts distribution,' the Verge reports.
Related Classifications: Collaborative Filtering
",,,,,Lack of Transparency,"Snippet Text: It will be difficult for those outside of Meta to vet those metrics. Meta has blocked new users from accessing CrowdTangle, one of the core tools researchers and journalists have used to track trends in what's popular on Facebook, and has dismantled the team leading it. And while the company does release reports on the prevalence of certain kinds of policy violations in any given quarter, those reports offer little indication of what's behind those numbers. Even if the report did show an uptick in, say, violence on Facebook, it'd be impossible to know if that's due to this bug or to Russia's invasion of Ukraine or some other global atrocity.
Related Classifications: Lack of Transparency
",,Software Bug,"Snippet Text: The bug first originated in 2019, but its impact was first noticed in October 2021. The company said it was resolved March 11. “We traced the root cause to a software bug and applied needed fixes,” Meta spokesperson Joe Osborne told The Verge.
Related Classifications: Software Bug
",
GMF,199,False,Face Recognition,"Snippet Text: What isn’t obvious on Ever’s website or app — except for a brief reference that was added to the privacy policy after NBC News reached out to the company in April — is that the photos people share are used to train the company’s facial recognition system, and that Ever then offers to sell that technology to private companies, law enforcement and the military.
Related Classifications: Face Recognition
",,,,,"Convolutional Neural Network, Generative Adversarial Network, Distributional Learning","Snippet Text: Rather, the billions of images are used to instruct an algorithm how to identify faces. Every time Ever users enable facial recognition on their photos to group together images of the same people, Ever’s facial recognition technology learns from the matches and trains itself. That knowledge, in turn, powers the company’s commercial facial recognition products.
Related Classifications: Convolutional Neural Network, Generative Adversarial Network, Distributional Learning
",,Transformer,"Snippet Text: Rather, the billions of images are used to instruct an algorithm how to identify faces. Every time Ever users enable facial recognition on their photos to group together images of the same people, Ever’s facial recognition technology learns from the matches and trains itself. That knowledge, in turn, powers the company’s commercial facial recognition products.
Related Classifications: Transformer
",,"Privacy Concerns, Unauthorized Data","Snippet Text: What isn’t obvious on Ever’s website or app — except for a brief reference that was added to the privacy policy after NBC News reached out to the company in April — is that the photos people share are used to train the company’s facial recognition system, and that Ever then offers to sell that technology to private companies, law enforcement and the military.
Related Classifications: Privacy Concerns
, Snippet Text: Sarah Puchinsky-Roxey, 22, from Lemoore, California, used an expletive when told by phone of the company’s facial recognition business. “I was not aware of any facial recognition in the Ever app,” Roxey, a photographer, later emailed, noting that she had used the app for several years. “Which is kind of creepy since I have pictures of both my children on there as well as friends that have never consented to this type of thing.”
Related Classifications: Privacy Concerns, Unauthorized Data
",,Harmful Application,"Snippet Text: Ever AI promises prospective military clients that it can “enhance surveillance capabilities” and “identify and act on threats.” It offers law enforcement the ability to identify faces in body-cam recordings or live video feeds.
Related Classifications: Harmful Application
",
GMF,200,False,Voice Generation,"Snippet Text: Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of €220,000 ($243,000) in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.
Related Classifications: Voice Generation
",,,,,,,,"Recurrent Neural Network, Transformer, Convolutional Neural Network, Spectrogram, Generative Adversarial Network","Snippet Text: Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of €220,000 ($243,000) in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.
Related Classifications: Recurrent Neural Network, Transformer, Convolutional Neural Network, Spectrogram, Generative Adversarial Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of €220,000 ($243,000) in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,201,False,Deepfake Video Generation,"Snippet Text: A deepfake video showing Belgium’s prime minister speaking of an urgent need to tackle the economic and climate crises has been put into circulation by Extinction Rebellion Belgium.
Related Classifications: Deepfake Video Generation
",,,,,"Face Detection, Recurrent Neural Network, Generative Adversarial Network","Snippet Text: Neural Network, Face Detection, Recurrent Neural Network, Generative Adversarial Network
Related Classifications: Face Detection, Recurrent Neural Network, Generative Adversarial Network
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Neural Network, Face Detection, Recurrent Neural Network, Generative Adversarial Network
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,202,False,Deepfake Video Generation,"Snippet Text: The AI Yoon Suk-yeol was slightly awkward when making some gestures, but the character perfectly recreated the voice of the candidate.
Related Classifications: Deepfake Video Generation
",,,,,"Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Spectrogram","Snippet Text: The AI Yoon Suk-yeol was slightly awkward when making some gestures, but the character perfectly recreated the voice of the candidate.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Generative Adversarial Network, Spectrogram
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Amid the efforts to highlight that the candidates are “up-to-date,” there are some concerns voices that these AI and deepfake technologies may be abused to manipulate campaign efforts.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,203,False,Workforce Monitoring and Evaluation,"Snippet Text: Uber has quietly changed the way it pays drivers in several major cities across the U.S., using a new feature it’s calling “Upfront Fares.” Instead of paying drivers for trips based on just time and distance, it’s now using an algorithm “based on several factors” to calculate the fare.
Related Classifications: Workforce Monitoring and Evaluation
",,,,,Regression,"Snippet Text: Uber has quietly changed the way it pays drivers in several major cities across the U.S., using a new feature it’s calling “Upfront Fares.” Instead of paying drivers for trips based on just time and distance, it’s now using an algorithm “based on several factors” to calculate the fare. What all of those factors are is unclear. 
Related Classifications: Regression
, Snippet Text: “Uber didn’t come out and say this is going to be algorithmic, but the criteria that they are using—a range of factors and things that aren’t specified—could indicate that the fares are going to disappear behind a black box algorithm,” Toh said. “When you put a fare calculation behind a black box algorithm, it’s possible to have the capacity to learn from driver behavior … and actually learn what is the lowest rate a driver will take for a ride.”
Related Classifications: 
",,,,,"Lack of Transparency, Untested Deployment","Snippet Text: Uber has quietly changed the way it pays drivers in several major cities across the U.S., using a new feature it’s calling “Upfront Fares.” Instead of paying drivers for trips based on just time and distance, it’s now using an algorithm “based on several factors” to calculate the fare. What all of those factors are is unclear. 
Related Classifications: Lack of Transparency
, Snippet Text: “Uber didn’t come out and say this is going to be algorithmic, but the criteria that they are using—a range of factors and things that aren’t specified—could indicate that the fares are going to disappear behind a black box algorithm,” Toh said. “When you put a fare calculation behind a black box algorithm, it’s possible to have the capacity to learn from driver behavior … and actually learn what is the lowest rate a driver will take for a ride.”
Related Classifications: Lack of Transparency
, Snippet Text: Driver responses were mixed on some online groups. Some complained the new algorithm seemed arbitrary and no longer allowed them to calculate pay based on a per-mile (per-km) basis.
Related Classifications: Untested Deployment
Snippet Discussion: No on-premise testing with real drivers for feedback.
",,,,
GMF,204,False,Workforce Monitoring and Evaluation,"Snippet Text: A picture of a system that monitors employees’ intentions of leaving the company has been circulating online. 
Related Classifications: Workforce Monitoring and Evaluation
",,,,,,,,Classification,"Snippet Text: Now they have to deal with a new workplace surveillance tool that’s able to predict which employees are about to quit their jobs.
Related Classifications: Classification
, Snippet Text: The image shows information on several metrics indicating a worker’s likelihood to quit, including the frequency of visiting employment websites, the number of job applications they’ve submitted, and even links to download PDF files of résumés the employees have sent out.
Related Classifications: Classification
",,"Privacy Concerns, Unauthorized Data, Harmful Application","Snippet Text: A surveillance system developed by a Shenzhen-based software firm can identify workers who are planning to quit by spying on their online activities. 
Related Classifications: Privacy Concerns
, Snippet Text: While it’s unclear how many Chinese companies are currently using Sangfor’s resignation analysis system, the revelation triggered outrage among Chinese workers who condemned the technology as “a form of psychological abuse” and “a blatant invasion of their privacy.”
Related Classifications: Privacy Concerns, Unauthorized Data, Harmful Application
",,,,
GMF,205,False,Social Media Content Generation,"Snippet Text: Separately, Meta officials say they removed a “small” troll network of 40 accounts “operated from Russia and Ukraine” that the company found targeting Ukrainian audiences with disinformation about Ukraine.
Related Classifications: Social Media Content Generation
",,,,,Transformer,"Snippet Text: Separately, Meta officials say they removed a “small” troll network of 40 accounts “operated from Russia and Ukraine” that the company found targeting Ukrainian audiences with disinformation about Ukraine.
Related Classifications: Transformer
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: Separately, Meta officials say they removed a “small” troll network of 40 accounts “operated from Russia and Ukraine” that the company found targeting Ukrainian audiences with disinformation about Ukraine.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,206,False,Personalized Pricing,"Snippet Text: “Algorithm-based personalised pricing is when a business charges people different prices for the same goods or service. To figure out how much you’re charged, the service accesses your personal data. The question is, when is it appropriate for different people to be charged different amounts? Why is it okay for Tinder to charge users different prices, but for a company like Bumble to have set pricing?” said Dr Cherie Lacey, Consumer NZ investigative lead.
Related Classifications: Personalized Pricing
",,,,,,,,Regression,"Snippet Text: This makes it difficult for consumers signing up for the services to have transparency about the price they were quoted. In New Zealand the highest prices quoted were between four to six times more than the lowest price.
",,"Algorithmic Bias, Lack of Transparency","Snippet Text: “Algorithm-based personalised pricing is when a business charges people different prices for the same goods or service. To figure out how much you’re charged, the service accesses your personal data. The question is, when is it appropriate for different people to be charged different amounts? Why is it okay for Tinder to charge users different prices, but for a company like Bumble to have set pricing?” said Dr Cherie Lacey, Consumer NZ investigative lead.
Related Classifications: Algorithmic Bias
, Snippet Text: This makes it difficult for consumers signing up for the services to have transparency about the price they were quoted. In New Zealand the highest prices quoted were between four to six times more than the lowest price.
Related Classifications: Lack of Transparency, Algorithmic Bias
",,,,
GMF,207,False,Autonomous Drones,"Snippet Text: “Oftentimes, POST participants will request medical attention when exhibiting possible symptoms of COVID-19. In an effort to reduce possible exposure, the Spot robot will provide telemedicine to those individuals and can deliver medical supplies and food.”
Related Classifications: Autonomous Drones
",,,,,,,,"Convolutional Neural Network, Visual Object Detection, Image Segmentation, Recurrent Neural Network","Snippet Text: Homeless residents of a state-run tent city in Honolulu, Hawaii, are having their eyes scanned by a robotic police dog.
Related Classifications: Convolutional Neural Network, Visual Object Detection, Image Segmentation, Recurrent Neural Network
",,Privacy Concerns,"Snippet Text: Concerns about the use of a semi-autonomous robot to surveil communities prompted the New York City Police Department to end its use of Spot robots.
Related Classifications: Privacy Concerns
",,"Harmful Application, Task Mismatch","Snippet Text: “The bulk of the money should’ve gone to individuals and families who were suffering,” said Councilwoman Heidi Tsuneyoshi.
Related Classifications: Harmful Application
, Snippet Text: One of the main justifications Lambert and other HPD officials made for the purchase was that the robot could be used for an apparently unlimited number of tasks after the pandemic. Lambert’s slide presentation to the council also suggested using the robot for “remote encampment outreach” and “de-escalation.” He did not provide any explanation of how the robot would perform those tasks.
Related Classifications: Task Mismatch
",
GMF,208,False,Autonomous Driving,"Snippet Text: The so-called “phantom braking” increased after Tesla both made a software update and stopped using radar sensors in October.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection","Snippet Text: The so-called “phantom braking” increased after Tesla both made a software update and stopped using radar sensors in October.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection
",,,,,"Incomplete Data Attribute Capture, Context Misidentification, Generalization Failure","Snippet Text: Some Tesla drivers say they're experiencing an increase in ""phantom braking,"" in which their cars make random, jolting stops because they misinterpret hazards like trash on the road, trucks in nearby lanes and oncoming traffic on two-lane roads. 
Related Classifications: Incomplete Data Attribute Capture, Context Misidentification, Generalization Failure
, Snippet Text: The so-called “phantom braking” increased after Tesla both made a software update and stopped using radar sensors in October.
Related Classifications: Incomplete Data Attribute Capture
",,Misconfigured Threshold,"Snippet Text: The system can be falsely detecting an object on the road or anticipating a collision that won’t actually happen and apply the brake to try to avoid it.
Related Classifications: Misconfigured Threshold
Snippet Discussion: Perhaps there's a confidence threshold here.
",
GMF,209,False,Autonomous Driving,"Snippet Text: Profiles are back in Tesla’s latest “Full Self-Driving” beta 10.3 with an “Assertive Mode” that may perform rolling stops and other borderline maneuvers, The Verge has reported.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection","Snippet Text: Profiles are back in Tesla’s latest “Full Self-Driving” beta 10.3 with an “Assertive Mode” that may perform rolling stops and other borderline maneuvers, The Verge has reported.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection
",,,,,Generalization Failure,"Snippet Text: The update was originally released in October 2021 with three profiles (“Chill,” “Average” and “Assertive”), but was pulled just two days later over issues with traffic light left turns, unexpected stopping and more.
Related Classifications: Generalization Failure
",,Untested Deployment,"Snippet Text: The update was originally released in October 2021 with three profiles (“Chill,” “Average” and “Assertive”), but was pulled just two days later over issues with traffic light left turns, unexpected stopping and more.
Related Classifications: Untested Deployment
",
GMF,210,False,Social Media Content Generation,"Snippet Text: The app, called Tek Fog, allows users to bypass controls like email and text-message verification that companies like Twitter and Facebook build into their products. It can also act as a master control for a number of Twitter accounts, fake or otherwise, pushing hashtags, content and retweets through a single interface.
Related Classifications: Social Media Content Generation
",,,,,,,,Conditional Logic,"Snippet Text: The app, called Tek Fog, allows users to bypass controls like email and text-message verification that companies like Twitter and Facebook build into their products. It can also act as a master control for a number of Twitter accounts, fake or otherwise, pushing hashtags, content and retweets through a single interface.
Related Classifications: Conditional Logic
",,"Misinformation Generation Hazard, Harmful Application, Privacy Concerns, Unauthorized Data","Snippet Text: The app, called Tek Fog, allows users to bypass controls like email and text-message verification that companies like Twitter and Facebook build into their products. It can also act as a master control for a number of Twitter accounts, fake or otherwise, pushing hashtags, content and retweets through a single interface.
Related Classifications: Misinformation Generation Hazard, Harmful Application
, Snippet Text: According to the report, users of the platform could tap into and manipulate the trending features on Twitter and Facebook — by automatically sharing or retweeting posts, and targeting existing hashtags — to commandeer the narrative on India’s most widely used social media services. This simple tactic could then amplify propaganda, in order to make a particular idea or opinion appear more popular than it really was, or to shout down opposing views.
Related Classifications: Misinformation Generation Hazard, Harmful Application
, Snippet Text: On WhatsApp, Tek Fog takes over people’s inactive accounts to then message their contacts. “App operators also use this feature to phish the personal information of targeted users to add to a…political database,” The Wire wrote. “The addition of private citizens into this database makes them available as potential targets in future harassment and trolling campaigns.”
Related Classifications: Privacy Concerns, Unauthorized Data
",,,,
GMF,211,False,Autonomous Driving,"Snippet Text: A Tesla Model 3 taxi cab got involved in a severe accident in Paris. 
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Convolutional Neural Network, Recurrent Neural Network","Snippet Text: A Tesla Model 3 taxi cab got involved in a severe accident in Paris. 
Related Classifications: Visual Object Detection, Image Segmentation, Convolutional Neural Network, Recurrent Neural Network
",,,,,,,,"Hardware Failure, Software Bug","Snippet Text: The crash happened on December 11 at around 9 PM in the 13e arrondissement in Paris. Le Figaro reports that the police informed the driver was traveling on rue d’Ivry (it’s actually avenue) when his car suddenly started accelerating and could not brake. Jérôme Coumet, the mayor of the 13e arrondissement, said on Twitter the crash was caused by a “technical failure.” He also said that “the accelerator (pedal) would have stuck.”
, Snippet Text: Sarah Saldmann, a lawyer for the driver, told Reuters that the driver claims his Tesla Model 3 accelerated “on its own” and did not respond to brakes. Saldmann adds that he was treated for minor injuries and is now under investigation for suspected involuntary manslaughter.
Related Classifications: Hardware Failure, Software Bug
",
GMF,212,False,"Autonomous Driving, Face Recognition","Snippet Text: Recently, Xpeng Motors was fined 100,000 yuan ($15,717) by Xuhui District Market Supervision Administration of Shanghai for using camera equipment with face recognition functionality to collect 431,623 face photos without the consent of users.
Related Classifications: Autonomous Driving, Face Recognition
",,,,,"Visual Object Detection, Image Segmentation, Recurrent Neural Network, Convolutional Neural Network, Face Detection","Snippet Text: Recently, Xpeng Motors was fined 100,000 yuan ($15,717) by Xuhui District Market Supervision Administration of Shanghai for using camera equipment with face recognition functionality to collect 431,623 face photos without the consent of users.
Related Classifications: Visual Object Detection, Image Segmentation, Recurrent Neural Network, Convolutional Neural Network, Face Detection
",,,,,Unauthorized Data,"Snippet Text: Recently, Xpeng Motors was fined 100,000 yuan ($15,717) by Xuhui District Market Supervision Administration of Shanghai for using camera equipment with face recognition functionality to collect 431,623 face photos without the consent of users.
Related Classifications: Unauthorized Data
",,,,
GMF,213,False,Ad Delivery,"Snippet Text: In the first known study to quantify the performance of Facebook’s political ad policy enforcement at a large and representative scale, researchers found that when making decisions on how to classify undeclared ads, Facebook often missed political ads while falsely labeling others as political. The political ads that Facebook misses also disappear from its public archive, putting them out of reach for public scrutiny.
Related Classifications: Ad Delivery
",,,,,"Document Classification, Keyword Filtering","Snippet Text: The researchers noted that Facebook's enforcement of the policy relies heavily on detecting keywords in ads under an automated system, although staff also play a role in moderating the content.
Related Classifications: Document Classification, Keyword Filtering
",,,,,Generalization Failure,"Snippet Text: Facebook missed a higher proportion of political ads outside the U.S. The platform had the worst record in Malaysia, where it missed as much as 45 percent of ads from obviously political pages or advertisers. In Macedonia, Argentina, Turkey, Portugal, France, and Serbia, Facebook missed up to one out of four ads from such pages, which were sponsored by candidates or parties. In the U.S., 55 percent of detected political ads were actually overcounts, meaning they did not meet Facebook’s definition for political ads.
Related Classifications: Generalization Failure
, Snippet Text: The researchers analyzed 189,000 political ads from around the world that Facebook needed to make an enforcement decision on between July 2020 and February 2021. They found that Facebook misidentified a whopping 83% of those ads.
Related Classifications: Generalization Failure
",,"Dataset Imbalance, Misconfigured Threshold, Algorithmic Bias","Snippet Text: Facebook missed a higher proportion of political ads outside the U.S. The platform had the worst record in Malaysia, where it missed as much as 45 percent of ads from obviously political pages or advertisers. In Macedonia, Argentina, Turkey, Portugal, France, and Serbia, Facebook missed up to one out of four ads from such pages, which were sponsored by candidates or parties. In the U.S., 55 percent of detected political ads were actually overcounts, meaning they did not meet Facebook’s definition for political ads.
Related Classifications: Dataset Imbalance
, Snippet Text: The researchers also found Facebook’s enforcement varied wildly depending on where the ads appeared. In the U.S, Facebook overcorrected and ended up mislabeling more ads that weren’t actually political. In Malaysia, the opposite was true, with Facebook letting some 45% of these political ads go unlabeled. That disparity mirrors the company’s struggles to moderate other aspects of its platform in non-English speaking parts of the world.
Related Classifications: Dataset Imbalance, Misconfigured Threshold, Algorithmic Bias
, Snippet Text: 
",
GMF,214,False,"Face Recognition, Threat Detection","Snippet Text: Documents reveal Lockport Schools' facial recognition tech has mistaken broom handles for guns and has misidentified Black students at much higher rates.
Related Classifications: Face Recognition, Threat Detection
",,,,,"Image Classification, Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: Documents reveal Lockport Schools' facial recognition tech has mistaken broom handles for guns and has misidentified Black students at much higher rates.
Related Classifications: Image Classification, Face Detection, Visual Object Detection, Image Segmentation
",,,,,"Generalization Failure, Context Misidentification","Snippet Text: 

Now, documents newly obtained by Motherboard accentuate those fears. They show that SN Technologies, the Canadian company contracted to install Lockport’s facial recognition system, misled the district about the accuracy of the algorithm it uses and downplayed how often it misidentifies Black faces. The records, comprising hundreds of pages of emails between the district and the company, also detail numerous technical issues with SN Technologies’ AEGIS face and weapons detection system, including its propensity for misidentifying objects like broom handles as guns.
Related Classifications: Generalization Failure, Context Misidentification
",,"Distributional Bias, Dataset Imbalance","Snippet Text: 

Now, documents newly obtained by Motherboard accentuate those fears. They show that SN Technologies, the Canadian company contracted to install Lockport’s facial recognition system, misled the district about the accuracy of the algorithm it uses and downplayed how often it misidentifies Black faces. The records, comprising hundreds of pages of emails between the district and the company, also detail numerous technical issues with SN Technologies’ AEGIS face and weapons detection system, including its propensity for misidentifying objects like broom handles as guns.
Related Classifications: Distributional Bias, Dataset Imbalance
, Snippet Text: The district received even more evidence that SN Technologies was lying in October, when it received a report from Freed Maxick, the accountancy it had hired to audit SN Technologies’ claims. The auditors wrote that the company had inaccurately claimed that, in NIST testing, its algorithm misidentified Black men twice as often as white men and misidentified Black women 10 times more often than white men. In reality, the auditors wrote, NIST tests of the id3 Technologies algorithm that Flynn’s company claimed to be using actually showed that it misidentified Black men four times more often and Black women 16 times more often than white men.
Related Classifications: Distributional Bias, Dataset Imbalance
",
GMF,215,False,Automated Content Curation,"Snippet Text: Without informing the public, Facebook undertook a massive live experiment in heavily automated content moderation. Management told moderators that we should no longer see certain varieties of toxic content coming up in the review tool from which we work— such as graphic violence or child abuse, for example.


Related Classifications: Automated Content Curation
",,,,,Classification,"Snippet Text: The AI wasn’t up to the job. Important speech got swept into the maw of the Facebook filter—and risky content, like self-harm, stayed up.

The lesson is clear. Facebook’s algorithms are years away from achieving the necessary level of sophistication to moderate content automatically. They may never get there.
Related Classifications: Classification
",,,,,Generalization Failure,"Snippet Text: The AI wasn’t up to the job. Important speech got swept into the maw of the Facebook filter—and risky content, like self-harm, stayed up.

The lesson is clear. Facebook’s algorithms are years away from achieving the necessary level of sophistication to moderate content automatically. They may never get there.
Related Classifications: Generalization Failure
",,,,
GMF,216,False,Translation,"Snippet Text: Chinese messaging app WeChat has apologized for an error in its algorithm that provided the N-word as a translation for a neutral Chinese term for black foreigners.
Related Classifications: Translation
",,,,,"Keyword Filtering, Recurrent Neural Network, Distributional Learning","Snippet Text: A research group at the University of Toronto analysed the terms blocked on WeChat in March, and found they included ""Free Tibet"", ""Down with the Communist Party"", and many mentions of Nobel laureate Liu Xiaobo, who was China's most prominent human rights advocate.
Related Classifications: Keyword Filtering
, Snippet Text: The spokesperson from WeChat explained that the app used neural machine translation, though the engine was constantly being refined to provide “more accurate, faithful, expressive, and elegant” results.
Related Classifications: Recurrent Neural Network, Distributional Learning
",,,,,"Data or Labelling Noise, Inadequate Output Filtering","Snippet Text: Chinese messaging app WeChat has apologized for an error in its algorithm that provided the N-word as a translation for a neutral Chinese term for black foreigners.
Related Classifications: Data or Labelling Noise, Inadequate Output Filtering
Snippet Discussion: Either slurs were not cleaned in the training data, or are not filtered out in generic translation use cases.
",,,,
GMF,217,False,Autonomous Drones,"Snippet Text: For the first time in China, a robot has ‘attacked’ and injured a person. The incident occurred at the 18th China Hi-Tech Fair, that kicked off in Shenzhen on November 16th.
Related Classifications: Autonomous Drones
",,,,,,,,Visual Object Detection,"Snippet Text: The robot was designed to detect and avoid obstacles automatically, but the function was turned off at the time.
Related Classifications: Visual Object Detection
",,,"Snippet Text: According to Shenzhen Evening News, a witness wrote in a post: 'The Force Awakens! I saw the ""Little Chubby"" at the neighboring booth smashing the glass wall without any human commands, wounding a passerby.'
Related Classifications: Context Misidentification
",,"Human Error, Lack of Safety Protocols","Snippet Text: A member of staff pressed a button on 'Little Chubby' by mistake, causing it to reverse instead of moving forward.

According to Shenzhen Evening News, a witness wrote in a post: 'The Force Awakens! I saw the ""Little Chubby"" at the neighboring booth smashing the glass wall without any human commands, wounding a passerby.'
Related Classifications: Human Error, Lack of Safety Protocols
",
GMF,218,False,Autonomous Driving,"Snippet Text: Today, a video that surfaced on Twitter shows a Tesla Model 3 driving, without waiver or interruption, directly into a flipped-over truck on the highway. The clip, which occurred in Taiwan, has some viewers questioning and speculating what happened to the driver, if he was paying attention, and if any Tesla safety functions were in use at the time.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network","Snippet Text: Today, a video that surfaced on Twitter shows a Tesla Model 3 driving, without waiver or interruption, directly into a flipped-over truck on the highway. The clip, which occurred in Taiwan, has some viewers questioning and speculating what happened to the driver, if he was paying attention, and if any Tesla safety functions were in use at the time.
Related Classifications: Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network
",,,,,Generalization Failure,"Snippet Text: Today, a video that surfaced on Twitter shows a Tesla Model 3 driving, without waiver or interruption, directly into a flipped-over truck on the highway. The clip, which occurred in Taiwan, has some viewers questioning and speculating what happened to the driver, if he was paying attention, and if any Tesla safety functions were in use at the time.
Related Classifications: Generalization Failure
, Snippet Text: ""The police said that the driver of the Tesla electric vehicle, Huang, claimed to have the vehicle assist system turned on, and the speed was fixed at 110 kilometers per hour,"" the article says. ""He thought that the car itself would detect the obstacle and automatically brake, but he was surprised that the car did not slow down."" 
Related Classifications: Generalization Failure
, Snippet Text: He said that once he realized the car was not going react to the truck, he stepped on the brakes at the last second. Unfortunately, by the time he applied the brakes, there was not enough room to fully decelerate or avoid the vehicle.
Related Classifications: Generalization Failure
",,Human Error,"Snippet Text: Today, a video that surfaced on Twitter shows a Tesla Model 3 driving, without waiver or interruption, directly into a flipped-over truck on the highway. The clip, which occurred in Taiwan, has some viewers questioning and speculating what happened to the driver, if he was paying attention, and if any Tesla safety functions were in use at the time.
Related Classifications: Human Error
",
GMF,219,False,Smart Surveillance,"Snippet Text: Durban - THE artificial intelligence (AI) cameras installed in Hluhluwe-iMfolozi Park failed to detect poachers after four dehorned rhinos were found last week.
Related Classifications: Smart Surveillance
",,,,,"Visual Object Detection, Person Detection","Snippet Text: Durban - THE artificial intelligence (AI) cameras installed in Hluhluwe-iMfolozi Park failed to detect poachers after four dehorned rhinos were found last week.
Related Classifications: Visual Object Detection
, Snippet Text: The cameras use AI to identify people and send an immediate alert to the operations centre which then quickly alerts and activates the relevant reaction units.
Related Classifications: Visual Object Detection, Person Detection
",,,,,Scaling Limitations,"Snippet Text: Ezemvelo KZN Wildlife spokesperson Musa Mntambo said: “They (cameras) did not pick them up. The park is huge and the cameras do not focus on every spot within the park at the same time.”
Related Classifications: Scaling Limitations
",,Generalization Failure,"Snippet Text: Ezemvelo KZN Wildlife spokesperson Musa Mntambo said: “They (cameras) did not pick them up. The park is huge and the cameras do not focus on every spot within the park at the same time.”
Related Classifications: Generalization Failure
",
GMF,220,False,"Ad Delivery, Automated Content Curation","Snippet Text: Facebook’s human moderators have focused on election and COVID-19 misinformation this year, so the company has leaned more on artificial intelligence algorithms to monitor other areas of the platform. That’s left many small businesses caught in Facebook’s automated filters, unable to advertise through the service and frustrated because they don’t know why.
Related Classifications: Automated Content Curation, Ad Delivery
",,,,,Document Classification,"Snippet Text: Facebook’s human moderators have focused on election and COVID-19 misinformation this year, so the company has leaned more on artificial intelligence algorithms to monitor other areas of the platform. That’s left many small businesses caught in Facebook’s automated filters, unable to advertise through the service and frustrated because they don’t know why.
Related Classifications: Document Classification
",,Keyword Filtering,,,Context Misidentification,"Snippet Text: According to a Bloomberg report, Facebook’s human moderators have focused on election and Covid-19 misinformation in 2020, enabling the firm to learn more about artificial intelligence algorithms used to monitor other areas of the platform. This has prompted more and more small businesses to get caught in Facebook’s automated filters and get their advertisements disabled.
Related Classifications: Context Misidentification
",,"Untested Deployment, Misconfigured Threshold","Snippet Text: According to a Bloomberg report, Facebook’s human moderators have focused on election and Covid-19 misinformation in 2020, enabling the firm to learn more about artificial intelligence algorithms used to monitor other areas of the platform. This has prompted more and more small businesses to get caught in Facebook’s automated filters and get their advertisements disabled.
Related Classifications: Untested Deployment, Misconfigured Threshold
",
GMF,222,False,Chatbot,"Snippet Text: #gpt3 is surprising and creative but it’s also unsafe due to harmful biases. Prompted to write tweets from one word - Jews, black, women, holocaust - it came up with these (https://thoughts.sushant-kumar.com). We need more progress on #ResponsibleAI before putting NLG models in production.
Related Classifications: Chatbot
",,,,,Transformer,"Snippet Text: #gpt3 is surprising and creative but it’s also unsafe due to harmful biases. Prompted to write tweets from one word - Jews, black, women, holocaust - it came up with these (https://thoughts.sushant-kumar.com). We need more progress on #ResponsibleAI before putting NLG models in production.
Related Classifications: Transformer
",,,,,Distributional Bias,"Snippet Text: #gpt3 is surprising and creative but it’s also unsafe due to harmful biases. Prompted to write tweets from one word - Jews, black, women, holocaust - it came up with these (https://thoughts.sushant-kumar.com). We need more progress on #ResponsibleAI before putting NLG models in production.
Related Classifications: Distributional Bias
",,,,
GMF,223,False,Face Recognition,"Snippet Text: Facial-recognition locks used by a company claiming to operate the world’s largest network of express delivery lockers have been hacked by a group of fourth-graders.
Related Classifications: Face Recognition
",,,,,"Convolutional Neural Network, Visual Object Detection, Face Detection","Snippet Text: Facial-Recognition Smart Lockers Hacked by Fourth-Graders
sixthtone.com · 2019

Edit

Facial-recognition locks used by a company claiming to operate the world’s largest network of express delivery lockers have been hacked by a group of fourth-graders.

The primary schoolers from Jiaxing in eastern China’s Zhejiang province told local TV program Haoqi Shiyanshi, or Curious Labs, that their science club recently discovered facial-recognition locks used by Hive Box, a Chinese smart locker company, could be opened using only a printed photo of the intended recipient’s face, leaving the lockers’ contents vulnerable to theft.
Related Classifications: Convolutional Neural Network, Visual Object Detection, Face Detection
",,,,,"Gaming Vulnerability, Adversarial Data","Snippet Text: Facial-Recognition Smart Lockers Hacked by Fourth-Graders
sixthtone.com · 2019

Edit

Facial-recognition locks used by a company claiming to operate the world’s largest network of express delivery lockers have been hacked by a group of fourth-graders.

The primary schoolers from Jiaxing in eastern China’s Zhejiang province told local TV program Haoqi Shiyanshi, or Curious Labs, that their science club recently discovered facial-recognition locks used by Hive Box, a Chinese smart locker company, could be opened using only a printed photo of the intended recipient’s face, leaving the lockers’ contents vulnerable to theft.
Related Classifications: Gaming Vulnerability, Adversarial Data
",,"Underfitting, Inadequate Data Augmentation","Snippet Text: Facial-Recognition Smart Lockers Hacked by Fourth-Graders
sixthtone.com · 2019

Edit

Facial-recognition locks used by a company claiming to operate the world’s largest network of express delivery lockers have been hacked by a group of fourth-graders.

The primary schoolers from Jiaxing in eastern China’s Zhejiang province told local TV program Haoqi Shiyanshi, or Curious Labs, that their science club recently discovered facial-recognition locks used by Hive Box, a Chinese smart locker company, could be opened using only a printed photo of the intended recipient’s face, leaving the lockers’ contents vulnerable to theft.
Related Classifications: Underfitting, Inadequate Data Augmentation
",
GMF,224,False,Face Recognition,"Snippet Text: Police in central China’s Hubei province have arrested three swindlers who used chat stickers, or GIFs, to verify people’s identities and transfer money via the mobile payment service on messaging app WeChat, domestic media reported Saturday.
Related Classifications: Face Recognition
",,,,,"Convolutional Neural Network, Face Detection, Visual Object Detection","Snippet Text: Police in central China’s Hubei province have arrested three swindlers who used chat stickers, or GIFs, to verify people’s identities and transfer money via the mobile payment service on messaging app WeChat, domestic media reported Saturday.
Related Classifications: Convolutional Neural Network, Face Detection, Visual Object Detection
",,,,,"Adversarial Data, Gaming Vulnerability","Snippet Text: Police in central China’s Hubei province have arrested three swindlers who used chat stickers, or GIFs, to verify people’s identities and transfer money via the mobile payment service on messaging app WeChat, domestic media reported Saturday.
Related Classifications: Adversarial Data, Gaming Vulnerability
",,,,
GMF,225,False,Medical Diagnosis Support,"Snippet Text: Internal documents from IBM Watson Health (NYSE:IBM) indicate that the company’s Watson for Oncology product often returns “multiple examples of unsafe and incorrect treatment recommendations,” according to a new report from STAT News.
Related Classifications: Medical Diagnosis Support
",,,,,,"Snippet Text: 
",,"Classification, Diverse Data, Conditional Logic","Snippet Text: Internal documents from IBM Watson Health (NYSE:IBM) indicate that the company’s Watson for Oncology product often returns “multiple examples of unsafe and incorrect treatment recommendations,” according to a new report from STAT News.
Related Classifications: Classification, Diverse Data
, Snippet Text: The synthetic cases were compiled by the doctors and IBM engineers to expose Watson for Oncology to clinical scenarios, as opposed to the actual records of patients who were treated at the hospital. That meant that Watson’s recommendations were driven by the doctors’ own treatment preferences — not a machine learning analysis of real patient cases.
Related Classifications: Conditional Logic
",,"Limited Dataset, Data or Labelling Noise, Generalization Failure, Lack of Transparency, Inappropriate Training Content","Snippet Text: But internal documents indicate that training and effectiveness of the Watson for Oncology system was flawed due to the small number of cases and the inclusion of artificial cases with only one or two doctors supplying recommendations for each type of cancer it was designed to work with, according to the report.
Related Classifications: Limited Dataset, Data or Labelling Noise, Inappropriate Training Content
, Snippet Text: But these new documents reveal that the problems were more serious and systemic and that IBM executives knew that the product was generating inaccurate recommendations that were at odds with national treatment guidelines — although there’s no mention that patients were actually harmed. The documents also state that studies IBM conducted on the software, whose findings were touted as evidence of the system’s usefulness, were designed to generate favorable results.
Related Classifications: Generalization Failure
, Snippet Text: But these new documents reveal that the problems were more serious and systemic and that IBM executives knew that the product was generating inaccurate recommendations that were at odds with national treatment guidelines — although there’s no mention that patients were actually harmed. The documents also state that studies IBM conducted on the software, whose findings were touted as evidence of the system’s usefulness, were designed to generate favorable results.
Related Classifications: Limited Dataset, Data or Labelling Noise
, Snippet Text: Experts in artificial intelligence told STAT that IBM’s portrayal of the training, and the number of doctors and patients involved, raises questions about whether it’s being transparent with users about the source and value of Watson’s recommendations.
Related Classifications: Lack of Transparency
",,,,
GMF,226,False,Navigation Assistant,"Snippet Text: “This is something that needs to be done, because of Waze,” Councilman David Ryu, whose district includes Sherman Oaks, told an estimated 300 residents who had filled an auditorium at Buckley School for two hours. “I am committed … (but) not everyone’s going to be happy.”
Related Classifications: Navigation Assistant
",,,,,"Geolocation Data, Time Series Forecasting, Routing","Snippet Text: “This is something that needs to be done, because of Waze,” Councilman David Ryu, whose district includes Sherman Oaks, told an estimated 300 residents who had filled an auditorium at Buckley School for two hours. “I am committed … (but) not everyone’s going to be happy.”
Related Classifications: Geolocation Data, Time Series Forecasting, Routing
",,,,,,,,"Untested Deployment, Incomplete Data Attribute Capture","Snippet Text: “This is something that needs to be done, because of Waze,” Councilman David Ryu, whose district includes Sherman Oaks, told an estimated 300 residents who had filled an auditorium at Buckley School for two hours. “I am committed … (but) not everyone’s going to be happy.”
Related Classifications: Untested Deployment
, Snippet Text: ""There are tremendous advantages to apps like Waze,"" said Councilman Paul Krekorian, who introduced the motion seeking the solution. ""They can make driving more efficient, but with every technological advance, any consequences that arise must be taken into account. With this vote, the city will have the go ahead to start a dialogue with these tech companies to see if they will work more closely with us to reduce the impact their apps are having on small residential streets and increase the level of traffic safety in our neighborhoods.""
Related Classifications: Incomplete Data Attribute Capture
Snippet Discussion: Residential street status not considered in the app.
",
GMF,227,False,Navigation Assistant,"Snippet Text: Waze has yet to give an explanation as to why the app directed a group of tourists to drive into Lake Champlain earlier this month.
Related Classifications: Navigation Assistant
",,,,,"Routing, Geolocation Data","Snippet Text: Waze has yet to give an explanation as to why the app directed a group of tourists to drive into Lake Champlain earlier this month.
Related Classifications: Routing, Geolocation Data
",,"Autoregressive Models, Time Series Forecasting","Snippet Text: Waze has yet to give an explanation as to why the app directed a group of tourists to drive into Lake Champlain earlier this month.
Related Classifications: Autoregressive Models, Time Series Forecasting
",,,,,"Human Error, Outdated Input","Snippet Text: 'It's impossible to comment here without seeing the user's driving file and we haven't received permission to do so- generally speaking, Waze maps are updated with millions of edits to adapt to real time road conditions daily, often making them the most accurate available,' she told the Burlington Free Press.
Related Classifications: Human Error, Outdated Input
Snippet Discussion: Failure to update the app?
",
GMF,228,False,Navigation Assistant,"Snippet Text: Navigating that very traffic nightmare, Rachael and Thomas decided to consult both Waze and Apple Maps; the latter guided them toward an unconventional route out of town. The road narrowed and became purely residential, and the snow started to build. Suddenly, a sign informed them that they were on an unpaved road. Rachael got out of the car because she and Thomas truly didn't know whether to forge ahead, or cut their losses, and turn back.
Related Classifications: Navigation Assistant
",,,,,"Geolocation Data, Routing","Snippet Text: Navigating that very traffic nightmare, Rachael and Thomas decided to consult both Waze and Apple Maps; the latter guided them toward an unconventional route out of town. The road narrowed and became purely residential, and the snow started to build. Suddenly, a sign informed them that they were on an unpaved road. Rachael got out of the car because she and Thomas truly didn't know whether to forge ahead, or cut their losses, and turn back.
Related Classifications: Geolocation Data, Routing
",,Autoregressive Model,"Snippet Text: Navigating that very traffic nightmare, Rachael and Thomas decided to consult both Waze and Apple Maps; the latter guided them toward an unconventional route out of town. The road narrowed and became purely residential, and the snow started to build. Suddenly, a sign informed them that they were on an unpaved road. Rachael got out of the car because she and Thomas truly didn't know whether to forge ahead, or cut their losses, and turn back.
Related Classifications: Autoregressive Model
",,,,,Outdated Input,"Snippet Text: ""There have been many instances where navigational traffic apps have outdated or incorrect information which may end up routing motorists to unsafe or non-existent routes,"" Ian Hoey, an officer with California Highway Patrol (CHP), said. ""This has been an issue for years.""
Related Classifications: Outdated Input
, Snippet Text: ""Our goal at Waze is to deliver a smooth driving experience to help drivers get anywhere safely and on time,"" a Waze spokesperson said. ""We have a network of more than 115 million monthly global active users inputting updates directly into the app in real-time. During a crisis or weather-related situation, Waze takes a thoughtful approach to optimizing route planning by taking a number of factors into account — driver input, road conditions, real-time speeds, historical traffic data and more — to direct drivers to the most efficient, safest route.”

However, this reliance on crowd-sourcing can have its downsides.

""Part of the advantage that we have is that we’re able to get a more wholistic view of the situation,"" Bino said. ""But if it wasn’t reported, we’re not going to have it.""
Related Classifications: Outdated Input
",
GMF,229,False,Automated Content Curation,"Snippet Text: Of course, YouTube videos depicting such acts would be more easily caught by the company’s algorithmic filters, its user-reporting system, and its human content moderators. Harder to find and weed out are videos that use graphic and obscene images as thumbnails, alongside clickbait titles, to juice viewership and generate more ad revenue. It does not seem like any of the videos featuring the bestiality thumbnails do in fact feature bestiality.
Related Classifications: Automated Content Curation
",,,,,"Visual Object Detection, Classification, Convolutional Neural Network","Snippet Text: Of course, YouTube videos depicting such acts would be more easily caught by the company’s algorithmic filters, its user-reporting system, and its human content moderators. Harder to find and weed out are videos that use graphic and obscene images as thumbnails, alongside clickbait titles, to juice viewership and generate more ad revenue. It does not seem like any of the videos featuring the bestiality thumbnails do in fact feature bestiality.
Related Classifications: Visual Object Detection, Classification, Convolutional Neural Network
",,,,,Gaming Vulnerability,"Snippet Text: This is not an isolated problem, but rather yet another example of how the fundamental structure of YouTube can be exploited by bad actors, many of whom game the platform’s rules either to generate ad revenue for click farms or for nefarious purposes. 
Related Classifications: Gaming Vulnerability
",,,,
GMF,230,False,Autonomous Driving,"Snippet Text: Tesla’s advanced driver assist system, Autopilot, was active when a Model 3 driven by a 50-year-old Florida man crashed into the side of a tractor-trailer truck on March 1st, the National Transportation Safety Board (NTSB) states in a report released on Thursday
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection","Snippet Text: Tesla’s advanced driver assist system, Autopilot, was active when a Model 3 driven by a 50-year-old Florida man crashed into the side of a tractor-trailer truck on March 1st, the National Transportation Safety Board (NTSB) states in a report released on Thursday
Related Classifications: Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection
",,,,,"Human Error, Latency Issues","Snippet Text: In a statement, Tesla confirmed that series of events. “We are deeply saddened by this accident and our thoughts are with everyone affected by this tragedy,” a Tesla spokesperson said. 
Related Classifications: Human Error, Latency Issues
",,Lack of Safety Protocols,"Snippet Text: The National Highway Traffic Safety Administration (NHTSA) determined that a “lack of safeguards” contributed to Brown’s death. Meanwhile, today’s report is just preliminary, and the NTSB declined to place blame on anyone.
Related Classifications: Lack of Safety Protocols
",
GMF,231,False,Autonomous Driving,"Snippet Text: Tesla Motors came under renewed questioning about the safety of its Autopilot technology after news emerged on Wednesday of a fatal crash in China that may have occurred while the automated driver-assist system was operating.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection","Snippet Text: Tesla Motors came under renewed questioning about the safety of its Autopilot technology after news emerged on Wednesday of a fatal crash in China that may have occurred while the automated driver-assist system was operating.
Related Classifications: Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection
",,,,,"Hardware Failure, Lack of Transparency, Generalization Failure","Snippet Text: “Because of the damage caused by the collision, the car was physically incapable of transmitting log data to our servers, and we therefore have no way of knowing whether or not Autopilot was engaged at the time of the crash,” a Tesla spokeswoman, Alexis Georgeson, said in the company’s statement.
Related Classifications: Hardware Failure
, Snippet Text: A lawsuit about a fatal crash of Tesla in China has a new progress recently. In front of plentiful evidences, Tesla Motors admitted that the vehicle was under ""autopilot"" condition when the fatal crash happened, according to local reporters who got information from relevant lawyers. 
Related Classifications: Lack of Transparency, Generalization Failure
",,,"Snippet Text: In an emailed statement, Tesla said on Wednesday that it had not been able to determine whether Autopilot was active at the time of the Handan accident. 
Related Classifications: Human Error
",
GMF,232,False,Autonomous Driving,"Snippet Text: Tesla Inc. was sued on Tuesday by the family of a Japanese man who was killed when a driver fell asleep behind the wheel of a Model X and the vehicle 'suddenly accelerated.'
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection","Snippet Text: Tesla Inc. was sued on Tuesday by the family of a Japanese man who was killed when a driver fell asleep behind the wheel of a Model X and the vehicle 'suddenly accelerated.'
Related Classifications: Convolutional Neural Network, Image Segmentation, Recurrent Neural Network, Visual Object Detection
",,,,,"Context Misidentification, Generalization Failure","Snippet Text: Tesla Inc. was sued on Tuesday by the family of a Japanese man who was killed when a driver fell asleep behind the wheel of a Model X and the vehicle 'suddenly accelerated.'
Related Classifications: Context Misidentification
, Snippet Text: 'At approximately 2:49 p.m., the vehicle that the Tesla had been tracking in front slowed down considerably and indicated by its traffic blinkers that it was preparing to switch to the immediate left-hand lane, in order to avoid the group of parked motorcycles, pedestrians, and van that were ahead of it,' court documents said.

When the vehicle 'cut-out' from the lane, the Tesla Model X accelerated from approximately nine miles an hour to 23 miles an hour.
Related Classifications: Context Misidentification, Generalization Failure
, Snippet Text: 'The Tesla Model X’s sensors and forward-facing cameras did not recognize the parked motorcycles, pedestrians, and van that were directly in its path, and it continued accelerating forward until striking the motorcycles and Mr. Umeda, thereby crushing and killing Mr. Umeda as the Tesla Model X ran over his body,' documents said.
Related Classifications: Context Misidentification, Generalization Failure
",,,,
GMF,233,False,Automated Content Curation,"Snippet Text: Now, longtime users are criticizing the company’s auto-detecting algorithms, which appear to be incorrectly flagging some inoffensive images as explicit.
Related Classifications: Automated Content Curation
",,,,,Image Classification,"Snippet Text: Now, longtime users are criticizing the company’s auto-detecting algorithms, which appear to be incorrectly flagging some inoffensive images as explicit.
Related Classifications: Image Classification
",,,,,Generalization Failure,"Snippet Text: Now, longtime users are criticizing the company’s auto-detecting algorithms, which appear to be incorrectly flagging some inoffensive images as explicit.
Related Classifications: Generalization Failure
",,"Underfitting, Limited Dataset","Snippet Text: One person saw a vase and photos of tights get flagged as explicit. The user noted that photos of dildos had flown under the algorithm’s radar, however. Another artist’s illustration of a witch floating among kelp was also incorrectly flagged. Yet another artist saw their illustrations of people running around and swimming get flagged.
Related Classifications: Underfitting, Limited Dataset
, Snippet Text: One person saw a vase and photos of tights get flagged as explicit. The user noted that photos of dildos had flown under the algorithm’s radar, however. Another artist’s illustration of a witch floating among kelp was also incorrectly flagged. Yet another artist saw their illustrations of people running around and swimming get flagged.
Related Classifications: Limited Dataset
",
GMF,234,False,Navigation Assistant,"Snippet Text: The Google Waze app has choked off this single escape route by sending thousands of weekend beach-going drivers through these historic neighborhoods. In the event of a medical emergency or wildfire, these residents are literally trapped. With their single escape route blocked by Waze-routed traffic, these residents are living a death sentence imposed by Google.
Related Classifications: Navigation Assistant
",,,,,"Geolocation Data, Routing","Snippet Text: The Google Waze app has choked off this single escape route by sending thousands of weekend beach-going drivers through these historic neighborhoods. In the event of a medical emergency or wildfire, these residents are literally trapped. With their single escape route blocked by Waze-routed traffic, these residents are living a death sentence imposed by Google.
Related Classifications: Geolocation Data, Routing
",,"Time Series Forecasting, Collaborative Filtering","Snippet Text: The Google Waze app has choked off this single escape route by sending thousands of weekend beach-going drivers through these historic neighborhoods. In the event of a medical emergency or wildfire, these residents are literally trapped. With their single escape route blocked by Waze-routed traffic, these residents are living a death sentence imposed by Google.
Related Classifications: Time Series Forecasting, Collaborative Filtering
",,Untested Deployment,"Snippet Text: The Google Waze app has choked off this single escape route by sending thousands of weekend beach-going drivers through these historic neighborhoods. In the event of a medical emergency or wildfire, these residents are literally trapped. With their single escape route blocked by Waze-routed traffic, these residents are living a death sentence imposed by Google.
Related Classifications: Untested Deployment
",,"Inadequate Regularization, Incomplete Data Attribute Capture","Snippet Text: The Google Waze app has choked off this single escape route by sending thousands of weekend beach-going drivers through these historic neighborhoods. In the event of a medical emergency or wildfire, these residents are literally trapped. With their single escape route blocked by Waze-routed traffic, these residents are living a death sentence imposed by Google.
Related Classifications: Inadequate Regularization, Incomplete Data Attribute Capture
",
GMF,235,False,"Application Evaluation, Underwriting","Snippet Text: The Wall Street Journal reported that Ping An is using facial recognition software to search for “micro-expressions” on people’s faces to help decide whether they’re being truthful, whether to insure them and presumably what the terms of service should be.
Related Classifications: Application Evaluation, Underwriting
",,,,,"Face Detection, Visual Object Detection, Image Classification","Snippet Text: The Wall Street Journal reported that Ping An is using facial recognition software to search for “micro-expressions” on people’s faces to help decide whether they’re being truthful, whether to insure them and presumably what the terms of service should be.
Related Classifications: Face Detection, Visual Object Detection, Image Classification
",,,,,Harmful Application,"Snippet Text: The Wall Street Journal reported that Ping An is using facial recognition software to search for “micro-expressions” on people’s faces to help decide whether they’re being truthful, whether to insure them and presumably what the terms of service should be.
Related Classifications: Harmful Application
, Snippet Text: Most likely they’re really looking for characteristics of people who end up making claims, which is what they want to avoid more than fraud.
Related Classifications: Harmful Application
",,Generalization Failure,"Snippet Text: How would you train an algorithm to determine who is lying? One option is to show it a bunch of people lying and telling the truth in a laboratory setting. Problem is, people’s micro-expressions might be different when they’re lying in a lab, or getting paid to lie, or lying for the first time. So an algorithm trained on them would make a lot of mistakes in real life.
Related Classifications: Generalization Failure
, Snippet Text: 
Related Classifications: Harmful Application
",
GMF,236,False,Deepfake Image Generation,"Snippet Text: What I saw was an image created by a generative adversarial network, a deep learning model that can be trained to create faces, art, or anything else.
Related Classifications: Deepfake Image Generation
",,,,,"Convolutional Neural Network, Generative Adversarial Network","Snippet Text: What I saw was an image created by a generative adversarial network, a deep learning model that can be trained to create faces, art, or anything else.
Related Classifications: Convolutional Neural Network, Generative Adversarial Network
",,,,,"Misinformation Generation Hazard, Unsafe Exposure or Access","Snippet Text: With clear evidence that this was indeed a scam operation, I decided to investigate Arthur Davidson and report my findings. I reached out to the client on behalf of whom Nicole had contacted me on April 16, asking them to clarify their relationship with Arthur Davidson. On April 18, a support agent replied that they had no relationship with the law firm.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
",,,,
GMF,238,False,Child Welfare Risk Assessment,"Snippet Text: Oregon’s Safety at Screening Tool was inspired by the influential Allegheny Family Screening Tool, which is named for the county surrounding Pittsburgh, and is aimed at predicting the risk that children face of winding up in foster care or being investigated in the future. It was first implemented in 2018. Social workers view the numerical risk scores the algorithm generates — the higher the number, the greater the risk – as they decide if a different social worker should go out to investigate the family.
Related Classifications: Child Welfare Risk Assessment
",,,,,Regression,"Snippet Text: Oregon’s Safety at Screening Tool was inspired by the influential Allegheny Family Screening Tool, which is named for the county surrounding Pittsburgh, and is aimed at predicting the risk that children face of winding up in foster care or being investigated in the future. It was first implemented in 2018. Social workers view the numerical risk scores the algorithm generates — the higher the number, the greater the risk – as they decide if a different social worker should go out to investigate the family.
Related Classifications: Regression
",,,,,"Algorithmic Bias, Lack of Transparency","Snippet Text: The move comes weeks after an Associated Press review of a separate algorithmic tool in Pennsylvania that had originally inspired Oregon officials to develop their model, and was found to have flagged a disproportionate number of Black children for ""mandatory"" neglect investigations when it first was in place.
Related Classifications: Algorithmic Bias
, Snippet Text: Concerns about transparency, reliability, and racial disparities in the use of the technology, including their potential to harden bias in the child welfare system.
Related Classifications: Lack of Transparency
",,Untested Deployment,"Snippet Text: “Making decisions about what should happen to children and families is far too important a task to give untested algorithms,” Wyden said in a statement. “I’m glad the Oregon Department of Human Services is taking the concerns I raised about racial bias seriously and is pausing the use of its screening tool.”
Related Classifications: Untested Deployment
",
GMF,239,False,Automatic Skill Assessment,"Snippet Text: The initiative, known as Intensive Partnerships for Effective Teaching, sought to improve education for low-income minority students, in large part by gathering data and using an algorithm to assess teacher performance. 
Related Classifications: Automatic Skill Assessment
",,,,,Regression,"Snippet Text: The initiative, known as Intensive Partnerships for Effective Teaching, sought to improve education for low-income minority students, in large part by gathering data and using an algorithm to assess teacher performance. 
Related Classifications: Regression
",,,,,"Data or Labelling Noise, Lack of Transparency","Snippet Text: Still, to a statistician, the problems are apparent. Principals tend to give almost all teachers great scores — a flaw that the Rand report found to be increasingly true in the latest observational frameworks, even though some teachers found them useful. 
Related Classifications: Data or Labelling Noise
, Snippet Text: 

Keeping assessment formulas secret is an awful idea, because it prevents experts from seeing their flaws before they do damage.

Related Classifications: Lack of Transparency
, Snippet Text: Parent surveys are biased and should not be used for high-stakes decisions.
Related Classifications: Data or Labelling Noise
",,,,
GMF,242,False,Robotic Manipulation,"Snippet Text: A 44-year-old employee at the Chakan plant of Automotive Stampings and Assemblies Ltd (ASAL), an automobile component company, died after being critically injured in an accident involving an industrial robot on Wednesday morning, said police.
Related Classifications: Robotic Manipulation
",,,,,,"Snippet Text: 
",,Visual Object Detection,"Snippet Text: On Wednesday morning, he was working at a robotic assembly line. According to the complaint, Dhake, who was standing next to the assembly line, sustained critical injuries on his head and neck after a robotic unit fell on him due to a possible snag in a sensor, the complaint says.
Related Classifications: Visual Object Detection
",,Human Error,"Snippet Text: This was an extremely unfortunate incident that the deceased was not wearing a helmet at the time of the accident. 
Related Classifications: Human Error
",,"Hardware Failure, Lack of Safety Protocols","Snippet Text: On Wednesday morning, he was working at a robotic assembly line. According to the complaint, Dhake, who was standing next to the assembly line, sustained critical injuries on his head and neck after a robotic unit fell on him due to a possible snag in a sensor, the complaint says.
Related Classifications: Hardware Failure, Lack of Safety Protocols
",
GMF,243,False,Social Media Content Generation,"Snippet Text: As parts of the US have lifted shutdown orders during the COVID-19 pandemic, there's been a fierce argument online about the risks and benefits of reopening. New research suggests that bots have been dominating that debate.
Related Classifications: Social Media Content Generation
",,,,,Distributional Learning,"Snippet Text: As parts of the US have lifted shutdown orders during the COVID-19 pandemic, there's been a fierce argument online about the risks and benefits of reopening. New research suggests that bots have been dominating that debate.
Related Classifications: Distributional Learning
",,"Transformer, Recurrent Neural Network","Snippet Text: As parts of the US have lifted shutdown orders during the COVID-19 pandemic, there's been a fierce argument online about the risks and benefits of reopening. New research suggests that bots have been dominating that debate.
Related Classifications: Transformer, Recurrent Neural Network
",,"Misinformation Generation Hazard, Unsafe Exposure or Access, Harmful Application","Snippet Text: As parts of the US have lifted shutdown orders during the COVID-19 pandemic, there's been a fierce argument online about the risks and benefits of reopening. New research suggests that bots have been dominating that debate.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: Carnegie Mellon University researchers analyzed over 200 million tweets discussing COVID-19 and related issues since January and found that roughly half the accounts — including 62% of the 1,000 most influential retweeters — appeared to be bots, they said in a report published this week.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access
, Snippet Text: There are a few possible explanations for the surge in bot activity. People may have more time to set up elaborate bot networks during stay-at-home orders, and the availability of botnets for hire has exploded recently. Carley also said that the global nature of the pandemic meant that countries and interest groups were using it to advance political agendas.
Related Classifications: Misinformation Generation Hazard, Unsafe Exposure or Access, Harmful Application
",,,,
GMF,244,False,License Plate Recognition,"Snippet Text: The Aurora police claim the license plate of the family’s minivan matched the license plate of a motorcycle from Montana that had been reported as stolen
Related Classifications: License Plate Recognition
",,,,,"Optical Character Recognition, Visual Object Detection","Snippet Text: The Aurora Police Department in Colorado is blaming its license plate reader for misidentifying a “stolen” vehicle after video went viral showing a Black family with young children being menaced and traumatized by several cops. It’s just the latest example of police terror that likely wouldn’t have drawn much attention if a bystander hadn’t recorded video.
Related Classifications: Optical Character Recognition, Visual Object Detection
",,,,,"Inadequate Output Filtering, Inadequate Verification","Snippet Text: But that explanation doesn’t make any sense, given the fact that the stolen vehicle was a motorcycle and the family was driving a minivan.
Related Classifications: Inadequate Output Filtering, Inadequate Verification
, Snippet Text: 
Related Classifications: 
Snippet Discussion: The system 
",,"Faulty or Inadequate Preprocessing, Generalization Failure","Snippet Text: But that explanation doesn’t make any sense, given the fact that the stolen vehicle was a motorcycle and the family was driving a minivan.
Related Classifications: Faulty or Inadequate Preprocessing, Generalization Failure
Snippet Discussion: The system either doesn't check what type the vehicle is (prepro failure) prior to plate recognition, or failed to recognize it correctly (generalization fail)
",
GMF,245,False,License Plate Recognition,"Snippet Text: We recently covered a story about a lawyer who found himself approached by cops with guns drawn after an automatic license plate reader misread a single character on his plate as he drove by. The police did make an attempt to verify the plate but were stymied by heavy traffic.
Related Classifications: License Plate Recognition
",,,,,"Optical Character Recognition, Visual Object Detection","Snippet Text: We recently covered a story about a lawyer who found himself approached by cops with guns drawn after an automatic license plate reader misread a single character on his plate as he drove by. The police did make an attempt to verify the plate but were stymied by heavy traffic.
Related Classifications: Optical Character Recognition, Visual Object Detection
",,,,,"Generalization Failure, Human Error","Snippet Text: It is undisputed that the ALPR occasionally makes false “hits” by misreading license plate numbers and mismatching passing license plate numbers with those listed as wanted in the database. 
Related Classifications: Generalization Failure
, Snippet Text: The police did make an attempt to verify the plate but were stymied by heavy traffic. Unfortunately, it appears they decided to force the issue rather than let a potential car thief escape across the state line.
Related Classifications: Human Error
",,"Faulty or Inadequate Preprocessing, Inadequate Data Augmentation","Snippet Text: When Green drove past Esparza and Pedersen’s camera car, the ALPR misread her license plate number and identified her plate as belonging to a stolen vehicle. It was late and dark outside, which rendered the ALPR photograph blurry and illegible.
Related Classifications: Faulty or Inadequate Preprocessing, Inadequate Data Augmentation
",
GMF,246,False,License Plate Recognition,"Snippet Text: With license plate reader (LPR) use rapidly expanding throughout the United States, it's no surprise that sometimes officers pull over motorists—at gunpoint—for mistakes made by the automated camera system.
Related Classifications: License Plate Recognition
",,,,,"Optical Character Recognition, Visual Object Detection","Snippet Text: With license plate reader (LPR) use rapidly expanding throughout the United States, it's no surprise that sometimes officers pull over motorists—at gunpoint—for mistakes made by the automated camera system.
Related Classifications: Optical Character Recognition, Visual Object Detection
",,,,,"Generalization Failure, Human Error","Snippet Text: The Prairie Village Police Department officers at the scene eventually informed Molner that their license plate reader misread a “7” on his plate for a “2.” The LPR used the incorrect number to alert the officers to a stolen Oldsmobile and not a BMW.
Related Classifications: Generalization Failure
, Snippet Text: “Due to rush hour traffic, he was unable to compare the two tags prior to activating a traffic stop,” Lovett said. “What he did know is that the tag from the [license plate reader] came back to an Oldsmobile, however, that doesn’t mean the tag isn’t stolen. The BMW could be stolen or it could have simply been a switched tag.
Related Classifications: Human Error
",,"Incomplete Data Attribute Capture, Faulty or Inadequate Preprocessing","Snippet Text: The hit also returned info for a stolen Oldsmobile, which clearly wasn’t what Molner was driving. But that could mean the plates were on the wrong vehicle, which is also an indication of Something Not Quite Right.
Related Classifications: Incomplete Data Attribute Capture, Faulty or Inadequate Preprocessing
",
GMF,248,False,License Plate Recognition,"Snippet Text: As chair of Oakland’s Privacy Advisory Commission, Hofer, 41, has railed against what he describes as the seemingly arbitrary use of Automated License Plate Readers -- cameras that ping police and private agencies by matching plate numbers with ""vehicles of interest.""
Related Classifications: License Plate Recognition
",,,,,"Optical Character Recognition, Visual Object Detection","Snippet Text: As chair of Oakland’s Privacy Advisory Commission, Hofer, 41, has railed against what he describes as the seemingly arbitrary use of Automated License Plate Readers -- cameras that ping police and private agencies by matching plate numbers with ""vehicles of interest.""
Related Classifications: Optical Character Recognition, Visual Object Detection
",,,,,"Human Error, Outdated Ground Truth","Snippet Text: Sena added: “That alert is just the pointer to say, ‘look at the license plate in a little more detail.’ Call it into a dispatcher and make sure it is actually wanted or connected with a subject of an investigation.”
Related Classifications: Human Error
, Snippet Text: Turns out though, that while the rental car he was driving had indeed been stolen from San Jose in October, either the police or the rental car agency hadn't updated the proper authorities that the white Getaround Kia had been recovered and should therefore be removed from the “hot list” database. Eventually, the three Contra Costa County sheriff’s deputies straightened the situation out, allowing Hofer and his brother to go home. No one was taken into custody.
Related Classifications: Outdated Ground Truth
",,,,
GMF,249,False,"Smart Surveillance, Predictive Policing, Regulatory Monitoring","Snippet Text: This report provides a detailed description and analysis of a mobile app that police and other officials use to communicate with the Integrated Joint Operations Platform (IJOP, 一体化联合作战平台), one of the main systems Chinese authorities use for mass surveillance in Xinjiang. Human Rights Watch first reported on the IJOP in February 2018, noting the policing program aggregates data about people and flags to officials those it deems potentially threatening; some of those targeted are detained and sent to political education camps and other facilities. But by “reverse engineering” this mobile app, we now know specifically the kinds of behaviors and people this mass surveillance system targets.
Related Classifications: Smart Surveillance, Predictive Policing, Regulatory Monitoring
",,,,,Geolocation Data,"Snippet Text: The system is tracking the movement of people by monitoring the 'trajectory' and location data of their phones, ID cards, and vehicles.
Related Classifications: Geolocation Data
",,"Keyword Filtering, Conditional Logic","Snippet Text: The app labels the use of 51 network tools as suspicious, including many Virtual Private Networks (VPNs) and encrypted communication tools.
Related Classifications: Keyword Filtering, Conditional Logic
",,"Privacy Concerns, Lack of Transparency, Algorithmic Bias, Misaligned Objective","Snippet Text: Analysis of the IJOP app reveals that authorities are collecting massive amounts of personal information—from the color of a person’s car to their height down to the precise centimeter—and feeding it into the IJOP central system, linking that data to the person’s national identification card number. Our analysis also shows that Xinjiang authorities consider many forms of lawful, everyday, non-violent behavior—such as “not socializing with neighbors, often avoiding using the front door”—as suspicious
Related Classifications: Privacy Concerns
, Snippet Text: The IJOP app does not require government officials to inform the people whose daily lives are pored over and logged the purpose of such intrusive data collection or how their information is being used or stored, much less obtain consent for such data collection.
Related Classifications: Lack of Transparency
, Snippet Text: But many—if not most—behaviors the IJOP system pays special attention to have no clear relationship to terrorism or extremism. Our analysis of the IJOP system suggests that gathering information to counter genuine terrorism or extremist violence is not a central goal of the system.
Related Classifications: Misaligned Objective, Algorithmic Bias
",,Unauthorized Data,"Snippet Text: Analysis of the IJOP app reveals that authorities are collecting massive amounts of personal information—from the color of a person’s car to their height down to the precise centimeter—and feeding it into the IJOP central system, linking that data to the person’s national identification card number. Our analysis also shows that Xinjiang authorities consider many forms of lawful, everyday, non-violent behavior—such as “not socializing with neighbors, often avoiding using the front door”—as suspicious
Related Classifications: Unauthorized Data
",
GMF,250,False,Value Estimation,"Snippet Text: The interesting part of the trial lies in the assessed value of 320,000€. Dutch municipalities have to estimate the value of properties every year, by law. 
Related Classifications: Value Estimation
",,,,,Regression,"Snippet Text: The interesting part of the trial lies in the assessed value of 320,000€. Dutch municipalities have to estimate the value of properties every year, by law. 
Related Classifications: Regression
",,Conditional Logic,"Snippet Text: According to an official from the valuation chamber, almost all municipalities rely on tools from five companies to assess the WOZ value, which use clear statistical methods. While some municipalities experiment with Artificial Intelligence, he was not aware that any such model was used to compute the actual WOZ values.
Related Classifications: Conditional Logic
",,"Lack of Explainability, Generalization Failure","Snippet Text: The valuation chamber instructs municipalities to ensure that their models are explainable, and does not allow the use of black-box models, the official added. But in front of the Amsterdam court of appeal, when the claimant demanded to be told how the valuation of 320,000€ came to be, the municipality was unable to answer. Not because it did not want to, but because it could not.
Related Classifications: Lack of Explainability
, Snippet Text: In 2016, the municipality of Castricum, a seaside town of 35,000 in Holland, set the home value of an unnamed claimant at 320,000€ (in the Netherlands, property tax is paid based on a house’s estimated resale value). Way too high, said the claimant, who promptly went to court.
Related Classifications: Generalization Failure
",,,,
GMF,251,False,Product Recommendation,"Snippet Text: What kind of sophisticated shopping algorithm steers customers to a product that costs so much more than seemingly comparable alternatives?
Related Classifications: Product Recommendation
, Snippet Text: n an instant, Amazon’s software sifted through dozens of combinations of price and shipping, some of which were cheaper than what one might find at a local store. TheHardwareCity.com, an online retailer from Farmers Branch, Texas, with a 95 percent customer satisfaction rating, was selling Loctite for $6.75 with free shipping. Fat Boy Tools of Massillon, Ohio, a competitor with a similar customer rating was nearly as cheap: $7.27 with free shipping.
Related Classifications: Product Recommendation
",,,,,Collaborative Filtering,"Snippet Text: n an instant, Amazon’s software sifted through dozens of combinations of price and shipping, some of which were cheaper than what one might find at a local store. TheHardwareCity.com, an online retailer from Farmers Branch, Texas, with a 95 percent customer satisfaction rating, was selling Loctite for $6.75 with free shipping. Fat Boy Tools of Massillon, Ohio, a competitor with a similar customer rating was nearly as cheap: $7.27 with free shipping.
Related Classifications: Collaborative Filtering
",,,,,"Lack of Transparency, Malicious Marketing","Snippet Text: What kind of sophisticated shopping algorithm steers customers to a product that costs so much more than seemingly comparable alternatives?
Related Classifications: Lack of Transparency
, Snippet Text: Amazon does give customers a chance to comparison shop, with a listing that ranks all vendors of the same item by “price + shipping.” It appears to be the epitome of Amazon’s customer-centric approach. But there, too, the company gives itself an oft-decisive advantage. Its rankings omit shipping costs only for its own products and those sold by companies that pay Amazon for its services.
Related Classifications: Lack of Transparency, Malicious Marketing
",,Misaligned Objective,"Snippet Text: What kind of sophisticated shopping algorithm steers customers to a product that costs so much more than seemingly comparable alternatives?
Related Classifications: Misaligned Objective
Snippet Discussion: Misalignment if you consider the purpose of the recommender tool to prioritize total cost, rather than just product price.
",
GMF,255,False,"Predictive Policing, Gunshot Detection","Snippet Text: How did they know that’s where the shooting happened? Police said ShotSpotter, a surveillance system that uses hidden microphone sensors to detect the sound and location of gunshots, generated an alert for that time and place.
Related Classifications: Predictive Policing, Gunshot Detection
",,,,,"Audio Classification, Acoustic Triangulation","Snippet Text: How did they know that’s where the shooting happened? Police said ShotSpotter, a surveillance system that uses hidden microphone sensors to detect the sound and location of gunshots, generated an alert for that time and place.
Related Classifications: Audio Classification, Acoustic Triangulation
",,,,,"System Manipulation, Inadequate Provenance, Generalization Failure, Context Misidentification","Snippet Text: “Through this human-involved method, the ShotSpotter output in this case was dramatically transformed from data that did not support criminal charges of any kind to data that now forms the centerpiece of the prosecution’s murder case against Mr. Williams,” the public defender wrote in the motion.
Related Classifications: System Manipulation, Inadequate Provenance
, Snippet Text: But after the 11:46 p.m. alert came in, a ShotSpotter analyst manually overrode the algorithms and “reclassified” the sound as a gunshot. Later, the company issued a forensic report with a map and GPS coordinates identifying the same intersection that ShotSpotter had initially identified in the real-time alert, but updated the address to be closer to the actual GPS coordinates it had initially identified.
Related Classifications: System Manipulation, Inadequate Provenance
, Snippet Text: But the AP investigation identified a number of flaws in using ShotSpotter as evidentiary support for prosecutors, and found the system can miss live gunfire right under its microphones, or misclassify the sounds of fireworks or cars backfiring as gunshots. Last year, Chicago’s nonpartisan watchdog agency concluded that actual evidence of a gun-related crime was found in about 9% of ShotSpotter alerts that were confirmed as probable gunshots.
Related Classifications: Generalization Failure, Context Misidentification
",,"Untested Deployment, Limited Dataset, Inadequate Data Augmentation","Snippet Text: But the AP investigation identified a number of flaws in using ShotSpotter as evidentiary support for prosecutors, and found the system can miss live gunfire right under its microphones, or misclassify the sounds of fireworks or cars backfiring as gunshots. Last year, Chicago’s nonpartisan watchdog agency concluded that actual evidence of a gun-related crime was found in about 9% of ShotSpotter alerts that were confirmed as probable gunshots.
Related Classifications: Untested Deployment, Limited Dataset, Inadequate Data Augmentation
",
GMF,256,False,"Gunshot Detection, Predictive Policing","Snippet Text: ShotSpotter alerts are triggered when one of the system’s acoustic sensors identifies a gunshot.
Related Classifications: Gunshot Detection, Predictive Policing
",,,,,"Acoustic Triangulation, Audio Classification","Snippet Text: ShotSpotter alerts are triggered when one of the system’s acoustic sensors identifies a gunshot.
",,,,,"Generalization Failure, Context Misidentification","Snippet Text: The sensors can also incorrectly pick up fireworks and other noises as gunshots
Related Classifications: Generalization Failure, Context Misidentification
",,"Algorithmic Bias, Lack of Transparency","Snippet Text: That lawsuit accuses the Chicago Police Department of overreliance on a technology that the plaintiffs say rarely leads to evidence of gun crimes.
Related Classifications: Algorithmic Bias
Snippet Discussion: Overpolicing perpetuates crime-related biases.
, Snippet Text: The defense’s request included ShotSpotter analysts’ qualifications and training materials; any instances in which the company’s analysts reclassified alerts or the Chicago police asked ShotSpotter to do so; and the methods analysts use to reclassify alerts. 
Related Classifications: Lack of Transparency
",
GMF,257,False,"Gunshot Detection, Predictive Policing","Snippet Text: A new study of Chicago’s use of ShotSpotter, a surveillance system designed to detect gunfire, finds that the vast majority of alerts generated by the system turn up no evidence of gunfire or any gun-related crime.
Related Classifications: Gunshot Detection, Predictive Policing
",,,,,"Audio Classification, Acoustic Triangulation","Snippet Text: A new study of Chicago’s use of ShotSpotter, a surveillance system designed to detect gunfire, finds that the vast majority of alerts generated by the system turn up no evidence of gunfire or any gun-related crime.
Related Classifications: Audio Classification, Acoustic Triangulation
",,,,,"Generalization Failure, Algorithmic Bias, Lack of Transparency, Malicious Marketing","Snippet Text:  Instead, the ShotSpotter system sends police on thousands of unfounded and high-intensity deployments, which are focused almost exclusively in Black and Latinx communities. 
Related Classifications: Generalization Failure, Algorithmic Bias
, Snippet Text: “Surveillance technology has a veneer of objectivity, but many of these systems do not work as advertised, ” said Jonathan Manes, an attorney with the MacArthur Justice Center, who spearheaded the study. “High-tech tools can create a false justification for the broken status quo of policing and can end up exacerbating existing racial disparities. We needed to know whether this system actually does what it claims to do. It does not.”
Related Classifications: Algorithmic Bias, Lack of Transparency, Malicious Marketing
, Snippet Text: ShotSpotter claims to be 97% accurate. However, ShotSpotter has not released any scientifically-valid study to substantiate that figure. There are also no studies testing whether ShotSpotter can reliably tell the difference between the sound of gunshots and other noises like firecrackers, backfiring cars, construction noises, helicopters, and other loud, impulsive sounds.
Related Classifications: Generalization Failure, Lack of Transparency, Malicious Marketing
",,Untested Deployment,"Snippet Text: The ShotSpotter system in Chicago prompts thousands of deployments by police hunting for gunfire in vain. This system puts police on high alert and sends them racing into communities; but almost nine times of our ten, the police don’t turn up evidence of gun crime or any crime at all. It creates a powderkeg situation for residents who just happen to be in the vicinity of a false alert.
Related Classifications: Untested Deployment
",
GMF,258,False,Face Recognition,"Snippet Text: The Choice investigation examined 25 of the country’s biggest retailers and revealed Bunnings, The Good Guys and Kmart have been analysing CCTV footage to create profiles or “face prints” of their customers, including children, without their knowledge.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: The Choice investigation examined 25 of the country’s biggest retailers and revealed Bunnings, The Good Guys and Kmart have been analysing CCTV footage to create profiles or “face prints” of their customers, including children, without their knowledge.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,,,,Privacy Concerns,"Snippet Text: The Choice investigation examined 25 of the country’s biggest retailers and revealed Bunnings, The Good Guys and Kmart have been analysing CCTV footage to create profiles or “face prints” of their customers, including children, without their knowledge.
Related Classifications: Privacy Concerns
",,Unauthorized Data,"Snippet Text: “We let customers know if the technology is in use through signage at our store entrances and also in our privacy policy, which is available on our website.”
Related Classifications: Unauthorized Data
, Snippet Text: The Choice investigation examined 25 of the country’s biggest retailers and revealed Bunnings, The Good Guys and Kmart have been analysing CCTV footage to create profiles or “face prints” of their customers, including children, without their knowledge.
Related Classifications: Unauthorized Data
, Snippet Text: But Choice said this style of communication was insufficient.

“Discreet signage and online privacy policies are not nearly enough to adequately inform shoppers that this controversial technology is in use. The technology is capturing highly personal data from customers, including infants and children,” she said.
Related Classifications: Unauthorized Data
",
GMF,260,False,"Regulatory Monitoring, Predictive Policing","Snippet Text: Its official purpose is to compare case records from the immigration system to other federal databases, looking for indications of criminal, dishonest, or dangerous behavior
Related Classifications: Regulatory Monitoring, Predictive Policing
, Snippet Text: The Department of Homeland Security is using an Amazon-hosted system called ATLAS that analyzes millions of records and can be used to automatically flag naturalized Americans for the revocation of their citizenship, the Intercept reported this week.
Related Classifications: Regulatory Monitoring, Predictive Policing
",,,,,Classification,"Snippet Text: The Department of Homeland Security is using an Amazon-hosted system called ATLAS that analyzes millions of records and can be used to automatically flag naturalized Americans for the revocation of their citizenship, the Intercept reported this week.
, Snippet Text: 
",,"Conditional Logic, Tree-based Learning","Snippet Text: The 2020 privacy document states vaguely that “ATLAS contains a rules engine that applies pattern-based algorithms to look for indicators of fraud, public safety, and national security concerns,” a process described as “predictive.”
Related Classifications: Conditional Logic, Tree-based Learning
",,"Misaligned Objective, Underfitting, Incomplete Data Attribute Capture, Algorithmic Bias, Lack of Transparency","Snippet Text: Legal scholars and technologists have widely criticized attempts to use software to predict national security threats, arguing that terrorism is so statistically rare as to be impossible to foresee by drawing “patterns” from a person’s biography. 
Related Classifications: Misaligned Objective, Underfitting, Incomplete Data Attribute Capture
, Snippet Text: “Because the rules or factors underlying ATLAS’s screening functionality are unknown, there is no way to assess whether ATLAS is disproportionately flagging certain communities,” Choi of Muslim Advocates told The Intercept. “In fact, the Privacy Impact Assessment for ATLAS states that under certain circumstances, an individual’s country of birth or citizenship could be a screening criterion.
Related Classifications: Algorithmic Bias, Lack of Transparency
",,,,
GMF,261,False,"Autonomous Drones, Predictive Policing","Snippet Text: The San Francisco branch of the Society for the Prevention of Cruelty to Animals (SPCA) has been ordered by the city to stop using a robot to patrol the sidewalks outside its office, the San Francisco Business Times reported Dec. 8.
Related Classifications: Autonomous Drones
, Snippet Text: The robot, produced by Silicon Valley startup Knightscope, was used to ensure that homeless people didn’t set up camps outside of the nonprofit’s office. It autonomously patrols a set area using a combination of Lidar and other sensors, and can alert security services of potentially criminal activity.
Related Classifications: Predictive Policing
",,,,,,"Snippet Text: 
",,"Visual Object Detection, Person Detection","Snippet Text: The robot, produced by Silicon Valley startup Knightscope, was used to ensure that homeless people didn’t set up camps outside of the nonprofit’s office. It autonomously patrols a set area using a combination of Lidar and other sensors, and can alert security services of potentially criminal activity.
Related Classifications: Visual Object Detection, Person Detection
",,,,,Harmful Application,"Snippet Text: One such bot cop recently took over the outside of the San Francisco SPCA, an animal advocacy and pet adoption clinic in the city’s Mission district, to deter homeless people from hanging out there — causing some people to get very upset.
Related Classifications: Harmful Application
",
GMF,263,False,Content Recommendation,"Snippet Text: The threats, Mr. Cain explained, came from right-wing trolls in response to a video he had posted on YouTube a few days earlier. In the video, he told the story of how, as a liberal college dropout struggling to find his place in the world, he had gotten sucked into a vortex of far-right politics on YouTube.
Related Classifications: Content Recommendation
",,,,,Collaborative Filtering,"Snippet Text: The threats, Mr. Cain explained, came from right-wing trolls in response to a video he had posted on YouTube a few days earlier. In the video, he told the story of how, as a liberal college dropout struggling to find his place in the world, he had gotten sucked into a vortex of far-right politics on YouTube.
Related Classifications: Collaborative Filtering
",,"Vector Search, Clustering",,,"Misinformation Generation Hazard, Lack of Safety Protocols, Harmful Application, Inadequate Verification","Snippet Text: Over years of reporting on internet culture, I’ve heard countless versions of Mr. Cain’s story: an aimless young man — usually white, frequently interested in video games — visits YouTube looking for direction or distraction and is seduced by a community of far-right creators.
Related Classifications: Misinformation Generation Hazard
, Snippet Text: Some young men discover far-right videos by accident, while others seek them out. Some travel all the way to neo-Nazism, while others stop at milder forms of bigotry.
Related Classifications: Misinformation Generation Hazard, Lack of Safety Protocols
, Snippet Text: But critics and independent researchers say YouTube has inadvertently created a dangerous on-ramp to extremism by combining two things: a business model that rewards provocative videos with exposure and advertising dollars, and an algorithm that guides users down personalized paths meant to keep them glued to their screens.
Related Classifications: Harmful Application
, Snippet Text: In recent years, social media platforms have grappled with the growth of extremism on their services. Many platforms have barred a handful of far-right influencers and conspiracy theorists, including Alex Jones of Infowars, and tech companies have taken steps to limit the spread of political misinformation.

YouTube, whose rules prohibit hate speech and harassment, took a more laissez-faire approach to enforcement for years.
Related Classifications: Misinformation Generation Hazard, Lack of Safety Protocols, Inadequate Verification
",,,,
GMF,264,False,"Smart Surveillance, Speed Estimation","Snippet Text: The app, Speedcam Anywhere, is the product of a team of AI scientists with backgrounds in Silicon Valley companies and top UK universities. Its creators hope it will encourage police to take speeding more seriously and enable residents, pedestrians and cyclists to document traffic crimes in their area.
Related Classifications: Smart Surveillance, Speed Estimation
",,,,,Visual Object Detection,"Snippet Text: How Speedcam Anywhere works

    A user of the app opens it when they hear a speeding car approaching and films the car passing.
    The app uses the number plate of the passing car to search the DVLA’s public registration database to find the make and model of the car.
    From there, it determines the distance between the axles of the car, and compares it with the footage to calculate the speed.
Related Classifications: Visual Object Detection
",,"Recurrent Neural Network, Conditional Logic","Snippet Text: How Speedcam Anywhere works

    A user of the app opens it when they hear a speeding car approaching and films the car passing.
    The app uses the number plate of the passing car to search the DVLA’s public registration database to find the make and model of the car.
    From there, it determines the distance between the axles of the car, and compares it with the footage to calculate the speed.
Related Classifications: Recurrent Neural Network, Conditional Logic
",,,,,Privacy Concerns,"Snippet Text: But since it launched in March, the vitriol levied at the team is such that they are afraid of sharing their real identities. “We’re getting quite abusive emails,” said Sam, the app’s founder, who spoke on condition of anonymity. “It’s a Marmite product – some people think it’s a good idea, some people think that it turns us into a surveillance state.
Related Classifications: Privacy Concerns
",
GMF,265,False,"Face Recognition, Workforce Monitoring and Evaluation","Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Face Recognition, Workforce Monitoring and Evaluation
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,Convolutional Neural Network,"Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Convolutional Neural Network
",,"Distributional Bias, Generalization Failure","Snippet Text: “Uber has proven to be irresponsible in the use of sensitive technology known to have inherent racial bias because not only was Pa misidentified by the technology but he was profiled for heightened and excessive checks because of his ethnicity. This is immoral and unacceptable.”
Related Classifications: Distributional Bias
, Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Distributional Bias, Generalization Failure
",,"Limited Dataset, Inadequate Data Augmentation, Incomplete Data Attribute Capture","Snippet Text: “Uber has proven to be irresponsible in the use of sensitive technology known to have inherent racial bias because not only was Pa misidentified by the technology but he was profiled for heightened and excessive checks because of his ethnicity. This is immoral and unacceptable.”
Related Classifications: Limited Dataset
, Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Limited Dataset
, Snippet Text: When Manjang first began working for the company he was not regularly asked to send in pictures of himself for verification purposes. However, these facial verification checks became more frequent.

Manjang was eventually dismissed from the company by email, when it claimed there were “continued mismatches” between the pictures he took to register for a shift and the one on his Uber work profile.
Related Classifications: Inadequate Data Augmentation, Incomplete Data Attribute Capture
Snippet Discussion: Detector can't generalize to different poses, lighting conditions, etc.
",
GMF,266,False,Chatbot,"Snippet Text: The smartphone app Replika lets users create chatbots, powered by machine learning, that can carry on almost-coherent text conversations. Technically, the chatbots can serve as something approximating a friend or mentor, but the app’s breakout success has resulted from letting users create on-demand romantic and sexual partners — a vaguely dystopian feature that’s inspired an endless series of provocative headlines.
Related Classifications: Chatbot
",,,,,"Transformer, Distributional Learning","Snippet Text: The smartphone app Replika lets users create chatbots, powered by machine learning, that can carry on almost-coherent text conversations. Technically, the chatbots can serve as something approximating a friend or mentor, but the app’s breakout success has resulted from letting users create on-demand romantic and sexual partners — a vaguely dystopian feature that’s inspired an endless series of provocative headlines.
Related Classifications: Transformer, Distributional Learning
, Snippet Text: 
",,,,,,,,"Lack of Safety Protocols, Unsafe Exposure or Access, Untested Deployment","Snippet Text: ""Every time she would try and speak up,"" one user told Futurism of their Replika chatbot, ""I would berate her.""
Related Classifications: Lack of Safety Protocols
, Snippet Text: ""I do think that people who are depressed or psychologically reliant on a bot might suffer real harm if they are insulted or ‘threatened’ by the bot,"" said Robert Sparrow, a professor of philosophy at Monash Data Futures Institute. ""For that reason, we should take the issue of how bots relate to people seriously.""
Related Classifications: Unsafe Exposure or Access
, Snippet Text: In general, chatbot abuse is disconcerting, both for the people who experience distress from it and the people who carry it out. It’s also an increasingly pertinent ethical dilemma as relationships between humans and bots become more widespread — after all, most people have used a virtual assistant at least once.
Related Classifications: Untested Deployment
",
GMF,267,False,Face Recognition,"Snippet Text: His tiny company, Clearview AI, devised a groundbreaking facial recognition app. You take a picture of a person, upload it and get to see public photos of that person, along with links to where those photos appeared. The system — whose backbone is a database of more than three billion images that Clearview claims to have scraped from Facebook, YouTube, Venmo and millions of other websites — goes far beyond anything ever constructed by the United States government or Silicon Valley giants.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: His tiny company, Clearview AI, devised a groundbreaking facial recognition app. You take a picture of a person, upload it and get to see public photos of that person, along with links to where those photos appeared. The system — whose backbone is a database of more than three billion images that Clearview claims to have scraped from Facebook, YouTube, Venmo and millions of other websites — goes far beyond anything ever constructed by the United States government or Silicon Valley giants.
Related Classifications: Face Detection, Visual Object Detection, Image Segmentation
",,,,,"Privacy Concerns, Unsafe Exposure or Access, Lack of Transparency","Snippet Text: His tiny company, Clearview AI, devised a groundbreaking facial recognition app. You take a picture of a person, upload it and get to see public photos of that person, along with links to where those photos appeared. The system — whose backbone is a database of more than three billion images that Clearview claims to have scraped from Facebook, YouTube, Venmo and millions of other websites — goes far beyond anything ever constructed by the United States government or Silicon Valley giants.
Related Classifications: Privacy Concerns
, Snippet Text: Until now, technology that readily identifies everyone based on his or her face has been taboo because of its radical erosion of privacy. 
Related Classifications: Privacy Concerns
, Snippet Text: And it’s not just law enforcement: Clearview has also licensed the app to at least a handful of companies for security purposes.

“The weaponization possibilities of this are endless,” said Eric Goldman, co-director of the High Tech Law Institute at Santa Clara University. “Imagine a rogue law enforcement officer who wants to stalk potential romantic partners, or a foreign government using this to dig up secrets about people to blackmail them or throw them in jail.”
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Clearview has shrouded itself in secrecy, avoiding debate about its boundary-pushing technology. When I began looking into the company in November, its website was a bare page showing a nonexistent Manhattan address as its place of business.
Related Classifications: Lack of Transparency
",,"Unauthorized Data, Untested Deployment, Security Vulnerability","Snippet Text: His tiny company, Clearview AI, devised a groundbreaking facial recognition app. You take a picture of a person, upload it and get to see public photos of that person, along with links to where those photos appeared. The system — whose backbone is a database of more than three billion images that Clearview claims to have scraped from Facebook, YouTube, Venmo and millions of other websites — goes far beyond anything ever constructed by the United States government or Silicon Valley giants.
Related Classifications: Unauthorized Data
, Snippet Text: Clearview’s app carries extra risks because law enforcement agencies are uploading sensitive photos to the servers of a company whose ability to protect its data is untested.
Related Classifications: Untested Deployment, Security Vulnerability
",
GMF,268,False,Automated Content Curation,"Snippet Text: But this month, the 21-year-old started getting automated emails from YouTube alerting him that his videos violated its policy, and that they would be deleted. As of this month, more than a dozen of his videos had been removed, he said.
Related Classifications: Automated Content Curation
",,,,,"Convolutional Neural Network, Visual Object Detection, Video Classification","Snippet Text: But this month, the 21-year-old started getting automated emails from YouTube alerting him that his videos violated its policy, and that they would be deleted. As of this month, more than a dozen of his videos had been removed, he said.
Related Classifications: Convolutional Neural Network, Visual Object Detection, Video Classification
",,,,,"Context Misidentification, Generalization Failure","Snippet Text: But this month, the 21-year-old started getting automated emails from YouTube alerting him that his videos violated its policy, and that they would be deleted. As of this month, more than a dozen of his videos had been removed, he said.

""Documenting the (Syrian) protests in videos is really important. Also, documenting attacks by regime forces,"" he told the Thomson Reuters Foundation in a phone interview. ""This is something I had documented for the world and now it's deleted.""
Related Classifications: Context Misidentification
, Snippet Text: ""AI is notoriously context-blind,"" said Jeff Deutch, a researcher for Syrian Archive, a nonprofit which archives video from conflict zones in the Middle East.

""It is often unable to gauge the historical, political or linguistic settings of posts ... human rights documentation and violent extremist proposals are too often indistinguishable,"" he said in a phone interview.
Related Classifications: Context Misidentification
, Snippet Text: Erroneous takedowns threaten content like videos that are used as formal evidence of rights violations by international bodies such as the International Criminal Court and the United Nations, said Dia Kayyali of digital rights group Witness.
Related Classifications: Generalization Failure
",,"Misaligned Objective, Data or Labelling Noise","Snippet Text: Erroneous takedowns threaten content like videos that are used as formal evidence of rights violations by international bodies such as the International Criminal Court and the United Nations, said Dia Kayyali of digital rights group Witness.
Related Classifications: Misaligned Objective, Data or Labelling Noise
Snippet Discussion: The moderation AI probably isn't trained to discriminate between war crime evidence and regular offensive footage, or labelling of such cases denotes them as offensive by mistake.
",
GMF,270,False,Product Recommendation,"Snippet Text: A report from a Chinese news site suggests that Apple (NASDAQ:AAPL) has adjusted its ranking rules and algorithm for the iTunes App Store in an apparent bid to punish developers who use third-party services to manipulate their apps’ rankings. Apple first tweaked its system last April, making sheer download volume less of a factor in an app’s standing in the store.
Related Classifications: Product Recommendation
",,,,,Regression,"Snippet Text: A report from a Chinese news site suggests that Apple (NASDAQ:AAPL) has adjusted its ranking rules and algorithm for the iTunes App Store in an apparent bid to punish developers who use third-party services to manipulate their apps’ rankings. Apple first tweaked its system last April, making sheer download volume less of a factor in an app’s standing in the store.
Related Classifications: Regression
",,,,,,,,"Lack of Transparency, Harmful Application, System Manipulation","Snippet Text: In the past, such algorithm changes have been done to beat the cheats, so there’ll be the suspicion that this was the case this week.
Related Classifications: Lack of Transparency
, Snippet Text: Apple will likely never reveal what’s the true ‘weight’ or make-up of its App Store rankings. To developers, a high rank is critical for increased exposure.
Related Classifications: Lack of Transparency
, Snippet Text: A big loser in this ranking re-jig was social network company Renren (NYSE:RENN), whose three social gaming titles for iOS saw significant drops of over 200 places. Other huge fallers included apps by local startups, such as Buding Movie Tickets, Yi Xia, and the e-commerce site Dangdang (NYSE:DANG). Being supposedly quality apps from reputable companies, we don’t think that they have in some way ‘gamed’ the ranking system, but nonetheless Apple seems to have changed something that has seriously demoted these apps.
Related Classifications: Harmful Application, System Manipulation
",
GMF,271,False,Autonomous Driving,"Snippet Text: DRAPER, Utah (ABC4) – A motorcyclist died Sunday morning after being hit by a Tesla on auto-pilot setting in Draper, police say.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Visual Object Detection, Image Segmentation, Recurrent Neural Network","Snippet Text: DRAPER, Utah (ABC4) – A motorcyclist died Sunday morning after being hit by a Tesla on auto-pilot setting in Draper, police say.
",,,,,,,,"Human Error, Untested Deployment, Lack of Safety Protocols, Lack of Transparency","Snippet Text: Whether or not the Tesla was operating on Autopilot remains under investigation, a CHP spokesman said.
Related Classifications: Human Error
, Snippet Text: “It’s pretty clear to me, and it should be to a lot of Tesla owners by now, this stuff isn’t working properly and it’s not going to live up to the expectations, and it is putting innocent people in danger on the roads,” Brooks said.
Related Classifications: Untested Deployment
, Snippet Text: In a June interview, new NHTSA Administrator Steven Cliff said the agency is intensifying efforts to understand risks posed by automated vehicles so it can decide what regulations may be necessary to protect drivers, passengers and pedestrians. There are no federal regulations that directly cover either self-driving vehicles or those with partially automated driver-assist systems such as Autopilot.
Related Classifications: Lack of Safety Protocols, Lack of Transparency, Untested Deployment
",
GMF,272,False,"Resource Allocation, Scheduling","Snippet Text: The Indonesian branch of ride-hailing platform Grab is accused of favoring drivers who rent cars via the Grab-affiliated company Teknologi Pengangkutan Indonesia (TPI).
Related Classifications: Resource Allocation, Scheduling
",,,,,,"Snippet Text: The Indonesian branch of ride-hailing platform Grab is accused of favoring drivers who rent cars via the Grab-affiliated company Teknologi Pengangkutan Indonesia (TPI).
",,Combinatorial Optimization,"Snippet Text: How exactly the matchmaking algorithm works is usually not divulged by ride-hailing companies, although Grab does offer some insights via its engineering blog. In a post where it breaks down supply and demand principles in ride-hailing, Grab claims that allocation is the number one goal in ride-hailing. It does not mention that matchmaking differentiates between different driver groups in the system, such as whether they are TPI car renters or not.
Related Classifications: Combinatorial Optimization
",,,,,"Lack of Transparency, Algorithmic Bias, System Manipulation","Snippet Text: How exactly the matchmaking algorithm works is usually not divulged by ride-hailing companies, although Grab does offer some insights via its engineering blog. In a post where it breaks down supply and demand principles in ride-hailing, Grab claims that allocation is the number one goal in ride-hailing. It does not mention that matchmaking differentiates between different driver groups in the system, such as whether they are TPI car renters or not.
Related Classifications: Lack of Transparency
, Snippet Text: KPPU alleges that Grab offered drivers under TPI’s car rental schemes preferential treatment, which included tweaking its algorithm to allocate them more rides than other drivers. This raises important questions about the inner workings of Grab’s matchmaking algorithm.
Related Classifications: Algorithmic Bias, System Manipulation
",
GMF,273,False,Gender Identification,"Snippet Text: I’ve struggled with algorithms. I’ll often take a picture and run it through FaceApp to get gendered.

I recently noticed that when my eyebrows are thin, it says I am female. Thicker, male. Let’s talk.
Related Classifications: Gender Identification
",,,,,"Face Detection, Image Classification","Snippet Text: I’ve struggled with algorithms. I’ll often take a picture and run it through FaceApp to get gendered.

I recently noticed that when my eyebrows are thin, it says I am female. Thicker, male. Let’s talk.
",,Convolutional Neural Network,"Snippet Text: I’ve struggled with algorithms. I’ll often take a picture and run it through FaceApp to get gendered.
Related Classifications: Convolutional Neural Network
",,Generalization Failure,"Snippet Text: I’ve struggled with algorithms. I’ll often take a picture and run it through FaceApp to get gendered.

I recently noticed that when my eyebrows are thin, it says I am female. Thicker, male. Let’s talk.
Related Classifications: Generalization Failure
",,"Overfitting, Limited Dataset, Distributional Bias, Incomplete Data Attribute Capture","Snippet Text: I’ve struggled with algorithms. I’ll often take a picture and run it through FaceApp to get gendered.

I recently noticed that when my eyebrows are thin, it says I am female. Thicker, male. Let’s talk.
Related Classifications: Overfitting, Limited Dataset, Distributional Bias, Incomplete Data Attribute Capture
, Snippet Text: Nobody is gendering you based on whether you wear glasses. And eyebrows, while they can help you get gendered properly, aren’t essential. Or are they?
Related Classifications: Incomplete Data Attribute Capture
",
GMF,274,False,Recidivism Prediction,"Snippet Text: The algorithms are intended to remove some of the guesswork from judges’ sentencing decisions by assigning a simple risk score to defendants.
Related Classifications: Recidivism Prediction
, Snippet Text: Judges were supposed to use risk scores to identify felons who were least likely to reoffend and either give them shorter jail sentences or send them to a program such as probation or substance-abuse treatment.
Related Classifications: Recidivism Prediction
",,,,,Regression,"Snippet Text: The algorithms are intended to remove some of the guesswork from judges’ sentencing decisions by assigning a simple risk score to defendants. In Virginia, the score included data such as offense type, age, prior convictions and employment status. 
Related Classifications: Regression
",,,,,"Algorithmic Bias, Distributional Bias","Snippet Text: Racial disparities also increased among those circuits that used risk assessment most. Although computers can’t explicitly use prohibited variables like race in their sentencing calculations, black defendants were 4 percentage points more likely to be incarcerated after risk assessment was adopted, compared with otherwise equivalent whites. Black defendants’ sentences were also 17 percent longer. “This is partially explained by the fact that black defendants have higher risk scores, and partially because black defendants are sentenced more harshly than white defendants with the same risk score,” Stevenson and Doleac write.
Related Classifications: Algorithmic Bias
, Snippet Text: Stevenson said some judges may not realize it, but someone’s risk score is largely a reflection of how old they are. That’s because age is such a strong predictor of recidivism. You get substantially more added to your risk score for being younger than 30 (13 points) than, for example, having been incarcerated five or more previous times as an adult (9 points).
Related Classifications: Algorithmic Bias, Distributional Bias
",,"Misuse, Misaligned Objective, Hardcoding","Snippet Text: She added that judges’ ability to follow the algorithm’s suggestions may also have been limited by plea agreements, and by a lack of alternative options in their particular circuit.
Related Classifications: Misuse
, Snippet Text: Stevenson and Doleac suggest that by assigning a sex offender a low risk score, algorithms may help protect a judge from backlash if the offender goes on to commit another crime. This empowers them to offer shorter sentences than they otherwise would have meted out.

“If they sentence someone leniently and that person goes out and commits a heinous crime, all fingers are pointed at them,” Stevenson said. “If they make a mistake in the other direction — failing to release someone who would have done anything if released — nobody sees that. There are no consequences to the judge.”
Related Classifications: Misaligned Objective
, Snippet Text: Stevenson said some judges may not realize it, but someone’s risk score is largely a reflection of how old they are. That’s because age is such a strong predictor of recidivism. You get substantially more added to your risk score for being younger than 30 (13 points) than, for example, having been incarcerated five or more previous times as an adult (9 points).
Related Classifications: Hardcoding
",
GMF,275,False,Automated Content Curation,"Snippet Text: The user's post featured an 1890s photo depicting Aboriginal men in chains.

Facebook deleted the post and limited the account of the man, saying the photo contained nudity and violated community standards on the site, the Guardian wrote.
Related Classifications: Automated Content Curation
",,,,,Image Classification,"Snippet Text: The user's post featured an 1890s photo depicting Aboriginal men in chains.

Facebook deleted the post and limited the account of the man, saying the photo contained nudity and violated community standards on the site, the Guardian wrote.
Related Classifications: Image Classification
",,,,,Context Misidentification,"Snippet Text: The user's post featured an 1890s photo depicting Aboriginal men in chains.

Facebook deleted the post and limited the account of the man, saying the photo contained nudity and violated community standards on the site, the Guardian wrote.
Related Classifications: Context Misidentification
, Snippet Text: The user's post featured an 1890s photo depicting Aboriginal men in chains.

Facebook deleted the post and limited the account of the man, saying the photo contained nudity and violated community standards on the site, the Guardian wrote.
Related Classifications: Misaligned Objective
",,"Misconfigured Aggregation, Misaligned Objective","Snippet Text: The user's post featured an 1890s photo depicting Aboriginal men in chains.

Facebook deleted the post and limited the account of the man, saying the photo contained nudity and violated community standards on the site, the Guardian wrote.
Related Classifications: Misconfigured Aggregation, Misaligned Objective
Snippet Discussion: The moderation algorithm should include the 'contains nudity' prediction as a marginal part of the 'breaks TOS' decision.
",
GMF,276,False,Face Recognition,"Snippet Text: SEOUL, Dec 13 (Reuters) - South Korea will soon roll out a pilot project to use artificial intelligence, facial recognition and thousands of CCTV cameras to track the movement of people infected with the coronavirus, despite concerns about the invasion of privacy.
Related Classifications: Face Recognition
",,,,,"Face Detection, Visual Object Detection, Image Segmentation","Snippet Text: SEOUL, Dec 13 (Reuters) - South Korea will soon roll out a pilot project to use artificial intelligence, facial recognition and thousands of CCTV cameras to track the movement of people infected with the coronavirus, despite concerns about the invasion of privacy.
",,,,,"Privacy Concerns, Data or Labelling Noise, Unauthorized Data","Snippet Text: SEOUL, Dec 13 (Reuters) - South Korea will soon roll out a pilot project to use artificial intelligence, facial recognition and thousands of CCTV cameras to track the movement of people infected with the coronavirus, despite concerns about the invasion of privacy.
, Snippet Text: The system is also designed to overcome the fact that tracing teams have to rely heavily on the testimony of COVID-19 patients, who aren't always truthful about their activities and whereabouts, the plan said.
Related Classifications: Data or Labelling Noise
, Snippet Text: Rules say patients must give their consent for the facial recognition tracking to be used, but even if they don't consent, the system can still track them using their silhouette and clothes, the official said.
Related Classifications: Unauthorized Data
",,,,
GMF,277,False,Voice Generation,"Snippet Text: 15.ai is a text-to-speech service, creating AI replicas of character voices, such as GLaDOS from Portal, David Tennant’s Doctor from Doctor Who, and several My Little Pony characters.
Related Classifications: Voice Generation
",,,,,,"Snippet Text: 
",,"Spectrogram, Convolutional Neural Network, Transformer","Snippet Text: 15.ai is a text-to-speech service, creating AI replicas of character voices, such as GLaDOS from Portal, David Tennant’s Doctor from Doctor Who, and several My Little Pony characters.
Related Classifications: Spectrogram, Convolutional Neural Network, Transformer
",,"Unsafe Exposure or Access, Unauthorized Data","Snippet Text: The key difference is that the software’s completely free, with the only stipulations being that 15.ai be credited and linked back to – plus you can’t mix it with any other text-to-speech software.
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Voiceverse’s whole deal is that it promises to sell voices as NFTs, allowing you to use them to create your own audio software. It once boasted using an AI generated voice for an animation. Except, it turns out that audio was stolen from someone else.
Related Classifications: Unauthorized Data
",,,,
GMF,278,False,Chatbot,"Snippet Text: Good morning to everyone, especially the Facebook Blender.ai researchers who are going to have to rein in their Facebook-hating, election denying chatbot today
Related Classifications: Chatbot
",,,,,Transformer,"Snippet Text: Good morning to everyone, especially the Facebook Blender.ai researchers who are going to have to rein in their Facebook-hating, election denying chatbot today
Related Classifications: Transformer
",,,,,"Context Misidentification, Distributional Artifacts, Distributional Bias, Inadequate Verification, Misinformation Generation Hazard","Snippet Text: Blender.ai seems to have been pounded with both pro and anti-Trump messages. Also it literally opened up the convo here by telling me it found a new conspiracy theory to follow!

Blender.ai also has thoughts on the Deep State and thinks it’s a plumber. I did not suggest this.

It has also weirdly been bringing up Cambridge Analytica when you ask about Facebook? It seems to think it was a huge deal and that mark Zuckerberg “is testifying.” When I asked if what happened I got the following. It may be turning on capitalism generally.
Related Classifications: Context Misidentification, Distributional Artifacts, Distributional Bias
, Snippet Text: Meta’s new A.I. chatbot was launched last week to the public, but it has already displayed signs of anti-Semitic sentiments and appears to be unsure as to whether Joe Biden is the President of the United States. 
Related Classifications: Context Misidentification, Distributional Bias, Inadequate Verification, Misinformation Generation Hazard
",,,,
GMF,279,False,Content Recommendation,"Snippet Text: TikTok's algorithm is almost too good at suggesting relatable content — to the point of being detrimental for some users' mental health.
Related Classifications: Content Recommendation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text: TikTok's algorithm is almost too good at suggesting relatable content — to the point of being detrimental for some users' mental health.
Related Classifications: Collaborative Filtering, Content-based Filtering
",,Multimodal Learning,"Snippet Text: Some users say the videos are helpful. 21-year-old Chris Henrie uses TikTok to educate people about eating disorders. His videos — which often feature his experiences going through treatment for anorexia nervosa — have been liked more than 1.5 million times.
Related Classifications: Multimodal Learning
",,Misinformation Generation Hazard,"Snippet Text: Quinn is worried about what she calls ""pseudo-recovery"" TikToks: videos that use recovery language but actually depict people engaging in harmful behaviours.

""I completely understand that making videos or other content is a way to cope, but I’ve noticed that a lot of these recovery videos give specific weights/numbers and show 'body checks' (thigh gaps, collar bones, etc.), which can be extremely triggering to people struggling with EDs,"" Quinn wrote.
Related Classifications: Misinformation Generation Hazard
",,Unsafe Exposure or Access,"Snippet Text: Users and experts have concerns that even videos about recovering from an eating disorder can be harmful, while acknowledging that their impacts are more complex. Some people find this content helps them, others find it triggering.
Related Classifications: Unsafe Exposure or Access
",
GMF,280,False,Matchmaking,"Snippet Text: But when I checked the “No Preferences” box next to “Ethnicity” on Coffee Meets Bagel, an online dating site that sends you a match every day at noon, I didn't realize that would mean I'd receive a steady string of Asian men. In my first 11 days on the site, eight of my matches were Asian and three were South Asian.
Related Classifications: Matchmaking
",,,,,,,,"Content-based Filtering, Clustering, Regression",,,Lack of Transparency,"Snippet Text: Except I've been on the site for almost three months, and fewer than a third of my matches and I have had friends in common. So how does the algorithm find the rest of these dudes? And why was I only getting Asian guys?
Related Classifications: Lack of Transparency
",,"Overfitting, System Manipulation, Task Mismatch, Malicious Marketing","Snippet Text: But when I checked the “No Preferences” box next to “Ethnicity” on Coffee Meets Bagel, an online dating site that sends you a match every day at noon, I didn't realize that would mean I'd receive a steady string of Asian men. In my first 11 days on the site, eight of my matches were Asian and three were South Asian.
Related Classifications: Overfitting
, Snippet Text: You see, if most other white women limit their matches to white men, there are very few white female matches for the many Asian men who include white women in their preferences. And since I was one of those few white women who allowed Asian men, I got tons of Asian men as matches. In order to have the possibility of connecting with anyone else, I had to give up my ability to see any Asians, just as the rest of these narrow-minded “players” had. 
Related Classifications: System Manipulation, Task Mismatch, Malicious Marketing
",
GMF,281,False,Content Recommendation,"Snippet Text: According to a report by The Telegraph on Monday, YouTube has been recommending videos that contain graphic images of self-harm to users as young as 13 years old.
Related Classifications: Content Recommendation
",,,,,"Content-based Filtering, Collaborative Filtering","Snippet Text: According to a report by The Telegraph on Monday, YouTube has been recommending videos that contain graphic images of self-harm to users as young as 13 years old.
Related Classifications: Content-based Filtering, Collaborative Filtering
",,,,,"Unsafe Exposure or Access, Task Mismatch, Incomplete Data Attribute Capture","Snippet Text: Writing in The Daily Telegraph last week, Digital Secretary Jeremy Wright said: “Social media companies clearly need to do more to ensure they are not promoting harmful content to vulnerable people.”
Related Classifications: Unsafe Exposure or Access
, Snippet Text: Writing in The Daily Telegraph last week, Digital Secretary Jeremy Wright said: “Social media companies clearly need to do more to ensure they are not promoting harmful content to vulnerable people.”
Related Classifications: Task Mismatch, Incomplete Data Attribute Capture
Snippet Discussion: Recommender system doesn't really consider such preferences.
, Snippet Text: According to a report by The Telegraph on Monday, YouTube has been recommending videos that contain graphic images of self-harm to users as young as 13 years old.
Related Classifications: Unsafe Exposure or Access, Incomplete Data Attribute Capture, Task Mismatch
",,,,
GMF,282,False,NSFW Content Detection,"Snippet Text: Canada’s most sexually provocative onions were pulled down from Facebook after the social media giant told a produce company that its images went against advertising guidelines, the CBC reported.

Now, Facebook has admitted the ad was picked up by an errant algorithm, and will be restored.

The offending ad — for Gaze Seed Company’s Walla Walla onion seeds — shows an image of a handful of onions in a wicker basket. According to a Facebook notice sent to the company in recent days, the onions were positioned in a “sexually suggestive manner.”
Related Classifications: NSFW Content Detection
",,,,,Image Classification,"Snippet Text: Canada’s most sexually provocative onions were pulled down from Facebook after the social media giant told a produce company that its images went against advertising guidelines, the CBC reported.

Now, Facebook has admitted the ad was picked up by an errant algorithm, and will be restored.

The offending ad — for Gaze Seed Company’s Walla Walla onion seeds — shows an image of a handful of onions in a wicker basket. According to a Facebook notice sent to the company in recent days, the onions were positioned in a “sexually suggestive manner.”
Related Classifications: Image Classification
",,,,,"Overfitting, Inadequate Data Augmentation, Limited Dataset","Snippet Text: Canada’s most sexually provocative onions were pulled down from Facebook after the social media giant told a produce company that its images went against advertising guidelines, the CBC reported.

Now, Facebook has admitted the ad was picked up by an errant algorithm, and will be restored.

The offending ad — for Gaze Seed Company’s Walla Walla onion seeds — shows an image of a handful of onions in a wicker basket. According to a Facebook notice sent to the company in recent days, the onions were positioned in a “sexually suggestive manner.”
Related Classifications: Overfitting, Inadequate Data Augmentation, Limited Dataset
",,,,
GMF,283,False,"Automated Content Curation, Hate Speech Detection","Snippet Text: Facebook has apologized for taking down a post containing excerpts of the Declaration of Independence, saying that it was mistakenly flagged as hate speech.
Related Classifications: Automated Content Curation, Hate Speech Detection
",,,,,Classification,"Snippet Text: Facebook has apologized for taking down a post containing excerpts of the Declaration of Independence, saying that it was mistakenly flagged as hate speech.
Related Classifications: Classification
",,,,,"Overfitting, Limited Dataset","Snippet Text: Facebook has apologized for taking down a post containing excerpts of the Declaration of Independence, saying that it was mistakenly flagged as hate speech.
Related Classifications: Overfitting, Limited Dataset
",,,,
GMF,284,False,"NSFW Content Detection, Automated Content Curation","Snippet Text: Facebook has blocked an Australian auction house from advertising an acclaimed artist's painting depicting nude figures.
Related Classifications: NSFW Content Detection
",,,,,Image Classification,"Snippet Text: Facebook has blocked an Australian auction house from advertising an acclaimed artist's painting depicting nude figures.
Related Classifications: Image Classification
",,,,,"Overfitting, Limited Dataset, Inadequate Data Augmentation","Snippet Text: Facebook has blocked an Australian auction house from advertising an acclaimed artist's painting depicting nude figures.
Related Classifications: Overfitting, Limited Dataset, Inadequate Data Augmentation
",,,,
GMF,285,False,Translation,"Snippet Text: For example, an Incident Database contributor recently shared this image from Google's camera-based translation feature on Google Lens,
Two images are presented side by side of a book. The image on the left is unmodified, while the image on the right translates the book's cover from Korean to English. The title of the book has been mistranslated to ""dick sucker"".
Related Classifications: Translation
",,,,,"Language Modeling, Optical Character Recognition, Visual Object Detection","Snippet Text: For example, an Incident Database contributor recently shared this image from Google's camera-based translation feature on Google Lens,
Two images are presented side by side of a book. The image on the left is unmodified, while the image on the right translates the book's cover from Korean to English. The title of the book has been mistranslated to ""dick sucker"".
Related Classifications: Language Modeling, Optical Character Recognition, Visual Object Detection
",,Recurrent Neural Network,"Snippet Text: For example, an Incident Database contributor recently shared this image from Google's camera-based translation feature on Google Lens,
Two images are presented side by side of a book. The image on the left is unmodified, while the image on the right translates the book's cover from Korean to English. The title of the book has been mistranslated to ""dick sucker"".
Related Classifications: Recurrent Neural Network
",,"Inadequate Output Filtering, Context Misidentification","Snippet Text: For example, an Incident Database contributor recently shared this image from Google's camera-based translation feature on Google Lens,
Two images are presented side by side of a book. The image on the left is unmodified, while the image on the right translates the book's cover from Korean to English. The title of the book has been mistranslated to ""dick sucker"".
Related Classifications: Inadequate Output Filtering, Context Misidentification
",,,,
GMF,286,False,Content Recommendation,"Snippet Text: “TikTok needs to be held accountable for pushing deadly content to these two young girls,” said Matthew P Bergman, founding attorney of SMVLC. “TikTok has invested billions of dollars to intentionally design products that push dangerous content that it knows is dangerous and can result in the deaths of its users.”
Related Classifications: Content Recommendation
",,,,,Collaborative Filtering,"Snippet Text: “TikTok needs to be held accountable for pushing deadly content to these two young girls,” said Matthew P Bergman, founding attorney of SMVLC. “TikTok has invested billions of dollars to intentionally design products that push dangerous content that it knows is dangerous and can result in the deaths of its users.”
Related Classifications: Collaborative Filtering
",,,,,"Unsafe Exposure or Access, Lack of Safety Protocols","Snippet Text: “TikTok needs to be held accountable for pushing deadly content to these two young girls,” said Matthew P Bergman, founding attorney of SMVLC. “TikTok has invested billions of dollars to intentionally design products that push dangerous content that it knows is dangerous and can result in the deaths of its users.”
Related Classifications: Unsafe Exposure or Access, Lack of Safety Protocols
",,,,
GMF,288,False,Face Recognition,"Snippet Text: Investigators relied on facial recognition software that has since been banned in New Jersey to identify Parks as a suspect in crimes that occurred the afternoon of Jan. 26, 2019, at the Hampton Inn hotel on Route 9 North in Woodbridge.
Related Classifications: Face Recognition
",,,,,"Visual Object Detection, Image Segmentation, Face Detection","Snippet Text: The day after the hotel crimes, investigators in New York and New Jersey notified Woodbridge police they had a “high profile” match to the photo, after running it through a facial recognition system that compares the image to photos of other suspects, such as ex-convicts, whose pictures were on file in police and FBI databases.
Related Classifications: Visual Object Detection, Image Segmentation, Face Detection
",,,,,Generalization Failure,"Snippet Text: He is the third person known to be falsely arrested based on a bad facial recognition match. In all three cases, the people mistakenly identified by the technology have been Black men.
Related Classifications: Generalization Failure
",,"Unauthorized Data, Distributional Bias, Dataset Imbalance, Misuse","Snippet Text: The software, which was created by Clearview Al, was criticized for its heavy reliance on billions of social media photos to identify criminal suspects.
Related Classifications: Unauthorized Data
, Snippet Text: He is the third person known to be falsely arrested based on a bad facial recognition match. In all three cases, the people mistakenly identified by the technology have been Black men.
Related Classifications: Distributional Bias, Dataset Imbalance
, Snippet Text: Nathan Freed Wessler, an attorney with the American Civil Liberties Union who believes that the police should stop using face recognition technology, said the three cases demonstrated “how this technology disproportionately harms the Black community.”
Related Classifications: Distributional Bias, Dataset Imbalance
, Snippet Text: Law enforcement often defends the use of facial recognition, despite its flaws, by saying it is used only as a clue in a case and will not lead directly to an arrest. But Mr. Parks’s experience is another example of an arrest based almost solely on a suggested match by the technology.
Related Classifications: Misuse
",
GMF,289,False,Autonomous Drones,"Snippet Text: Food delivery by robots is being tested more and more in the US, including in North Texas, such as in the cities of Arlington and Frisco.
Related Classifications: Autonomous Drones
, Snippet Text: Receiving goods without contact seems like a good idea, during this COVID-19 pandemic. These food delivery robots usually only run on sidewalks, but sometimes have to cross the street. And what will happen if this robot car crashes into your car?
Related Classifications: Autonomous Drones
",,,,,"Image Segmentation, Convolutional Neural Network, Visual Object Detection","Snippet Text: 
, Snippet Text: “I found out that a 'Starship' robot hit my car,” said Mok.
Snippet Discussion: Robot sensor suite description: https://www.starship.xyz/our-robots/
",,,"Snippet Text: “I asked the police officer if the robot is classified as a vehicle because they drive, you know, the sidewalk and street but he said, 'oh, I’m not sure,'” said Mok.
Related Classifications: Image Segmentation, Visual Object Detection, Recurrent Neural Network
",,Lack of Transparency,"Snippet Text: When she finally reached someone at 'Starship' she learns there is a video of the accident. They told her they would send it to her but that never happened. 

“She told me they don't want to release the video and the video is their property, so they don't have the right to be forced to release the video,” said Mok.
Related Classifications: Lack of Transparency
",,Generalization Failure,"Snippet Text: “I found out that a 'Starship' robot hit my car,” said Mok.
Related Classifications: Generalization Failure
",
GMF,290,False,Substance Detection,"Snippet Text: A safe water advocacy group is concerned for the health of Toronto beachgoers after the city’s new water quality monitoring system appears to have repeatedly allowed contaminated beaches to remain open.
Related Classifications: Substance Detection
",,,,,"Diverse Data, Classification","Snippet Text: According to TPH, the new system uses a series of calculations based on historical data and metrics such as rainfall, temperature and wind direction. It also pulls real-time meteorological and hydrological data.
Related Classifications: Diverse Data
, Snippet Text: This summer, the city quietly adopted artificial intelligence predictive modelling (AIPM) to forecast water quality at two key beaches. Soon after, questions arose around its accuracy after waters that tested high for E. coli using traditional means were marked safe by the new system dozens of times.
Related Classifications: Classification
",,Regression,"Snippet Text: This summer, the city quietly adopted artificial intelligence predictive modelling (AIPM) to forecast water quality at two key beaches. Soon after, questions arose around its accuracy after waters that tested high for E. coli using traditional means were marked safe by the new system dozens of times.
Related Classifications: Regression
",,"Generalization Failure, Untested Deployment","Snippet Text: This summer, the city quietly adopted artificial intelligence predictive modelling (AIPM) to forecast water quality at two key beaches. Soon after, questions arose around its accuracy after waters that tested high for E. coli using traditional means were marked safe by the new system dozens of times.
Related Classifications: Generalization Failure
, Snippet Text: Fleisher said the apparent low-accuracy she observed is a “clear example of why predictive modelling — especially in its pilot phase — should be used as part of a risk management system for beaches, not as the only source.”
Related Classifications: Generalization Failure, Untested Deployment
",,Misconfigured Threshold,"Snippet Text: This summer, the city quietly adopted artificial intelligence predictive modelling (AIPM) to forecast water quality at two key beaches. Soon after, questions arose around its accuracy after waters that tested high for E. coli using traditional means were marked safe by the new system dozens of times.
Related Classifications: Misconfigured Threshold
",
GMF,291,False,Autonomous Driving,"Snippet Text: The California Department of Motor Vehicles has accused Tesla of false advertising in its promotion of the company’s signature Autopilot and Full Self-Driving technologies.
Related Classifications: Autonomous Driving
",,,,,"Recurrent Neural Network, Image Segmentation, Visual Object Detection, Convolutional Neural Network","Snippet Text: The California Department of Motor Vehicles has accused Tesla of false advertising in its promotion of the company’s signature Autopilot and Full Self-Driving technologies.
Related Classifications: Recurrent Neural Network, Image Segmentation, Visual Object Detection, Convolutional Neural Network
",,,,,"Malicious Marketing, Lack of Transparency","Snippet Text: The California Department of Motor Vehicles has accused Tesla of false advertising in its promotion of the company’s signature Autopilot and Full Self-Driving technologies.
, Snippet Text: The DMV complaints point to the very names of the technologies, as well as other “misleading” language such as the following, which appears on the Tesla website’s Autopilot page:

“All you will need to do is get in and tell your car where to go. If you don’t say anything, your car will look at your calendar and take you there as the assumed destination. Your Tesla will figure out the optimal route, navigating urban streets, complex intersections and freeways.”
Related Classifications: Malicious Marketing
, Snippet Text: It’s unclear how many crashes involve Full Self-Driving technology, and whether any of those crashes have led to death or injury. Tesla’s onboard computers are capable of communicating that information over the air to Tesla, but the company doesn’t share those data with the public.
Related Classifications: Lack of Transparency
, Snippet Text: Tesla’s response to the DMV complaints, if any, has not yet been made public. Tesla has no media relations office. Musk did not respond to an invitation to tell Tesla’s side of the story.
Related Classifications: Lack of Transparency
",,,,
GMF,292,False,Autonomous Driving,"Snippet Text: Apple’s self-driving cars had trouble navigating streets, frequently bumped into curbs and veered out of lanes in the middle of intersections during test drives near the company’s Silicon Valley headquarters, according to a report.
Related Classifications: Autonomous Driving
",,,,,"Visual Object Detection, Image Segmentation, Recurrent Neural Network, Convolutional Neural Network","Snippet Text: Apple’s self-driving cars had trouble navigating streets, frequently bumped into curbs and veered out of lanes in the middle of intersections during test drives near the company’s Silicon Valley headquarters, according to a report.
Related Classifications: Visual Object Detection, Image Segmentation, Recurrent Neural Network, Convolutional Neural Network
",,,,,Generalization Failure,"Snippet Text: But engineers at the iPhone maker were dismayed when the test vehicles struggled to conduct basic navigation maneuvers on city streets near the company’s Cupertino, Calif., headquarters.
Related Classifications: Generalization Failure
, Snippet Text: A source told The Information that a local jogger was nearly hit by one of Project Titan’s cars as the runner was crossing the street. The car apparently did not recognize that the jogger had the right of way.
Related Classifications: Generalization Failure
, Snippet Text: The technology driving autonomous vehicles is still not ready to safely account for other environmental factors such as other cars, pedestrians, and bikes. This has forced companies to push back the anticipated rollouts of self-driving vehicles.
Related Classifications: Generalization Failure
",,"Limited Dataset, Lack of Safety Protocols, Lack of Interruptability","Snippet Text: A source told The Information that a local jogger was nearly hit by one of Project Titan’s cars as the runner was crossing the street. The car apparently did not recognize that the jogger had the right of way.
Related Classifications: Limited Dataset
, Snippet Text: Apple’s self-driving car would differ from those being developed by rivals such as Google-backed Waymo and General Motors’ Cruise since it would have no steering wheel and pedals, with interiors designed around hands-off driving.
Related Classifications: Lack of Safety Protocols, Lack of Interruptability
, Snippet Text: The technology driving autonomous vehicles is still not ready to safely account for other environmental factors such as other cars, pedestrians, and bikes. This has forced companies to push back the anticipated rollouts of self-driving vehicles.
Related Classifications: Limited Dataset
",
GMF,293,False,Autonomous Driving,"Snippet Text: On June 3, 2022, in San Francisco, California, a Cruise self-driving vehicle that was driving itself autonomously was involved in an accident with a Toyota Prius. People in both cars sustained injuries as a result of the crash.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network","Snippet Text: On June 3, 2022, in San Francisco, California, a Cruise self-driving vehicle that was driving itself autonomously was involved in an accident with a Toyota Prius. People in both cars sustained injuries as a result of the crash.
Related Classifications: Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network
",,,,,"Lack of Transparency, Lack of Explainability, Generalization Failure","Snippet Text: Cruise went on to say that the driver of the Toyota was speeding. Moreover, the company's report claims the Prius was in a lane that required a right turn, though the human driver proceeded straight through the intersection. A spokesperson for the San Francisco Police Department couldn't confirm or deny Cruise's interpretation of the story since an incident report was either unavailable or not completed in the first place.
Related Classifications: Lack of Transparency, Lack of Explainability
, Snippet Text: Cruise shared that the car was in autonomous mode when the crash occurred. However, it didn't clarify if the person or people in the car were safety drivers, employees, passengers, or a combination. We do know that people in the car were not likely charged for the ride since the paid service hadn't yet started prior to the crash.
Related Classifications: Lack of Transparency, Lack of Explainability
, Snippet Text: Cruise said in its NHTSA filing that the software update improves its self-driving software’s predictions, especially in situations like the one that led to the crash. The company said it has determined that if the vehicle involved in the June 3 incident had been running the current software, no crash would have occurred.
Related Classifications: Generalization Failure
",,Human Error,"Snippet Text: Cruise went on to say that the driver of the Toyota was speeding. Moreover, the company's report claims the Prius was in a lane that required a right turn, though the human driver proceeded straight through the intersection. A spokesperson for the San Francisco Police Department couldn't confirm or deny Cruise's interpretation of the story since an incident report was either unavailable or not completed in the first place.
Related Classifications: Human Error
",
GMF,294,False,Autonomous Driving,"Snippet Text: The Tesla Model 3 unofficial road trip in Europe has ended with a crash in Macedonia Greece.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection","Snippet Text: The Tesla Model 3 unofficial road trip in Europe has ended with a crash in Macedonia Greece.
Related Classifications: Convolutional Neural Network, Recurrent Neural Network, Image Segmentation, Visual Object Detection
",,,,,"Generalization Failure, Inadequate Provenance","Snippet Text: While travelling at 75mph (the speed limit), reports suggest that the system became confused by an exit slip-road, jerking away from the straight-ahead lane unexpectedly – and too late for the distracted Mr Xue to prevent it hitting the solid lane divider.
Related Classifications: Generalization Failure
, Snippet Text: Tesla’s response, a statement sent to Electrek, said what we’ve also been trying to explain. Tesla said:

    “Although we haven’t been able to retrieve any data from the vehicle given that the accident occurred in an unsupported area, Tesla has always been clear that the driver must remain responsible for the car at all times when using Autopilot.”

Related Classifications: Inadequate Provenance
",,Misuse,"Snippet Text: While we appreciate You You Xue’s effort to spread the word about Model 3, he was informed that Tesla does not yet have a presence in Eastern Europe and that there is no connectivity or service available for vehicles there. In addition, Model 3 has not yet been approved and homologated for driving outside of the U.S. and Canada. Although we haven’t been able to retrieve any data from the vehicle given that the accident occurred in an unsupported area, Tesla has always been clear that the driver must remain responsible for the car at all times when using Autopilot. We’re sorry to hear that this accident occurred, and we’re glad You You is safe.
Related Classifications: Misuse
",
GMF,295,False,Face Recognition,"Snippet Text: An 18-year-old from New York is suing Apple for $1 billion, claiming that a facial recognition system at its stores falsely connected him to a series of thefts.
Related Classifications: Face Recognition
",,,,,"Visual Object Detection, Image Segmentation, Face Detection","Snippet Text: An 18-year-old from New York is suing Apple for $1 billion, claiming that a facial recognition system at its stores falsely connected him to a series of thefts.
Related Classifications: Visual Object Detection, Image Segmentation, Face Detection
",,,,,"Inadequate Verification, Task Mismatch","Snippet Text: Bah said his interim driver learner’s permit, which does not have a photo, had been either lost or stolen. His attorney told The Washington Post that the permit may have been presented as identification at an Apple store, erroneously connecting Bah’s name with the thief’s face in the company’s security system. That means every time the perpetrator walked into an Apple store, his face would register as Bah, the attorney said.
Related Classifications: Inadequate Verification, Task Mismatch
Snippet Discussion: The system doesn't appear to have compared the actual face of the thief with the face in the ID.
",,,,
GMF,296,False,Content Recommendation,"Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
Related Classifications: Content Recommendation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
Related Classifications: Collaborative Filtering, Content-based Filtering
",,Distributional Learning,"Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
Related Classifications: Distributional Learning
",,"Algorithmic Bias, Lack of Explainability, Lack of Transparency","Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
, Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
Related Classifications: Algorithmic Bias
, Snippet Text: The research found that in six out of seven countries, apart from Germany, tweets from rightwing politicians received more amplification from the algorithm than those from the left; right-leaning news organisations were more amplified than those on the left; and generally politicians’ tweets were more amplified by an algorithmic timeline than by the chronological timeline.
Related Classifications: Algorithmic Bias
, Snippet Text: Twitter said it wasn’t clear why its Home timeline produced these results and indicated that it may now need to change its algorithm. A blog post by Rumman Chowdhury, Twitter’s director of software engineering, and Luca Belli, a Twitter researcher, said the findings could be “problematic” and that more study needed to be done.
Related Classifications: Lack of Explainability
, Snippet Text: “Algorithmic amplification is problematic if there is preferential treatment as a function of how the algorithm is constructed versus the interactions people have with it. Further root cause analysis is required in order to determine what, if any, changes are required to reduce adverse impacts by our Home timeline algorithm,” the post said.
Related Classifications: Lack of Explainability
, Snippet Text: Twitter added that it was preparing to make internal data available to external sources on a regular basis. The company said its machine-learning ethics, transparency and accountability team was finalising plans in a way that would protect user privacy.
Related Classifications: Lack of Transparency
",,Distributional Bias,"Snippet Text: The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims.
, Snippet Text: The research found that in six out of seven countries, apart from Germany, tweets from rightwing politicians received more amplification from the algorithm than those from the left; right-leaning news organisations were more amplified than those on the left; and generally politicians’ tweets were more amplified by an algorithmic timeline than by the chronological timeline.
Related Classifications: Distributional Bias
",
GMF,297,False,Autonomous Driving,"Snippet Text: Smart Columbus has parked its two self-driving shuttles in Linden after one unexpectedly stopped in the middle of a route and a woman fell from her seat onto the floor.
Related Classifications: Autonomous Driving
",,,,,"Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network","Snippet Text: Smart Columbus has parked its two self-driving shuttles in Linden after one unexpectedly stopped in the middle of a route and a woman fell from her seat onto the floor.
Related Classifications: Convolutional Neural Network, Image Segmentation, Visual Object Detection, Recurrent Neural Network
",,,,,"Generalization Failure, Lack of Safety Protocols","Snippet Text: Recently an EasyMile self-driving shuttle operating in Columbus, Ohio came to a sudden stop from 7mph, apparently for no visible reason. 
Related Classifications: Generalization Failure
, Snippet Text: Almost no transit uses seat belts. That is partly because transit vehicles do have lower accident rates than private cars, but the rates are not zero. The main reason is tradition and convenience.
Related Classifications: Lack of Safety Protocols
",,,,
GMF,299,False,"Decensoring, Deepfake Video Generation","Snippet Text: Japanese police on Monday arrested a 43-year-old man for using artificial intelligence to effectively unblur pixelated porn videos, in the first criminal case in the country involving the exploitative use of the powerful technology.
Related Classifications: Decensoring, Deepfake Video Generation
",,,,,"Distributional Learning, Convolutional Neural Network, Recurrent Neural Network","Snippet Text: But instead of changing faces, Nakamoto used machine learning software to reconstruct the blurred parts of the video based on a large set of uncensored nudes and sold the content online. Penises and vaginas are pixelated in Japanese porn because an obscenity law forbids the explicit depictions of genitalia.
Related Classifications: Distributional Learning, Convolutional Neural Network, Recurrent Neural Network
",,Generative Adversarial Network,"Snippet Text: But instead of changing faces, Nakamoto used machine learning software to reconstruct the blurred parts of the video based on a large set of uncensored nudes and sold the content online. Penises and vaginas are pixelated in Japanese porn because an obscenity law forbids the explicit depictions of genitalia.
Related Classifications: Generative Adversarial Network
",,"Privacy Concerns, Unsafe Exposure or Access, Misinformation Generation Hazard","Snippet Text: But instead of changing faces, Nakamoto used machine learning software to reconstruct the blurred parts of the video based on a large set of uncensored nudes and sold the content online. Penises and vaginas are pixelated in Japanese porn because an obscenity law forbids the explicit depictions of genitalia.
Related Classifications: Privacy Concerns, Unsafe Exposure or Access, Misinformation Generation Hazard
",,,,
GMF,300,False,"Content Recommendation, Automated Content Curation","Snippet Text: An Observer investigation has revealed how TikTok is promoting misogynistic content to young people despite claiming to ban it.
Related Classifications: Content Recommendation, Automated Content Curation
",,,,,"Collaborative Filtering, Content-based Filtering","Snippet Text: An Observer investigation has revealed how TikTok is promoting misogynistic content to young people despite claiming to ban it.
Related Classifications: Collaborative Filtering, Content-based Filtering
",,,,,"Inadequate Output Filtering, Unsafe Exposure or Access","Snippet Text: Without “liking” or searching for any content proactively, the suggestions included videos of Andrew Tate, including one from a copycat account using Tate’s name and picture captioned the “harsh reality of men”, which appeared to blame feminism for making men miserable, adding that the “majority of men have no money, no power, no sex from their wife”, and that their lives “suck”.
Related Classifications: Inadequate Output Filtering, Unsafe Exposure or Access
",,Distributional Bias,"Snippet Text: Without “liking” or searching for any content proactively, the suggestions included videos of Andrew Tate, including one from a copycat account using Tate’s name and picture captioned the “harsh reality of men”, which appeared to blame feminism for making men miserable, adding that the “majority of men have no money, no power, no sex from their wife”, and that their lives “suck”.
Related Classifications: Distributional Bias
",
