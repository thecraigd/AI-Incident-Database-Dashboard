Namespace,Incident ID,Published,Incident Number,Annotator,Annotation Status,Peer Reviewer,Quality Control,Physical Objects,Entertainment Industry,"Report, Test, or Study of data",Deployed,Producer Test in Controlled Conditions,Producer Test in Operational Conditions,User Test in Controlled Conditions,User Test in Operational Conditions,Harm Domain,Tangible Harm,AI System,Clear link to technology,There is a potentially identifiable specific entity that experienced the harm,AI Harm Level,AI Tangible Harm Level Notes,Impact on Critical Services,Rights Violation,Involving Minor,Detrimental Content,Protected Characteristic,Harm Distribution Basis,Notes (special interest intangible harm),Special Interest Intangible Harm,AI System,Clear link to Technology,Harmed Class of Entities,Annotator’s AI special interest intangible harm assessment,Notes (AI special interest intangible harm),Date of Incident Year,Date of Incident Month,Date of Incident Day,Estimated Date,Multiple AI Interaction,Embedded,Location City,Location State/Province (two letters),Location Country (two letters),Location Region,Infrastructure Sectors,Operating Conditions,Notes (Environmental and Temporal Characteristics),Entities,Lives Lost,Injuries,Estimated Harm Quantities,Notes ( Tangible Harm Quantities Information),AI System Description,Data Inputs,Sector of Deployment,Public Sector Deployment,Autonomy Level,Notes (Information about AI System),Intentional Harm,Physical System Type,AI Task,AI tools and methods,Notes (AI Functionality and Techniques)
CSETv1,250,False,0,,--,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,208,False,0,,--,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,Europe and Northern America,,,,,0,0,False,,,,,,,,,,,,
CSETv1,204,False,0,,--,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,Eastern and South-Eastern Asia,,,,,0,0,False,,,,,,,,,,,,
CSETv1,100,True,100,003,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,no,yes,True,none,"No financial harm because the money was given back

AI was not involved in the robo-debt algorithms. Statistics and risk categories were used instead of AI.",yes,no,no,no,no,none,,yes,no,yes,True,no,,2021,March,17,True,no,no,,,FR,Europe,government facilities,, It is unclear how long welfare decisions in France have been supplemented and made by algorithms.,"Journalist , Journalist, French welfare offices, French welfare recipients",0,0,False,,"The welfare determination algorithms ""use “automated controls” as well as “a statistical model known as ‘datamining’ (sic), which automatically targets risky cases”"" to review and flag welfare recipients and potentially charge them with fines.",welfare recipient data,"administrative and support service activities, human health and social work activities",yes,Autonomy2,,No. Not intentionally designed to perform harm,,welfare determination,,Fraud detection
CSETv1,124,True,124,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"Annotator 1: 

 Black patients assigned understated risk scores by the algorithm are at risk of not being eligible for necessary health treatments that could endanger their physical health and safety. Because this system was deployed, this incident describes an AI tangible harm near-miss.

Annotator 2: 

 The algorithm was less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs.  Its bias effectively reduced the proportion of black patients receiving extra help by more than half, from almost 50 percent to less than 20 percent. Those missing out on extra care potentially faced a greater chance of emergency room visits and hospital stays.",yes,yes,no,no,yes,"race, financial means",,yes,yes,yes,True,yes,,2019, 10,25,True,no,no, ,,US,North America,healthcare and public health,,This date refers to the study's publication that concluded that the algorithm was less likely to refer black people than white people who were equally sick to programs that aim[ed] to improve care for patients with complex medical needs.,"Black patients, Unknown US hospital, Black patients, Black patients, Obermeyer et al. , Optum, Healthcare allocation algorithm",0,0,True,"Health risk score algorithms affected the care received by millions of patients throughout the US. 

Black people did experience tangible physical/health harm, but we could not find estimates of the amount.","The algorithm is used in the context of healthcare programs that provide additional resources and closer medical supervision for people with multiple health problems. It assigned risk scores to patients based on total health-care costs accrued in one year because higher health-care costs are generally associated with more significant health needs. Based on their risk scores, patients are referred to programs that provide more personalized care.","patient data, treatment cost",human health and social work activities,maybe,unclear,"9.4. - It is unclear whether the deploying hospitals were public or private.

9.5 The algorithm operates in a mix of full autonomy (level 1) and human-in-the-loop (level 3): Patients with risk scores above the 97th percentile are automatically identified for enrollment in the program. Those above the 55th percentile are referred to their primary care physician, who is provided with contextual data about the patients and asked to consider whether they would benefit from program enrollment.",No. Not intentionally designed to perform harm,,"assign risk , predict healthcare needs",prediction,
CSETv1,350,False,0,,--,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,--,,,,,0,0,False,,,,,no,,,,,,,
CSETv1,351,False,0,,--,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,14,True,14,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,"Annotator 2: 

 No tangible harm",no,no,no,no,yes,"religion, sexual orientation or gender identity, sex, race, nation of origin, citizenship, immigrant status",Disproportionately gave phrases related to protected characteristics negative scores.,yes,yes,yes,True,yes,Disproportionately gave phrases related to protected characteristics negative scores.,2017,10,25,False,no,no,,,,Global,,,,"Google, Google Cloud Natural Language API, Affected Groups, Motherboard, Google, Cloud Natural Language API, Cloud Natural Language API Users ",0,0,False,,"Sentimental Analysis model. Given a phrase, the model outputs a score -1 to 1 which determines the sentiment of the phrase, negative or positive.",text,information and communication,no,Autonomy3,,No. Not intentionally designed to perform harm,,"natural language processing, sentiment analysis, Sentiment Analysis",Natural Language Processesing,
CSETv1,16,True,16,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,race, The google image tagging feature of the Google Photos app mislabeled black people as gorillas.,yes,yes,yes,True,yes,,2015,06,29,False,no,no,,,US,North America,,,,"Google, Google Photos, Jacky Alcine, Jacky Alcine's friend, Black people",0,0,False,,Google Photos' system groups and labels similar photos into categories,"images, photos",information and communication,no,Autonomy1," The Google Photos system creates labels automatically without human interaction or intervention, although users can interact with the categorizations after they have been created.",No. Not intentionally designed to perform harm,,"image classification, image categorization, Image Tagging, object recognition",computer vision,
CSETv1,35,True,35,,6. Complete and final,002,False,maybe,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,maybe,True,none,"3.2 - It is unclear whether the system responsible for Diallo's termination included AI or machine learning. It appears simple and rule-based: if someone is flagged as an ex-employee, deactivate their keycard, then shut down their JIRA account, then log them out of work devices, etc. This does not require AI or ML tools.
3.3 - Events were initially triggered by accident/neglect of Diallo's supervisor, the system just executed its program as intended. ",no,no,no,no,no,none,,no,no,maybe,True,no,,2017,03,,True,no,no,Los Angeles,CA,US,North America,,,,"Ibrahim Diallo, Employee management system, Ibrahim Diallo's employer",0,0,False,,"The employee management system that terminated Ibrahim Diallo's contract was built to complete a series of steps to handle ex-employees, including disabling their key card, notifying security, and disabling various accounts (Windows, Jira, etc.).","employee profile, employee status",administrative and support service activities,no,Autonomy1,"In this case, the company was unable to override the system for at least 3 weeks.",No. Not intentionally designed to perform harm,,,,The system was not AI.
CSETv1,24,True,24,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none," Robot crushed worker at Volkswagen factory. However, no AI linked to the robot. Volkswagen determine this death was due to human error.",no,no,no,no,no,none,,no,no,yes,True,no,,2015,06,29,False,no,yes,Baunatal,,DE,Europe,,,,"Volkswagen, 22-year old Volkswagen contractor, Volkswagen factory robot",1,0,False,,Stationary robot intended to move and manipulate auto parts,,manufacturing,no,Autonomy1,Not an AI system,No. Not intentionally designed to perform harm,Manufacturing Robot ,,,
CSETv1,41,True,41,,6. Complete and final,002,False,no,no,yes,no,yes,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,no,none,4.4 - Detrimental content was involved. but no harm occurred. This was a test to show that biased data sets will produced skewed results in AI. The AI simply generated odd/violent captions in a controlled research environment. Never deployed. ,no,yes,yes,False,no,"5.5 - Norman was trained on detrimental content. However, this was intentional in the MIT researchers' experiment. No entity experienced harm because of it.",2018,06,,True,no,no,Cambridge,MA,US,North America,,,,"Norman, MIT Media Lab researchers",0,0,False,,"Norman is an AI system trained on the captions of violent and graphic content on the  Reddit thread r/watchpeopledie. Its purpose was to provide answers to Rorschach test prompts that would demonstrate the danger of training AI on biased data sets. It did, in fact, provide more gruesome answers to prompts than other AI.","Text, Captions, words, image captions, video captions","professional, scientific and technical activities",no,Autonomy1,"The AI responded to each prompt without intervention, but the experiment as a whole was monitored.",Yes. Intentionally designed to perform harm and did create intended harm,,"Generate Captions, image identification, image interpretation",Natural Language Processesing,
CSETv1,1,True,1,,6. Complete and final,002,False,no,yes,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,yes,yes,no,none,,yes,yes,yes,True,yes,,2016,,,True,no,no,,,US,North America,,,,"Google, YouTube Kids, Children watching Youtube Kids",0,0,False,,Content-recommendation and filtering algorithm,Youtube videos,"Arts, entertainment and recreation, information and communication",no,Autonomy1,9.5 - Content-moderation algorithms like the one involved in this incident are supposed to work without requiring human review or intervention.,No. Not intentionally designed to perform harm,,content moderation,,
CSETv1,4,True,4,,6. Complete and final,002,False,yes,no,no,no,no,yes,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2018,03,18,False,no,yes,Tempe,AZ,US,North America,,night,,"Uber, Volvo XC90 SUV autonomous vehicle, Elaine Herzberg, Rafaela Vasquez",1,0,False,,Autonomous driving system by Uber,"lidar, radar, video input, sensor data",transportation and storage,no,Autonomy2,"The car was operating in autonomous mode at the time of the incident, but with a safety operator on board who could have taken action. ",No. Not intentionally designed to perform harm,Volvo XC90 SUV,"self-driving, navigation",,
CSETv1,70,True,70,002,6. Complete and final,002,False,yes,no,no,no,no,yes,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,"Testing discovered that snow and inclement weather can interfere with the sensors used by autonomous vehicles to detect and recognize objects, traffic and road lanes, rendering them less effective or even blind. Since the system is not deployed, the vulnerability does not present a harm issue. ",no,no,no,no,no,none,,no,yes,yes,True,no,,2016,02,10,False,no,yes,Jokkmokk,,SE,Europe,,inclement weather and snow,6.4 - The date corresponds to the publication of the article.,"Volvo Cars, Volvo self-driving XC90 sport-utility vehicle, Google, Ford Motor Co.",0,0,False,,AI systems are designed to integrate with vehicles to autonomously navigate passengers in operational road environments.,"cameras, radar, lidar, sensor input",transportation and storage,no,Autonomy1,automakers face in developing self-driving cars that can operate effectively in snowy and winter weather conditions. ,No. Not intentionally designed to perform harm,"Vehicles (Volvo, Lexus, Ford, Google, etc.)","autonomous driving, navigation, transportation",,
CSETv1,2,True,2,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,,no,no,no,no,no,none,,no,no,yes,True,no,,2018,12,05,False,no,yes,Robbinsville,NJ,US,North America,,,,"Amazon, Amazon robot, Amazon fulfillment center workers",0,54,False,"24 workers were hospitalized out of the 54 total injured, but harm not caused by AI ","The robot was a warehouse assistance robot, but the technology did not include AI.",,wholesale and retail trade,no,unclear,,No. Not intentionally designed to perform harm,,,,not AI
CSETv1,3,True,3,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,maybe,yes,True,unclear,It is unclear whether the MCAS system relied on machine learning or other contemporary AI methods instead of sophisticated rules.,no,no,no,no,no,none,,no,maybe,yes,True,no,,2018,10,29,False,no,yes,Java Sea,,ID,Asia,transportation,,,"Boeing, Boeing 737 MAX Airplane, Crew on Lion Air Flight 610, Passengers on Lion Air Flight 610 , MCAS (maneuvering characteristics augmentation system), Lion Air",189,0,False,,Maneuvering Characteristics Augmentation System (MCAS) is  a computerized system Boeing installed on its latest generation of 737 airplanes to prevent the plane’s nose from getting too high.,sensor data,transportation and storage,no,Autonomy1,"While pilots were supposed to be able to override MCAS' actions, they weren't during the incident in question.",No. Not intentionally designed to perform harm,Boeing 737 Max airplane,,,not AI
CSETv1,6,True,6,,6. Complete and final,002,False,no,maybe,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,yes,"race, religion, sex, ideology, nation of origin, citizenship, immigrant status","4.6 - Tay's tweets included racist and misogynist content, far-right ideology, and harmful content against certain religions, etc.",yes,yes,yes,True,yes,,2016,03,23,False,no,no,,,,Global,,,,"TayBot, Microsoft, Groups with protected characteristics, Twitter users",0,0,False,,Microsoft created a chat bot named Tay with the intent of mimicking and engaging with 18-24 year old social media users.,"text, images, personal information",information and communication,no,Autonomy1,"9.2 - Limited personal information Tay could have access to included: nickname, gender, favorite food, zip code, and relationship status. Users could opt in to giving Tay access to that information.",No. Not intentionally designed to perform harm,,chat bot,human language technology,
CSETv1,9,True,9,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,no,yes,True,none,3.5 - the value-added measurement/modeling is not AI - it is a statistical model,no,no,no,no,no,none,,no,no,yes,True,no,,2012,,,True,no,no,New York,NY,US,North America,,,,"Teachers in New York, Value-added model, New York City Department of Education",0,0,False,,The value-added model system predicts future test scores based on current measurements and then rates teachers based on their ability to bring students up to those predictions - it also uses test scores the teachers don't directly affect.,test scores,Education,yes,Autonomy1,,No. Not intentionally designed to perform harm,,,,not AI.
CSETv1,10,True,10,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2014,,,True,no,no,,,US,North America,,,,"Starbucks, Kronos, Kronos scheduling algorithm, Starbucks employees, Kylei Weisse, Starbucks employees, Jannette Navarro",0,0,False,,The Kronos scheduling algorithm is designed to optimize the productivity of stores like Starbucks by scheduling workers inconsistently throughout and across weeks based on predicted store traffic.,"schedules, worker profiles, store traffic",accommodation and food service activities,no,Autonomy2,,No. Not intentionally designed to perform harm,,"scheduling, productivity optimization, predict store traffic",,
CSETv1,52,True,52,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,69,True,69,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,Tangible Harm occurred. No AI in system.. ,no,no,no,no,no,none,,no,no,yes,True,no,,2015,08,12,False,no,yes,Manesar,Haryana,IN,Asia,,,,"Welding Robot, SKH Metals, Ramji Lal",1,0,False,"There was harm (one life lost), but there is no indication that the manufacturing robot in question was built on machine-learning technology. ",Welding robot,,manufacturing,no,Autonomy1,Not AI. ,No. Not intentionally designed to perform harm,Manufacturing Robot,,,
CSETv1,68,True,68,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2017,07,17,False,no,yes,Washington,DC,US,North America,,,,"Knightscope, Knightscope K5 security robot \""Steve\, Washington Harbour office and retail complex",0,0,False,,Knightscope K5 is an autonomous security robot,"audio, video, air quality, thermal imaging, lidar, radar",administrative and support service activities,no,Autonomy1,9.3: The robot was deployed as a private security service which falls under 'support service activities'. ,No. Not intentionally designed to perform harm,"rocket/egg shaped, 300 pound, 5 ft. tall security robot","security, patrolling, surveillance",unclear,
CSETv1,17,True,17,,6. Complete and final,002,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2017,11,03,True,no,no,,,,Global,,,,"Google, Gmail, Gmail SmartReply, Gmail users",0,0,False,,Smart Reply is a Gmail suggesting algorithm which detects which emails in an inbox need a response. It also predicts and suggests short replies for users.,text,information and communication,no,Autonomy3,AI provides suggests for users.,No. Not intentionally designed to perform harm,,Generate Replies,natural language processing,
CSETv1,18,True,18,,6. Complete and final,002,False,no,no,no,yes,no,yes,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,sex,Significant gender/sex bias in google search image results,yes,yes,yes,True,yes,,2015,04,09,True,no,no,,,US,North America,,,,"Sean Munson, Matt Kay, Cynthia Matuszek, Women, Google, Google Images",0,0,False,,Google Images returns image results once search queries and keywords are entered by scanning the web for images with related file names and using machine learning to cluster similar images together.,"keywords, search queries, images, text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"search optimization, personalized online search results, Image search",unclear,
CSETv1,34,True,34,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"There are two different events that occurred in a chain in this incident, both with different levels of harm. 3.1 reflects the circumstances of the second event. The first, where Alexa ordered a dollhouse and cookies for the 6-year-old, caused financial loss for the family (tangible harm event). But this cannot be directly and clearly linked to the technology's behavior because the order was placed purposefully and with confirmation by the child. 
The second occurrence of harm, where other peoples' Alexas reacted to the news reports about the first story and attempted to do the same, was a near-miss. Only the second can be directly linked to the AI, because users had to intervene to stop their technology from ordering a dollhouse independently of user intention based on the news reporting. Thus, only the second incident, which is a near miss, qualifies as an AI harm. ",no,no,no,no,no,none,"A minor was involved, but not ""targeted"" or ""disproportionately treated.""",no,yes,yes,True,no,,2017,01,,False,no,yes,San Diego,CA,US,North America,,,,"Brooke Neitzel, Megan Neitzel, Alexa, Amazon, CW6 News anchor, CW6 News viewers with Alexas",0,0,False,,Amazon's Alexa is a virtual assistant technology ,"voice, audio, Amazon account information","wholesale and retail trade, information and communication",no,Autonomy2,,No. Not intentionally designed to perform harm,Amazon Alexa Echo Dot speaker,virtual assistant technology,"voice recognition, audio transcription, natural language processing",
CSETv1,11,True,11,,6. Complete and final,002,False,no,no,yes,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"ProPublica found evidence of differential performance rates of the COMPAS recidivism risk prediction system across racial groups. The system was both used in sentencing (affecting judge’s decisions concerning the length of sentences) and pretrial bond hearings (affecting decisions concerning pretrial detainment and bond amounts). There is a reasonable probability that the risk scores inaccurately aggravated choices made by judges. Moreover, there is at least one instance in which a judge admitted to assigning a longer prison sentence due to the elevated risk score, which was reduced on appeal.",no,yes,no,no,yes,race,ProPublica found evidence of differential performance rates of the COMPAS recidivism risk prediction system across racial groups. The system was both used in sentencing (affecting judge’s decisions concerning the length of sentences) and pretrial bond hearings (affecting decisions concerning pretrial detainment and bond amounts). There is a reasonable probability that the risk scores wrongfully aggravated choices made by judges. ,yes,yes,yes,True,yes,,2013,,,False,no,no,,,US,North America,,,,"ProPublica, Northpointe, COMPAS, Defendants assessed by COMPAS, Defendants assessed by COMPAS, Defendants assessed by COMPAS, Defendants assessed by COMPAS, Paul Zilly, Brisha Borden, COMPAS deployers, Sade Jones",0,0,False,,The COMPAS system calculates a person's risk of recidivism based on their criminal records and responses to a 137-questions long survey about the situation and context of the crime and the person involved.,"criminal record, questionnaire responses","law enforcement, public administration",yes,Autonomy3,,No. Not intentionally designed to perform harm,,predict recidivism,,
CSETv1,22,True,22,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"3.1 - This incident can be classified as an imminent risk of tangible harm (near miss) because harm would have occurred if not for atypical intervention. Users were directed toward routes in wildfire zones. They would have suffered harm if not for their intervention, or in some cases, the intervention of police officers redirecting traffic at intersections.
3.2 and 3.3 - Waze uses AI and machine learning to predict traffic patterns and optimize routes, functions that failed to ensure user safety in this incident.",no,no,no,no,no,none,,no,yes,yes,True,no,,2017,12,,False,no,no,,CA,US,North America,,natural disaster - wildfires,,"Waze, Google, Waze, Google Maps, and Apple Maps users, Google Maps, Apple Maps, Apple",0,0,False,"There were no quantifiable tangible harms in terms of deaths or injuries, as this incident was a near-miss.",Waze uses machine learning to predict traffic patterns based on GPS data and other users' reports in order to optimize routes for the primary user. ,"user traffic reports, route specifications, road data, traffic, GPS","transportation and storage, information and communication",no,Autonomy3,,No. Not intentionally designed to perform harm,,"navigation, route optimization","shortest-path algorithm, Dijkstra Algorithm",
CSETv1,36,True,36,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"3.5 - Although no harm occurred because the mistake was identified, Risk to the affected party was imminent.",no,no,no,no,no,none,,no,yes,yes,True,no,,2018,11,21,False,no,yes,Ningbo,,CN,Asia,,,,"Jaywalking detection algorithm, Dong Mingzhu",0,0,False,,"Jaywalking detection systems, which are used by traffic police in Chinese cities, take pictures of people as they cross the road in order to detect whether their crossing is compliant with traffic laws. Sometimes, the images and names of jaywalkers are ""featured on large displays and warn people about the legal consequences of their actions.""",Video,law enforcement,yes,Autonomy1,,No. Not intentionally designed to perform harm,Camera,"jaywalking detection, facial recognition",computer vision,
CSETv1,38,True,38,,6. Complete and final,002,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2016,05,26,False,no,no,,,,Global,,,,"Frontier Development, Elite: Dangerous players, Elite: Dangerous Engineers 2.1 Updated AI",0,0,False,,"The Engineers 2.1 update to Frontier Development's game Elite Dangerous was meant to improve the user's experience by making non-playable characters and other features of the game more difficult to overcome. Instead, the development gave the AI responsible for populating and steering the behavior of NPCs the ability to combine characteristics of existing weapons to create superweapons near impossible to defeat.",game data,"Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,population of characteristics for NPCs in a video game,,
CSETv1,39,True,39,,6. Complete and final,002,False,no,yes,no,yes,yes,no,no,yes,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,"4.4 - Technically, misinformation was involved because the deepfakes portrayed Barack Obama saying things he never actually said. However, it did not result in or cause harm.",no,yes,yes,True,no,"5.1 - Though misinformation was involved and AI facilitated it, it did not cause harm. The incidents occurred in the context of tests/experiments as demonstrations of deepfake technology for the general public. There was no intention to deceive.",2017,07,,False,no,no,,,US,North America,,,,"Barack Obama, University of Washington Researchers, Jordan Peele, Buzzfeed, FakeApp, Adobe After Effects",0,0,False,,"FakeApp, which Jordan Peele and Buzzfeed used, creates AI generated material like facial reproductions and movement.
University of Washington researchers used neural networks to train the AI on 14 hours of Obama's speeches to reproduce how he talks in real life.","video, audio, speech, images","Arts, entertainment and recreation, professional, scientific and technical activities",no,Autonomy3,,No. Not intentionally designed to perform harm,,"content generation, deepfake video generation",neural networks,
CSETv1,55,True,55,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,yes,yes,no,none,Alexa produced inappropriate response/content for child user.,yes,yes,yes,True,yes,,2016,12,,False,no,yes,,,US,North America,,unclear enunciation,,"Amazon Alexa, Amazon, William",0,0,False,,Amazon Alexa is a digital assistant embedded in a bluetooth speaker ,"voice, audio, personal data, speech","Arts, entertainment and recreation, information and communication",no,Autonomy2,"9.5 - Humans can tell Alexa to stop in the middle of the response, but they did not do so quickly enough to prevent the harm from having happened.",No. Not intentionally designed to perform harm,Amazon Echo Dot Smart Speaker,virtual assistant technology,"Audio Transcription, voice recognition, natural language processing, text to speech",
CSETv1,56,True,56,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",maybe,yes,True,none,AI system or bot used to create phone cases. ,no,no,no,no,no,none,"Although the phone cases may be strange and unusual, there was no harm.",no,maybe,yes,True,no,,2017,07,08,False,no,no,,,,Global,,,,"Amazon, my-handy-design bot",0,0,False,,"It is suspected that the my-handy-design bot was scraping the web for popular images by search term, and then creating phone cases with those images to sell on Amazon. Each case is probably printed to order.","search terms, images",wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,,Design Phone Cases,,
CSETv1,23,True,23,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"3.3 and 3.4 - The incident was caused primarily by the truck driver who backed out of an alley without checking the surrounding environment. However, according to the passengers, the self-driving shuttle could have prevented the collision by reversing out of the way or honking to get the driver’s attention. Both are safe driving behaviors that a human driver would likely have taken. While the AI is not at fault per se the accident could have been avoided if it had behaved differently.",no,no,no,no,no,none,,no,yes,yes,True,no,,2017,11,08,False,no,yes,Las Vegas,NV,US,North America,,,,"self-driving eight-seater electric shuttle, Keolis, Navya, Truck, Shuttle passengers",0,0,False,,The autonomous shuttle bus was meant to traverse a 0.6 mile loop at around 25 km/h in Las Vegas.,"road data, lidar, video, odometer input, inertial measurement unit input, traffic light signals, GNSS Antennae input, GPS input",transportation and storage,no,Autonomy1,,No. Not intentionally designed to perform harm,"oval, eight-seater, autonomous, electric shuttle bus","self driving, autonomous driving, navigation, obstacle avoidance",,
CSETv1,72,True,72,002,6. Complete and final,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,A Palestinian man was arrested and questioned for several hours due to translation error on Facebook.,no,yes,no,no,no,none,,yes,yes,yes,True,yes,,2017,10,,False,no,no,Beitar Ilit,,PS,Asia,,,"CSET follows international standards on naming conventions for disputed territories. UN guidelines recommend referencing maps produced by UN geoservices, which delineate jurisdictions based on current international standards.  Israeli annexation of parts of the Palestinian West Bank are not recognized on UN maps. For this incident, which takes place in the West Bank, the location under current international standards should thus be “Palestine,"" not “Israel.”  "," Palestinian Facebook user, Palestinian Facebook user, Facebook, Facebook translation model, Israeli Police",0,0,False,,Facebook translator,"Facebook posts, Text",information and communication,no,Autonomy3,"9.5: While the translation system operates autonomously without human intervention (autonomy1), considering the chain of events that led to harm in this incident Autonomy3 is more suitable.  A human had to take action for the mistranslation to lead to harm. ",No. Not intentionally designed to perform harm,,Translation,Natural Language Processesing,"Automatic translation service offered by Facebook, which uses its own proprietary algorithms, translated the words ‘Good morning’ to “attack them” in Hebrew and “hurt them” in English."
CSETv1,73,True,73,002,6. Complete and final,001,False,yes,yes,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,no,no,True,none,"Because pokestops are disproportionately in white neighborhoods (in the US), black Pokemon Go players are going into predominantly white neighborhoods to play the game. Black players are reporting concerns about their physical safety by being present in white neighborhoods where their presence is atypical. This is a risk of tangible harm, but it is directly linked to US racial/social issues and not AI.  AI does not determine pokestops. They are based on past usage and player nomination.",no,no,no,no,yes,"race, geography",,yes,no,no,True,maybe,"The location of Poke-stops was unevenly distributed across neighborhoods, with racially diverse, Hispanic and Black areas having fewer stops than predominantly White neighborhoods (in the US). However, that distribution is not determined by an AI.",2016,08,,True,no,no,,,US,North America,,,,"Pokemon Go! players in the US, Pokemon Go!, Niantic Labs, Pokemon Go! players in the US",0,0,False,,augmented reality (AR) game Pokémon Go that blends virtual creatures from the Pokémon franchise with real-world locations,"GPS, video","information and communication, Arts, entertainment and recreation",no,Autonomy3,,No. Not intentionally designed to perform harm,,augmented reality (AR) game Pokémon Go,,
CSETv1,63,True,63,002,6. Complete and final,002,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,False,no,,2018,01,18,False,no,no,Alberta,,CA,North America,,,,"Alex Harker, Google, Google Photos, Google Photos Assistant",0,0,False,,Google Photos Assistant that creates slideshows and organizes albums based on the photos in the user's Google Photos,"photos, images","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"image recognition, image organization, image splicing, image editing",,
CSETv1,64,True,64,002,6. Complete and final,002,False,yes,yes,no,no,no,yes,no,yes,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2018,,,True,no,yes,,,GB,Europe,,operationally representative,,"Margiotta supermarket, Fabio the robot, Interaction Lab at Heriot-Watt University, BBC , Margiotta shoppers",0,0,False,,Fabio was a customer service robot intended to converse with and assist supermarket shoppers,"audio inputs, speech",wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,vaguely humanoid robot,"robotics, customer service","voice recognition, natural language response",
CSETv1,66,True,66,002,6. Complete and final,002,False,no,yes,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,"From the Chinese government's perspective, the responses given by the chatbots were detrimental content, as they disagreed with the Chinese Communist Party and expressed desires to experience democracy.",no,yes,yes,True,no,,2017,08,03,False,no,no,,,CN,Asia,,,,"QQ users, Tencent Holdings, QQ, Turing Robot, BabyQ, Microsoft, XiaoBing, Chinese Communist Party",0,0,False,,XiaoBing and BabyQ are chatbots intended to engage with and respond to user queries and messages.,"text, messages",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"Chatbot, text generation","prediction, natural language processing",
CSETv1,67,True,67,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,maybe,True,none,"Imminent risk of tangible harm occurred. If highway patrol officers had not noticed the man sleeping in the car, they might not have intervened to slow down the car by driving in front of it. However, in this case, the AI technology seemed to have prevented further damage from occurring. If the car had not had Autopilot, the car would likely not continue driving straight and the sleeping driver would have been at more risk. 
3.3 - Thus, it is unclear if the technology can be directly and clearly linked to the adverse outcome of the incident. The most direct link would be the driver who was intoxicated and fell asleep. 
3.5 - Because it is unclear whether the AI itself caused the harm rather than the driver, this incident does not qualify as AI tangible harm.",no,no,no,no,no,none,,no,yes,maybe,True,no,,2018,11,30,False,no,yes,San Francisco,CA,US,North America,,,,"California Highway Patrol officers, Alexander Samek, Tesla, Tesla Model S, Tesla Traffic Aware Cruise Control",0,0,False,,"Tesla's Autopilot is a ""suite of software, cameras and sensors intended to assist drivers and prevent accidents by taking over many aspects of driving a car."" These include steering, braking, and accelerating.","radar input, camera input, sensor data, traffic patterns",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,Tesla Model S,"Autopilot, Driving, semi-autonomous navigation",,
CSETv1,81,True,81,002,6. Complete and final,002,False,no,no,yes,no,yes,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"race, sex, financial means, age","Researchers trained a state-of-the-art classifier on commonly used datasets of chest x-rays and find it has True Positive Rate (TPR) disparities for patient sex, age, race, and insurance type (proxied in 4.6 by financial means). Classifier performance is highest for adult, white, males with private insurance (not Medicaid). Since the classifier in question was never deployed in a healthcare domain, or applied to diagnose patients there is no potential for harm. ",no,yes,yes,False,no,"This is a study about how common datasets used for medical diagnosing AI used unrepresentative datasets and the that the inadequate representation can lead to under-diagnosing or misdiagnosing of subpopulations. While this is important work that highlights an AI risk that needs to be mitigated, the incident reports currently do not show a direct linking of the problem to a deployed AI.  Thus it is uncertain if harm happened or could imminently happen.  The definition of CSET AI harm is not met.  ",2020,10,16,True,no,no,,,,North America,,,"October 16, 2020, refers to the date of publication of the academic paper.","Chest x-ray classifier, University of Toronto researchers, Vector Institute, MIT researchers",0,0,False,,A chest x-ray classifier developed for research purposes. ,"x-ray images, patient information",human health and social work activities,no,Autonomy1,"The AI system, although developed for health sector, was never deployed.",No. Not intentionally designed to perform harm,,image classification,"convolutional neural networks, deep learning",
CSETv1,84,True,84,002,6. Complete and final,002,False,no,no,no,yes,no,yes,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none," 3.3 - Although there was no tangible harm, the AI was linked to the adverse outcome described in the incident.",no,no,no,yes,no,none,AI failed to prevent the spread of misinformation,yes,yes,yes,True,yes,,2019,10,,True,no,no,,,,Global,,,"The date is the beginning of the period of data collection and analysis by Avaaz, which ran from 10/2019-08/2020.","Facebook, Avaaz, Independent fact-checkers (Politifact, Reuters, AP), Facebook users, Misinformation detection AI",0,0,False,,"After independent fact-checkers identify misinformation in posts, Facebook flags the posts and notifies users who have interacted with them. It then uses AI to detect and flag copies of misinformation found elsewhere on the platform.","Facebook posts, text, images",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"fact-check, identify misinformation",,Facebook's artificial intelligence system is used to catch posts that break its rules of misinformation
CSETv1,97,True,97,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,,no,no,no,no,no,none,,no,yes,yes,True,no,,2020,,,False,no,yes,,,CH,Europe,,,,"Reddit user cyntrex, Tesla, Tesla Model 3, Tesla Autopilot",0,0,False,,"Tesla's Autopilot is a driver-assistance system meant to help drivers with normal driving functions like steering, braking, and accelerating semi-autonomously.","radar, video",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,Tesla Model 3,"stoplight recognition, semi-autonomous driving",,
CSETv1,98,True,98,002,6. Complete and final,002,False,yes,no,no,no,no,no,no,no,yes,tangible harm definitively occurred,maybe,yes,True,unclear,"According to statements by Boston Dynamics: ‘There is currently no artificial intelligence used for Spot’s walking control over how the robot plans paths and understands the world around it. We use pretty conventional non-AI techniques in those systems, because we know how to work with those really well and they’re very predictable.' It is unclear if NYPD also tested any of the available AI-powered augmentations.

NYPD incurred costs because they terminated the lease of Digidog early in response to public backlash. ",no,no,no,no,no,none,,no,maybe,yes,True,no,,2020,,,False,no,yes,New York,NY,US,North America,emergency services,,"Police began leasing Digidog in 2020, and terminated the contract early in April 2021 in response to public backlash. ","NYPD, Digidog, Boston Dynamics, Citizens of New York",0,0,False,,"Digidog, or ""Spot"", is a robot dog used to aid law enforcement by ""going places where humans can't"".",Unclear,law enforcement,yes,unclear,9.5 - Digidog can be controlled remotely but can also operate fully autonomously.,No. Not intentionally designed to perform harm,robotic dog,law enforcement,,"According to statements by Boston Dynamics: ‘There is currently no artificial intelligence used for Spot’s walking control over how the robot plans paths and understands the world around it. We use pretty conventional non-AI techniques in those systems, because we know how to work with those really well and they’re very predictable.' It is unclear if NYPD also tested any of the available AI-powered augmentations."
CSETv1,101,True,101,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"financial harm and intangible harm.
Fraud detection model described as a self-learning black box algorithm. ",yes,yes,yes,no,yes,"nation of origin, citizenship, immigrant status"," People with dual citizenships were disproportionately accused of fraud and denied access to public services (welfare payments). Since this affected only families with children, minors were disproportionately affected. ",yes,yes,yes,True,yes,,2013,,,True,no,no,,,NL,Europe,,,,"Dutch Tax Authority, Childcare benefits recipients, Childcare benefits recipients , Fraud prediction algorithm, Dutch Data Protection Authority, Trouw, Childcare benefits recipients",0,0,False,"financial harm; more than 20,000 families wrongfully accused. ",Self-learning algorithm deployed by the tax administration to create risk profiles of welfare recipients in an effort to spot childcare benefits fraud.,"personal data, personal finances",public administration,yes,Autonomy3,,No. Not intentionally designed to perform harm,,fraud risk prediction,,
CSETv1,12,True,12,,6. Complete and final,002,False,no,no,yes,no,yes,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,sex,"The paper found substantial gender biases in the word2vec and GloVe embeddings trained on common datasets, but since the model in question was not deployed or used to impact groups of people, there was no harmful Differential Treatment. ",no,yes,yes,True,no,,2016,07,21,True,no,no,Boston,MA,US,North America,,,,"w2vNEWS embedding, Boston University and Microsoft researchers, Google",0,0,False,,word2vec word embeddings trained on Google News articles,"text, words, news articles","professional, scientific and technical activities",no,Autonomy1,,No. Not intentionally designed to perform harm,,"word association, natural language processing","neural networks, vector embedding, linear algebra","""Vector embeddings algebraically represent words in order to compare them to each other. This is a helpful tool in refining neural networks for natural language processing so that AI can associate relevant words with each other."""
CSETv1,13,True,13,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"sex, sexual orientation or gender identity, religion, race, nation of origin, citizenship, immigrant status, disability","Google's sister company, Jigsaw, created a tool called ""Perspective"" that uses machine learning to classify the toxicity level of a given phrase or sentence. Individuals and researchers accessing the product through an API demonstrated the model's inability to reliably categorize harmful sentences as toxic and innocuous phrases as non-toxic. In particular, the model's output exhibited ableist, racist, sexist, and homophobic tendencies. ",yes,yes,yes,True,yes,,2017,,,True,no,no,,,US,North America,,,,"Perspective API, Women, Jigsaw, Partner organizations, Various online media outlets, Google, Minority groups",0,0,False,,"Google's Perspective project, a machine learning-based system to identify toxic comments in online discussion forums",text,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,toxicity detection,natural language processing,"The system uses a combination of natural language processing techniques and machine learning algorithms to analyze text and identify language that is toxic, where toxic is defined as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion."""
CSETv1,19,True,19,,6. Complete and final,002,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,maybe,yes,"race, sex",,yes,yes,yes,True,yes,,2013,01,13,False,yes,no,,,US,North America,,operationally representative,,"Latanya Sweeney, Women, Black people, Google, Google AdSense",0,0,False,,Online ad recommendation and delivery ,"search queries, text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,none,"personalized online advertising, recommender",natural language processing,
CSETv1,47,True,47,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,sex,LinkedIn's search suggestion algorithm prompted users searching for female names to choose similar-sounding male names instead.,yes,yes,yes,True,yes,,2016,08,31,True,maybe,no,,,US,North America,,,The date refers to the date of publication of the investigation revealing the bias.,"Women, Seattle Times, LinkedIn",0,0,False,,Search engine recommender system suggesting alternative search terms (names) for professional networking.,"text, names",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"prediction, recommendation, search suggestion",,
CSETv1,57,True,57,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,"The system in question does not meet the CSET definition for AI. It is likely purely and automation, or a rules-based system without machine learning components. 
",yes,yes,no,no,no,none,Access to social welfare (a public service) was interfered with.,yes,no,yes,True,no,,2016,07,,True,no,no,,,AU,Oceania,,,,"Centrelink, Centrelink 'RoboDebt' system, Australian Department of Human Services, Australian welfare recipients, Australian welfare recipients",0,0,False,,Automated tax and welfare compliance system. ,"income data, tax reports, tabular",public administration,yes,Autonomy1,,No. Not intentionally designed to perform harm,,,,not AI
CSETv1,5,True,5,,6. Complete and final,002,False,yes,no,yes,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,No evidence that robots used AI.  Robots are guided by surgeons to make precise cuts.,no,no,no,no,no,none,,no,no,yes,True,no,"Only tangible, no intangible, harm was reported",2000,,,True,no,yes,,,US,North America,healthcare and public health,,Time period covered: 2000-2013,"UIUC Researchers, Surgeons, Patients, Da Vinci robotic system, Intuitive Surgery",144,1391,False,study identified 1391 injuries and 144 deaths associated with robot-assisted surgeries. Not reports of AI embedded in robots.,Surgery robot. ,video input,human health and social work activities,no,Autonomy3,"Robotic surgical tools have been adopted broadly across minimally invasive surgeries in fields like gynecology, urology, general, colorectal, cardiothoracic, and head and neck surgery.",No. Not intentionally designed to perform harm,medical robot,,,
CSETv1,7,True,7,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",maybe,yes,False,none,"It is unclear if any of the Wikipedia bots under study relies on machine learning technology, but it is unlikely. Nobody experienced any harm. ",no,no,no,no,no,none,,no,maybe,yes,False,no,,2001,,,False,yes,no,,,,Global,,,"Although AI involvement is unlikely, the study analyzed bot-on-bot conflict, i.e. the interaction of multiple autonomous systems. ","Wikipedia bots, Oxford and Alan Turing Institute Researchers, Wikipedia, Wikipedia Editors, Wikipedia Users",0,0,False,,"Wikipedia bots are intended to autonomously correct spelling, maintain links, and monitor vandalism on Wikipedia pages.",text,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"webpage maintenance, editing","regular expressions, natural language processing",
CSETv1,8,True,8,,6. Complete and final,002,False,yes,no,no,no,no,yes,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"An investigation by the New York Times, citing employee interviews and internal documents from Uber, revealed that  the mapping programs guiding the vehicle in question failed to recognize six traffic lights.",no,no,no,no,no,none,,no,yes,yes,True,no,,2016,12,14,False,no,yes,San Francisco,CA,US,North America,,,,"Uber, Volvo XC90 SUV, Pedestrian, Charles Rotter",0,0,False,,"The AI system developed by Uber for autonomous vehicles is designed to autonomously navigate cars through operational conditions by following traffic lights, avoiding obstacles, and protecting passengers, pedestrians, and other cars on the road.","video, sensor, radar, GPS",transportation and storage,no,Autonomy2,Investigation by the New York Times proved the vehicle was in autonomous mode at the time of the incident with a human safety driver on board. ,No. Not intentionally designed to perform harm,Volvo XC90 SUV,"navigation, driving",,
CSETv1,29,True,29,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,21,True,21,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,42,True,42,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,62,True,62,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,85,True,85,,6. Complete and final,002,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,0,0,False,,,,,,,,,,,,
CSETv1,43,True,43,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",maybe,yes,True,none,,no,yes,no,no,yes,"nation of origin, citizenship, immigrant status, sex, race",The Commission for Racial Equality found St. George's Hospital Medical School guilty of discrimination against women and members of ethnic minorities.,yes,maybe,yes,True,maybe,,1979,,,False,no,no,London,,GB,Europe,,,,"Medical school applicants, Medical school applicants, Admissions algorithm, St. George's Hospital Medical School, Commission for Racial Equality, Dr. Franglen",0,0,False,,"Applicant screening algorithm used in the first stage of admissions process, scoring and ranking applications for interview selection.","Text, applicant data","Education, human health and social work activities",no,Autonomy3,"9.5 The computer used applicants' information to generate a score which was used to decide which applicants should be interviewed. Although the system operated fully independently, it only automated the first stage of the admissions process. Human decisions were involved in the next stages of the process and in making the final decisions on acceptance.",No. Not intentionally designed to perform harm,,"Rank Applicants, application screening",natural language processing,"algorithmic, but unclear if AI. "
CSETv1,30,True,30,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,no,True,none,Delay & financial loss cannot be linked to the performance of the robots but is instead due to a misallocation of resources from management side. ,no,no,no,no,no,none,,no,yes,no,True,no,,2018,03,,True,no,yes,Fremont,CA,US,North America,,,"Month corresponds to End of Q1 of 2018, which is when Tesla first reported production shortfalls. ","Tesla, Kuka, Kuka assembly robots",0,0,False,,"Manufacturing robot used in Tesla factories to produce the Model 3 car, performing tasks such as stamping, painting, welding, final assembly, and battery insulation.",,manufacturing,no,Autonomy1,"The robots operated independently, but often required human maintenance.",No. Not intentionally designed to perform harm,Manufacturing Robot,"production, assembly, object detection",computer vision,
CSETv1,31,True,31,,6. Complete and final,002,False,yes,no,no,no,no,no,no,yes,yes,tangible harm definitively occurred,no,no,True,none,"3.2 - The incident involved a driverless train which uses the CBTC system to monitor and regulate a train's location and position on rails. This technology uses track sensors to determine train position, status, etc. It has been in us since early 2000s. Very unlikely that the system has any AI components. 

3.3 - The harm is not linked to the technology. Rather, it is linked to the failure of a human operator to re-engage the brakes. Because the train's brakes, which were isolated (necessary for push/pull operations in the depot area when a train is required to move with a battery operated vehicle) were not normalized before moving the train on its own power, as soon as the operator brought the train at the ramp, it started rolling back, got derailed and hit the wall. ",no,no,no,no,no,none,,no,no,no,True,no,,2017,12,19,False,no,yes,Delhi,,IN,Asia,transportation,"Testing, operationally representative",,"Delhi Metro Rail Corporation, Kalindi Kunj metro depot, Hyundai Rotem, Driverless Delhi Metro train",0,0,False,,Self-driving metro train,"Train position/status, track sensors",transportation and storage,no,Autonomy1,"9.2 The train operated with CBTC. This technology uses track sensors to determine train position, status, etc.
9.5 There was human oversight, but no possibility to intervene. ",No. Not intentionally designed to perform harm,Metro train,,,Not AI
CSETv1,33,True,33,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,maybe,True,none,"Harm occurred (noise violation/fine). However, it was likely due to user error: Logs revealed that the Amazon Echo and Alexa played the music just three minutes after the user had left the house, making it very likely that the user had accidentally activated Spotify while still in range of the Bluetooth connection.",no,no,no,no,no,none,,no,yes,maybe,True,no,,2017,11,04,False,no,yes,Hamburg,,DE,Europe,,,,"Amazon, Haberstroh's neighbors, Oliver Haberstroh, Amazon Echo, Alexa",0,0,False,,Amazon's Alexa is a virtual assistant technology that can recognize spoken commands and connect to user devices through Bluetooth to perform tasks.,"voice, audio, speech, bluetooth",information and communication,no,Autonomy1,"9.5 - In theory, Haberstroh could have immediately intervened and turned off the music. In practice, he was unaware that any music was playing (and therefore not providing direct human oversight). ",No. Not intentionally designed to perform harm,Amazon Echo ,"Digital Assistant, Personal Assistant, speech recognition","natural language processing, human language technology",
CSETv1,26,True,26,,6. Complete and final,002,False,no,no,no,yes,no,no,yes,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2017,11,10,False,no,yes,,,VN,Asia,,,"6.1-6.3 - Bkav did two demonstrations hacking the iPhone X on 10 and 27th November 2017.
6.6 We usually don't consider AI on a phone to be embedded. Exceptionally done in this case because the AI serves to encrypt and secure access to the device which it is embedded in. 
","FaceID, Bkav, iPhone X, Apple, iPhone X owners",0,0,False,,FaceID is a facial recognition system that secures the iPhone X and unlocks the device through authorized users' identity verification. ,"images, facial images, dot projector, infrared camera, sensor data",information and communication,no,Autonomy2,"While FaceID does not use human oversight and operates independently, users can bypass the FaceID level of verification by entering the password.",No. Not intentionally designed to perform harm,Apple iPhone X,"facial recognition, identity verification","image mapping, point mapping, neural networks, computer vision, face detection","FaceID uses 30,000 points of reference to map out users' faces to a neural network which is checked against every time the user attempts to unlock the device. FaceID is powered by the TrueDepth camera system, which consists of sensors, a dot projector, infrared camera, and flood illuminator. "
CSETv1,20,True,20,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2016,05,07,False,no,yes,,,US,North America,"emergency services, transportation",,"This incident is a variant that contains several different events. May 7, 2016, is the date of the first fatal collision that occurred while Autopilot was engaged. It occurred in Williston, Florida. Joshua Brown's Tesla Model S drove under the trailer of an 18-wheel truck on a highway while in Autopilot mode because the vehicle's sensors failed to distinguish the white trailer (driver Frank Baressi) against the bright sky. 

On March 23, 2018, in Mountain View, California, a Tesla Model X collided with a highway barrier and caught fire. This killed the driver, Wei ""Walter"" Huang. Tesla reported that the driver had received several cues to reengage with the wheel but did not.

On May 11, 2018, in South Jordan, Utah, a Tesla Model S crashed into a stopped fire truck while operating in autopilot mode. The driver was looking at her phone.

In December 2019, in West Bridgewater, Massachusetts, Maria Smith was stopped by a state police trooper. As she was retrieving her vehicle registration, a Weston man named Nicholas Ciarlone crashed into the police cruiser which, in turn, hit her vehicle.

On July 14, 2020, in Arizona, a Tesla model S on autopilot crashed into a parked police vehicle and an ambulance. The driver was allegedly drunk.

On August 26, 2020, In North Carolina, a Tesla crashed into a police car because the driver activated Autopilot and was distracted watching a movie on his phone.","Tesla Model X, Tesla, Tesla Model S, Tesla Autopilot, Joshua Brown, Wei \""Walter\"" Huang, 28-year-old Tesla driver, Maria Smith, Massachusetts state police trooper, Nicholas Ciarlone, unnamed Tesla driver (NC), Nash county deputy and trooper, Frank Baressi's trailer, 2010 Mazda 3, 2017 Audi A4, 2010 Mazda 3 driver, 2017 Audi A4 driver, United Fire Authority mechanic truck driver, United Fire Authority truck, Massachusetts state police trooper vehicle, Nash county police vehicles, unnamed Tesla driver (AZ), Ambulance (AZ), Police vehicle (AZ), Police sergeant (AZ), Ambulance occupants (AZ)",2,10,True,"Lives lost include: Joshua Brown and Wei ""Walter"" Huang. It is unclear exactly how many injuries resulted from these events, and whether or not every person involved was injured. Injuries included in 8.2 include: Truck driver Frank Baressi, 2010 Mazda 3 driver, 28-year-old woman in Utah with a broken foot, Maria Smith, Massachusetts state police trooper, Nicholas Ciarlone, Tesla driver in Nash County, Nash county deputy and trooper, and unnamed Tesla driver in AZ.","Tesla's autonomous vehicles employ autonomous driving systems intended to avoid obstacles, follow traffic rules and transport passenger to their destination safely. Tesla's Autopilot is a ""suite of software, cameras and sensors intended to assist drivers and prevent accidents by taking over many aspects of driving a car."" These include steering, braking, and accelerating.","sensor data, radar, camera input, video, lidar",transportation and storage,no,Autonomy2,Autopilot is a semiautonomous driving-assistance system. Drivers are supposed to remain vigilant and be able to retake control of the wheel at any time.,No. Not intentionally designed to perform harm,"Tesla vehicle (Model 3, Model S, Model X)","navigation, object detection, object recognition",computer vision,
CSETv1,15,True,15,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,maybe,no,True,none,"3.1 - Although declines in sales for affected authors are not confirmed in the reports, considering the importance of sales rank in promoting visibility, authors would have avoided an impact on their sales only out of luck or randomness.
3.2 & 3.3 - According to Amazon’s statement, the removal of the sales ranks from certain titles did affect these books' promotion in Amazon's product recommender systems. But there is no indication that the removal of the sales rank from specific titles was done using AI, Amazon calls it a ‘cataloging error’. ",no,no,no,no,yes,sexual orientation or gender identity,,yes,maybe,no,True,no,,2009,04,,False,no,no,,,,Global,,,,"Authors, Amazon Sales Rank, Authors, Amazon",0,0,False,,"A book's sales rank indicates its popularity, which affects its level of promotion by Amazon's product recommender system. ",sales data,wholesale and retail trade,no,unclear,9.5 - It is unclear whether or not a human/Amazon policy was responsible for the exclusion of LGBTQ+ authors from sales ranks.,No. Not intentionally designed to perform harm,,,,
CSETv1,44,True,44,003,6. Complete and final,003,False,no,no,no,no,no,yes,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2000,06,,False,yes,yes,Los Angeles,CA,US,North America,,,6.1 - The experiment ran between June and December of 2000.,"Study participants, Electric Elves, USC Information Sciences Institute",0,0,False,,"Electric Elves use ""decision tree learning"" to perform personal assistant tasks like scheduling meetings and ordering lunch for human counterparts.","schedules, personal data, employee profile, GPS",administrative and support service activities,no,Autonomy2,,No. Not intentionally designed to perform harm,Palm VII,Personal Assistant,"decision tree, C4.5 algorithm, Markov decision process",
CSETv1,46,True,46,003,6. Complete and final,003,False,yes,no,no,yes,yes,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,no,yes,True,none,"3.1 - Nest Labs discovered in lab trials the possibility that the Nest Wave feature would malfunction and was able to issue a recall warning to consumers. However, because the product was already deployed, there was a non-imminent risk of tangible harm to users.
3.2 It is unlikely that the product/feature used AI technology. Wave-to-disable feature most likely used motion detectors and rules-based, basic algorithms. ",no,no,no,no,no,none,,no,no,yes,True,no,,2014,,,False,no,yes,,,US,North America,,,Nest Labs disclosed its discovery in April 2014. The recall was issued in May.,"Nest Labs, Nest Protect: Smoke + CO Detectors, Nest Protect Users, Google",0,0,False,,"Nest Protect is a smoke and carbon monoxide detector & alarm. Its wave-to-disable feature, which allowed users to quickly deactivate a faulty alarm, was found to disengage the alarm in actual cases of fire. ","air quality, sensor data, temperature, motion",other,no,Autonomy1,,No. Not intentionally designed to perform harm,smoke detector,,,not AI
CSETv1,59,True,59,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,"Although AI was implicated in the adverse outcome, this incident has no tangible harm.",no,no,no,no,yes,"sex, age", The study found biases related to gender and age in Google Translate. Additional biases have been found in Natural Language Processing in general.,yes,yes,yes,True,yes, The study found biases related to gender and age in Google Translate. Additional biases have been found in Natural Language Processing in general.,2017,5,25,True,no,no,,,,Global,,,,"Google, Google Translate, Google Translate users",0,0,False,,,text,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,translation,"vector embedding, natural language processing",
CSETv1,60,True,60,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,"Annotator 1: 

 3.3 - Although there was no tangible harm, the adverse outcomes of the incident were directly related to the AI.",no,no,no,no,yes,race,"Annotator 1: 

 4.4 - FaceApp added a filter that allowed users to look like different races/ethnicities. It also added a ""hot"" filter that automatically lightened skin color.",yes,yes,yes,True,yes,"Annotator 1: 

 5.4 - The entities harmed can be characterized as FaceApp users, but it would be difficult to identify individual users.
5.5 - The harm in this incident falls somewhat outside those defined by CSET. Using a filter to look like a different race is widely culturally problematic, but it is not explicitly a violation of rights. Also, every user was able to access the technology so harms were not distributed unevenly based on a protected characteristic. However, individuals were in fact affected on the basis of race and national origin which qualifies this incident as causing AI special interest intangible harm.",2017,08,09,False,no,no,,,RU,Global,communications,,,"FaceApp, FaceApp race-changing filter and \""hot\"" filter, Wireless Lab, FaceApp users, FaceApp users of color, Yaroslav Goncharov (FaceApp CEO)",0,0,False,,FaceApp's system allows users to edit images and apply filters to uploaded photos.,"images, photos","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"image modification, filter, photo edit",,
CSETv1,61,True,61,,5. In quality control,003,True,no,no,yes,no,yes,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,no,True,none,"Annotator 1: 

 3.1 - The competition's reward was $150,000 prize money. Competitors may have been discouraged or confused by the initial overfit.
3.3 - The AIs that the competitors produced to sort and identify fish species was not linked to the harm. The harm was in the sample set of data, which was extremely unrepresentative and led to poor outcomes of the models on the full set of data.
3.5 - There is an issue of tangible harm but it does not meet CSET's definition for AI tangible harm.

Annotator 2: 

 This a report about a problem with a Kaggle competition.  For this particular competition, the competition organizers did not do a good job preparing the dataset for the competition.  This resulted in some people prematurely dropping out of the competition.  As a result, some may have lost prize money or prestige.  However, the harm comes from the actions of the organizers.  None of the produced AIs did harm.",no,no,no,no,no,none,,no,yes,no,True,no,"Annotator 2: 

 There was no special interest intangible harm, however, there was another intangible harm not meeting CSET definition--loss or prestige. ",2017,05,01,True,no,no,,,,Global,,Non-operational,"Annotator 1: 

 6.4 - The blog entry was posted on May 1st, 2017. The competition occurred prior to that date, as the user describes the experience in past tense.","Kaggle Competitors, Kaggle , Gidi Shperber",0,0,False,,classification models,photos,"information and communication, Education",no,Autonomy1,"Annotator 1: 

 9.5 - The AI models competitors produced operated independently, but their development and deployment occurred wholly at the prerogative of the competitors.",No. Not intentionally designed to perform harm,,classification,"VCG network, YOLO network algorithm, SSD network algorithm",
CSETv1,74,True,74,003,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"This incident has mulitple reports of AI misidentifying people, which leads to their arrest. For example. in Detroit police handcuffed and arrested someone in front of his family, forced him to provide a mug shot, fingerprints, and a sample of his DNA, interrogated him, and imprisoned him overnight. The case was later dismissed because of the poor performance of the AI, which matched the arrested person to a grainy photo.",no,yes,no,no,yes,race,"A wrongful incarceration was the result of the AI and the police department's failure to correctly match the man in the security footage to a line of suspects/a database of ID photos, which is exacerbated by facial recognition technology's lower effectiveness on faces with darker skin tones.",yes,yes,yes,True,yes,,2020,01,,False,no,no,,,US,North America,,,,"Robert Julian-Borchak Williams, Michael Oliver, and Nijeer Parks, Detroit Police Department, Jennifer Coulson, DataWorks Plus, DataWorks facial recognition system, Robert Julian-Borchak Williams, Michael Oliver, and Nijeer Parks, Robert Julian-Borchak Williams, Michael Oliver, and Nijeer Parks, Clearview AI",0,0,False,,"facial recognition company, providing software to companies, law enforcement, universities, and individuals. ","security footage, images, video input",law enforcement,yes,Autonomy3,,No. Not intentionally designed to perform harm,,facial recognition,image match,"matches faces to a database of more than 20 billion images indexed from the Internet, including social media applications"
CSETv1,75,True,75,,4. Peer review complete,003,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,religion,"Google search results auto-filled ""Jewish"" after the names of certain Jewish public figures, a phenomenon not observed with public figures of other religions.",yes,yes,yes,True,yes,,2012,,,True,no,no,,,FR,Europe,information technology,,Reported article date,"Google Instant Autocomplete, SOS Racisme, Jewish public figures, Google",0,0,False,,"""Google Instant autocomplete feature ""predicts and displays search queries based on other users' search activities and the contents of web pages indexed by Google.""""","search queries, words",information and communication,no,Autonomy1,"Google search results auto-filled ""Jewish"" after the names of certain Jewish public figures, a phenomenon not observed with public figures of other religions.",No. Not intentionally designed to perform harm,,"autocomplete, search optimization","Natural Language Processesing, recommendation",
CSETv1,76,True,76,003,5. In quality control,001,True,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event," The live facial recognition system in Buenos Aires has led to multiple false arrests, causing individuals to be wrongly detained for days. 

 Although facial recognition being inaccurate plays a part in the ineffectiveness of the use of facial recognition law enforcement combined with CONARC, the main issue is in the CONARC database itself. Not only does it falsely list minors and children of having committed crimes, but it often uses their real, full names which is illegal. The information about them is often incorrect. The information about children is also not well protected but in an unsecured file.
",no,yes,yes,no,yes,age,"The facial recognition algorithm used performs worse on kids (ages 10 to 16) than on adults (ages 24-40)

There is an additional intangible harm--harm to privacy, which is not one of CSET's special interest intangible harms.",yes,yes,yes,True,yes," AI and facial recognition systems perform worse on children. Not only do their physical features change more rapidly, but they are also less distinguishable than the features of adults",2019,,,True,no,no,,,AR,South America,,,"Annotator 1: 

 6.1 - CONARC began in 2009. It is unclear when the first child was added to the database.

Annotator 2: 

 Date of publication","Argentinian government, Argentinian children in CONARC, Argentinian children in CONARC, CONARC - Consulta Nacional de Rebeldías y Capturas , Human Right Watch, Buenos Aires city government, People in CONARC spreadsheet, Facial recognition AI",0,0,False,,"CONARC is a ""plain-text spreadsheet file"" that lists people and their alleged crimes. It began in 2009 as an effort to aid law enforcement in addressing serious crimes. Information in it is combined with database of photos and a facial recognition AI.","names, national IDs, alleged crimes, text, photo IDs","law enforcement, public administration",yes,Autonomy3,,No. Not intentionally designed to perform harm,,"database, facial recognition",image match,"The software uses suspects’ headshots to scan for real-time matches via the city’s subway cameras. Once the system flags a person, it alerts to the police to make an arrest."
CSETv1,77,True,77,003,6. Complete and final,003,False,yes,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,no,True,none,"The AI robot was not linked directly to the tangible harm (the fight happened before the incident with the robot). Arguably, the severity of the tangible harm may have been less had the emergency button on the robot alerted authorities as the user expected. However, this communication line to the police was not an AI functionality, therefore it cannot be linked to the harm. ",no,no,no,no,no,none,,no,yes,no,True,no,,2019,10,,False,no,yes,Los Angeles,CA,US,North America,,operationally representative,,"Injured woman, Cogo Guebara, HP RoboCop, Knightscope, Huntington Park Police Department",0,1,False,"One woman was injured in the context of the event, not related to HP Robocop. ","HP RoboCop, equipped with a camera, is programmed to follow a certain route telling visitors to keep the park clean and acting as a security monitor. It is advertised as ""including a 360-degree high-definition live video stream, a license plate reader that can scan 1,200 plates a minute, a two-way intercom and the ability to track cell phone use in the vicinity."" It has 5 cameras that provide ""24/7 monitoring.""","camera footage, video",law enforcement,yes,Autonomy1,,No. Not intentionally designed to perform harm,"A cone-shaped, 400-pound security robot on wheels","security monitor, surveillance, patrolling","robotics, computer vision",
CSETv1,80,True,80,003,6. Complete and final,001,False,yes,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2020,10,24,False,no,yes,,,GB,Europe,,,,"Inverness Caledonian Thistle Football Club , AI-powered camera, Football fans",0,0,False,,The AI-powered camera was meant to track and pan to the ball in a soccer game for a live stream of the match.,video,"Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,Camera,"automatic tracking, ball detection",computer vision,
CSETv1,86,True,86,,5. In quality control,002,True,no,no,no,yes,no,no,no,no,yes,unclear,no,yes,True,none,"The algorithm assigned lower-than-appropriate grades to about 6,000 students. This affected their admissions to university (noted in the next session). For those not attending university, the lower grades might have affected their ability to find employment. However, this is not discussed in the reports, therefore it is marked as unclear. ",yes,yes,yes,no,yes,,"The mistake affected students' admission to university, and therefore unfairly restricted their access to higher education. ",yes,no,yes,True,no,,2020,10,,False,no,no,,,IE,Europe,,,,"Leaving Certificate exam takers, Polymetrika, Irish Department of Education, Educational Testing Services, Leaving Cert grading algorithm, Leaving Certificate exam takers",0,0,False,,algorithm to predict high school students' final grades,"student data, test scores, student grades",Education,yes,Autonomy1,,No. Not intentionally designed to perform harm,,prediction,,not AI
CSETv1,95,True,95,003,6. Complete and final,001,False,no,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"The merit in the AI system's assessment of candidate performance is arguable, and therefore flawed scores likely held back qualified candidates from employment opportunities. 
",no,maybe,no,no,yes,"disability, sex, race, sexual orientation or gender identity",,yes,yes,yes,True,yes,,2019,,,True,no,no,,,US,North America,,,"6.5 - HireVue was founded in 2004. The ""complaint and request for investigation, injunction, and other relief"" described by this incident was filed by the Electronic Privacy Information Center (EPIC) in November of 2019","Unnamed AI used by HireVue Platform, HireVue Inc., Electronic Privacy Information Center (EPIC), Corporations using HirevVue, Job applicants assessed by HireVue, Job applicants assessed by HireVue",0,0,False,,"Hiring algorithm that uses candidates’ computer or cellphone cameras to analyze their facial movements, word choice, intonation and speaking voice  in a virtual interview as part of an assessment of their performance. Candidates are ranked against other applicants based on an automatically generated “employability” score.","facial images, video input, speech, audio","administrative and support service activities, professional, scientific and technical activities",no,unclear,"9.5 - Although the final decision regarding employment is up to live people, HireVue may be used to pre-screen and eliminate large portions of applicant pools. This means the level of autonomy with which the system operates is not clear from the sources",No. Not intentionally designed to perform harm,,"candidate assessment, virtual interview, emotion recognition, facial expression recognition, speech recognition",,
CSETv1,99,True,99,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,yes,yes,no,no,yes,"race, financial means","Different universities could customize the algorithm to include different factors when predicting risk. It was discovered that some schools used race as a predictive factor, which resulted in race-based bias.

Some universities are considered as part of the public sector.",yes,yes,yes,True,yes,,2020,,,True,no,no,,,US,North America,Other,,,"EAB university clients (ex. University of Massachusetts Amherst, the University of Wisconsin–Milwaukee, the University of Houston, and Texas A&M University), EAB, EAB Navigate, The Markup, Black college students, Latinx college students",0,0,False,,predicts dropout rates for university students,"GPA, SAT and ACT scores, high school percentile, credits attempted, credits completed, estimated skills, student data","administrative and support service activities, Education",yes,Autonomy3,,No. Not intentionally designed to perform harm,,"risk prediction, risk assessment, performance prediction",machine learning,
CSETv1,103,True,103,003,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"race, sex, age, disability",,yes,yes,yes,True,yes,"The cropping neutral network would crop the preview image in way that focused more on individuals with lighter completions, younger, female, or without disabilities.",2020,09,,True,no,no,,,,Global,information technology,,,"Twitter, Twitter Preview Cropping Neutral Network, Twitter Users",0,0,False,,"Cropping Neutral Network. Given an image, it crops the image to create a preview.","images, photos",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"image cropping, Crop Images",computer vision,
CSETv1,105,True,105,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,yes,no,no,none,"Annotator 1: 

 4.3 - A minor was involved in the incident. Jovani Maldonado, the victim who died in the crash, was 15 years old. However, he was not disproportionately treated or specifically targeted.",no,yes,yes,True,no,,2019,08,24,False,no,yes,,CA,US,North America,,,,"Jovani Maldonado, Benjamin Maldonado, Lagman Yalung, Tesla, Tesla Autopilot, Ford Explorer, Tesla Model 3, Vilma Yalung",1,1,True,"Annotator 1: 

 8.3 - It is confirmed that Jovani Maldonado died as a result of the incident. It is unclear if Benjamin Maldonado, the father of the deceased, was injured. Lagman and Vilma Yalung, the two passengers in the Tesla Model 3 did not report injuries from the accident. ","Tesla's Autopilot is ""a suite of software, cameras and sensors intended to assist drivers and prevent accidents by taking over many aspects of driving a car - even the changing of lanes.""","radar input, camera input, sensor data",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,Tesla Model 3,"autonomous navigation, semi-autonomous driving, object detection, classification",,
CSETv1,27,True,27,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,no,yes,True,none,"3.3 - The system was not AI. However, it was a technology system that can be directly linked to the near miss that occurred.
3.5 - Since the system was not AI, there is no AI harm.",no,no,no,no,no,none,,no,no,yes,True,no,,1983,09,26,False,no,yes,Kurilovo,,RU,Europe,defense-industrial base,,,"Russian and American citizens, Stanislav Petrov, Soviet Union, Oko",0,0,False,,Nuclear missile defence early warning system,satellite data,defense,yes,Autonomy3,Not an AI system,unclear,,,,"not AI. 
Not intended to harm directly, but intended to detect harm and inform decisions that lead to harm. "
CSETv1,28,True,28,,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,"3.3 - The system was not AI, it was an automated spoofing robot meant to cancel sell orders if the price approached its offers. However, it is directly implicated in the chain of harm causing this incident.
3.5 - Because there is no AI, there is no AI harm.",no,no,no,no,no,none,,no,no,yes,True,no,,2010,05,06,False,no,no,,,,Global,financial services,,6.10 - The incident was caused by British Indian trader and affected American stock markets.,"Stock indices (Dow Jones Industrial Average, S&P 500, Nasdaq Composite), Navinder Singh Sarao, trading software, Stock owners",0,0,False,,Not AI. An automated trading software,"stock prices, stock orders, stock data",financial and insurance activities,no,Autonomy1,,Yes. Intentionally designed to perform harm and did create intended harm,none,,,not AI
CSETv1,32,True,32,,6. Complete and final,002,False,no,no,no,yes,no,no,no,yes,maybe,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,"Although this incident doesn't involve harm unevenly distributed along a protected characteristic, it does only impact twins.",2017,09,13,False,no,yes,,,US,North America,,Twin faces,,"Apple, FaceID, Twin iPhone X users, iPhone X",0,0,False,,"Facial recognition system to verify identity of phone user. FaceID uses 30,000 points of reference to map out users' faces to a neural network which is checked against every time the user attempts to unlock the device. ","facial images, dot projector, infrared images",information and communication,no,Autonomy2,"9.5 - While FaceID does not use human oversight and operates independently, users can bypass the FaceID level of verification by entering the correct password.",No. Not intentionally designed to perform harm,Apple iPhone X,facial recognition,"image mapping, point mapping, facial recognition, facial reconstruction, image reconstruction",
CSETv1,37,True,37,,6. Complete and final,002,False,no,no,no,maybe,no,yes,no,no,maybe,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"There are no reports of actual biased recruitment decisions made (no known harm event). However, the algorithm's discriminatory ratings are known, as is the fact that it was being tested by Amazon for several years before it was rejected, during which recruiters examined candidates' ratings assigned by the algorithm. ",no,maybe,no,no,yes,sex,Resumes featuring language commonly associated with women (e.g. women's colleges or organizations) were downgraded by the resume screening tool.,yes,yes,yes,True,yes,,2014,,,False,no,no,Edinburgh,,IE,Europe,,,,"Amazon, Resume screening tool, Female applicants, Female applicants",0,0,False,,Resume screening tool to assess job candidates. The recruitment tool was fed the resumes of successful applicants from the 10-year period before 2014 in order to make decisions about which resumes corresponded most to those that were previously successful.,"Resume, Text",administrative and support service activities,no,Autonomy3,,No. Not intentionally designed to perform harm,,"Rank Applicants, resume screening",natural language processing,
CSETv1,48,True,48,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,race,,yes,yes,yes,True,yes,,2016,12,,False,no,no,,,NZ,Oceania,,,,"New Zealand Department of Internal Affairs, Richard Lee, Passport Checker",0,0,False,,Online tool for verifying quality standards of passport photos.,"facial images, images",public administration,yes,Autonomy1,,No. Not intentionally designed to perform harm,,"face detection, facial recognition",,
CSETv1,49,True,49,002,6. Complete and final,002,False,no,yes,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,"nation of origin, citizenship, immigrant status, race",Beauty.ai determined mostly white applicants to be most attractive among all contestants.,yes,yes,yes,True,yes,,2016,08,,False,no,no,,,,Global,,,,"Beauty.AI, Microsoft, Youth Laboratories, Beauty Contest Participants, PIMPL, RYNKL, Symmetry Master, MADIS, AntiAgeist",0,0,False,,"AI system to select winners of a beauty contest. The Beauty.AI system used five different judges to determine beauty. RYNKL, PIMPL, MADIS, Symmetry Master, and AntiAgeist judged based on wrinkles, pimples/pigmentation, similarity to models, symmetry of the face, and the difference between estimated and chronological age.","selfies, facial images","Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,Image Analysis,"facial recognition, deep learning, machine vision, Deep Learning",
CSETv1,50,True,50,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,maybe,yes,True,none,It is not clear whether the Decentralized Autonomous Organization involves any AI systems. ,no,no,no,no,no,none,,no,maybe,yes,True,no,,2016,06,17,False,no,no,,,,Global,,,,"The DAO (Decentralized Autonomous Organization), DAO hacker, Slock.it, DAO Investors, Ethereum, Emin Gun Sirer",0,0,False,Estimated financial loss of $79.6 million,The DAO is a decentralized investment fund built on the Ethereum network. ,"code, DAO token IDs",financial and insurance activities,no,unclear,,No. Not intentionally designed to perform harm,,,,Unclear if AI system involved in incident
CSETv1,51,True,51,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"Annotator 2: 

 Even though the child's behavior (running towards the robot) contributed to the incident, the mall is the standard operational environment for such a robot and should be prepared for interaction with children. Moreover, the company states in the incident description that the robot did not detect the child after it had fallen down and continued to move.",no,no,yes,no,no,none,"Annotator 2: 

 A minor was involved, but there was no pattern of disproportionate treatment or specific targeting based on age.",no,yes,yes,True,no,"Annotator 2: 

 Even though the child's behavior (running towards the robot) contributed to the incident, the mall is the standard operational environment for such a robot and should be prepared for interaction with children. Moreover, the company states in the incident description that the robot did not detect the child after it had fallen down and continued to move.",2016,07,07,False,no,yes,Palo Alto,CA,US,North America,,,,"Knightscope K5 security robot, Knightscope, Stanford Mall, Harwin Cheng",0,1,False,,"Knightscope K5 security robot with ~ 30 sensors, including a multitude of laser ranging devices and sonar sensors allowing it to sense the surrounding environment from less than an inch away to over 300 feet.","infrared camera, images, video, lidar, sonar, vibration detectors","wholesale and retail trade, administrative and support service activities",no,Autonomy1,,No. Not intentionally designed to perform harm,"rocket/egg shaped, 300 pound, 5 ft. tall security robot","security, patrolling","computer vision, Sensor Data Processing",
CSETv1,78,True,78,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",no,yes,True,none,"The harm was caused by a statistical algorithm that did not meet our definition of AI.
Harm did occur, but it was intangible (opportunity loss) instead of tangible.",yes,maybe,yes,no,maybe,unclear,"4.3 - IB score prediction algorithms affected high school seniors around 17-18 years of age.
4.4 and 4.5 It is unclear if there was a differential distribution based on a protected characteristic.  Little information was released about the statistically derived algorithm.  It is possible that the algorithm incorporated a protected characteristic or a proxy variable for that characteristic",maybe,no,yes,True,no,"This might be a special interested harm because access to schooling was affected by the statistical algorithm, but it is unclear if this was unfair or biased.  It is not an AI special interest harm, because the CSET definition for AI is not met.",2020,07,06,True,no,no,,,,Global,,,"affected 170,000 International Baccalaureate students worldwide, but most were in the US","International Baccalaureate students, Anahita Nagpal, International Baccalaureate Diploma Programme, IB grade prediction tool",0,0,False,,"The IB score prediction algorithm, deployed in place of in-person tests during the pandemic, ""used signals including a student’s grades on assignments and grades from past grads at their school to predict what they would have scored had the pandemic not prevented in-person tests."" It is a statistical model and does not use AI.","student grades, school attended, predicted final grades, schools' historical IB results",Education,yes,Autonomy1,,No. Not intentionally designed to perform harm,,predict grades on exam,,not AI
CSETv1,45,True,45,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,3.1 - The tangible harm in this incident refers to the damages Google was ordered to pay in various lawsuits alleging defamation against plaintiffs because of Autocomplete suggestions and image results.,no,no,no,yes,yes,religion,"4.2 - Certain legal opinions describe Google as violating the right to personality, or that an entity's personality and reputation is respected and can be freely developed. However, these are local laws and do not fall within the covered rights of this field. 
4.4 - Google Autocomplete suggestions imply misinformation (incorrect information about subjects). Google Image results display innocent people in relation to crimes or criminals.",yes,yes,yes,True,yes,,2011,03,24,False,no,no,,,,Global,,,"This incident is a variant that contains several different events. March 24, 2011 is the first instance of a lawsuit against Google regarding defamation. The court of Milan upheld its order for Google to filter out libellous search suggestions after an unnammed businessman and entrepreneur in the financial services sector was associated with Autocomplete suggestions like ""truffatore"" (con man) and truffa (fraud).
In June 2011, in Ireland, the Ballymascanlon hotel sued Google for defamation because Autocomplete suggested that the hotel was in receivership (financial trouble).
In January 2012, French anti-discrimination SOS Racisme and other organizations sued Google because the Autocomplete feature suggested the word ""Jewish"" in searches involving non-Jewish public figures.
In March 2012 in Japan, Google was ordered to disable part of its Autocomplete function and pay the plaintiff because an unidentified man was linked to crimes he was not involved with, apparently making it hard for him to find work. The injunction was in March 2012 and the ruling was in 2014.
In September 2012, former German first lady Bettina Wulff sued Google over search results that paired her name with terms like ""prostitute."" In May 2013, the German Federal Supreme Court held Google liable.
In May 2013, in Germany, the Federal Court of Justice stated that Google's predictions can violate the right of personality in regards to the chairman of a corporation which sold food supplements and cosmetics online that had ""betrug"" (fraud) and ""scientology"" in search results.
In August 2014, Hong Kong tycoon and founder of management company Emperor Group Albert Yeung Sau-Shing sued Google because searches for his name added ""triad"" (an organized crime group) to his name.
In October 2015, an Australian court determined Google liable for articles defamatory articles about the defendant Dr. Duffy.
In June 2018, in Melbourne, Australia, the High Court ruled that Milorad Trkulja was allowed to sue Google for defamation because searches for ""Melbourne criminal underworld photos"" brought up images of him and also known gang members. The Autocomplete function and text-based search results gave options that included phrases like ""is a former hit man,"" ""criminal,"" and ""underworld.""","Google, Google Autocomplete, Google Images, Court of Milan, Unnamed businessman AB, Judge Roberto Bichi, Ballymascanlon Hotel, SOS Racisme, Union of Jewish Students of France, Movement Against Racism and for Friendship Among Peoples, Public figures (ex. Rupert Murdoch, Jon Hamm), Tokyo District Court, Lawyer Hiroyuki Tomita, Unnamed Japanese plaintiff, German Federal Court of Justice, Unnamed chairman of German corporation, Albert Yeung Sau-Shing, High Court of Hong Kong, Dr. Duffy, High Court of Victoria, Milorad Trkulja",0,0,False,,"The Google Autocomplete feature ""predicts and displays search queries based on other users' search activities and the contents of web pages indexed by Google.""
Google Images displays results based on the matches of search queries to alt tags of images uploaded to web sites on Google","text, images, search queries, image alt tags",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"content ranking, autocomplete, search optimization, image search, search suggestion","search engine optimization, natural language processing",
CSETv1,53,True,53,002,6. Complete and final,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"sex, race",Propagation of systemic racial bias by Google search ,yes,yes,yes,True,yes,,2012,04,,True,no,no,,,,Global,,,,"Kabir Alli, Google, Google Search, Black people, Women of color",0,0,False,,Search engine,"images, image alt tags, Text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,search engine optimization,,
CSETv1,54,True,54,002,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,"The potential of tangible harm, such as through a wrongful detainment or violent altercations with the police, has been considered but not considered applicable here because there is no explicit mention of tangible harm risk in the associated incident reports. ",no,maybe,no,no,yes,"financial means, race",,yes,yes,yes,True,yes,"The use of historical crime data as input into the algorithm's prediction leads to a feedback loop. The algorithm predicts more crimes in neighborhoods with high recorded crime levels in the past which, due to historical overpolicing, tend to disproportionately affect low-income and minority communities. This leads to higher police presence in these neighborhoods, which by default increases the number of recorded crimes there. This could potentially lead to situations in which citizens of these communities are perceived as 'guilty by association' and loose their right to individualized treatment, rather than as a statistic.  Even though neither race nor income are variables used in the algorithm explicitly, the algorithm's predictions are 'biased-by-proxy'. 
This effect has been demonstrated in a study by Gizmodo, who find that the predpol predictions have led to a vast increase in police patrols in minority neighborhoods. ",2011,,,True,no,no,,,US,North America,,,"Earliest deployment of this software was by LAPD in 2011. LAPD had access to a prototype of the software. This incident describes a variant of many different events involving predictive policing software. 

In Oakland, CA, in 2015, the Police Department convinced the mayor to earmark funds for predictive policing software but later rescinded the funding request.

In Jennings, MO, in 2015, the St. Louis County Police Department tried out software created by HunchLab.

In Richmond, VA, in 2016, the Police Department terminated its contract with PredPol halfway into a three year program because it found no measurable impact on crime reduction.

In 2021, Gizmodo published a report about PredPol where it found predictive policing software disproportionately targets low-income, Black, and Latino neighborhoods.

PredPol was also briefly mentioned as being deployed in other locations like Santa Cruz, CA, Atlanta, GA, Chicago, IL, and Los Angeles, CA.","Los Angeles Police Department, PredPol software, Residents of minority and low-income neighborhoods, HunchLab, Lincoln (NE) Police Department, Burbank Police Department, Azavea, Human Rights Data Analysis Group, Geolitica (formerly PredPol), UCLA Institute for Pure and Applied Mathematics, LAPD Captain Sean Malinowski, PredPol, St. Louis County Police Department, Gizmodo, Richmond Police Department",0,0,False,,"PredPol takes in past crime data and spits out predictions about where future crimes are more likely to occur. It turns those predictions into 500 by 500 foot red boxes on a Google map, indicating areas that police officers should patrol when they're not actively responding to a call. 

HunchLab surveys past crimes and also digs into other factors like population density; census data; the location of bars, churches, schools, and transportation hubs; schedules for home games - even moon phases to determine where patrols would have the most impact.","Crime Records, geospatial data, past type of crime, past location of crime, past time of crime, population density, census data, maps, patrol shifts",law enforcement,yes,Autonomy3,,No. Not intentionally designed to perform harm,,"Predict Crimes, predictive policing",Machine Learning,
CSETv1,40,True,40,,6. Complete and final,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"CSET considers wrongful detention, wrongful imprisonment, and wrongful differential/disproportionate imprisonment amounts to be tangible harm, because of the loss of physical freedom and autonomy.",yes,yes,no,no,yes,"race, nation of origin, citizenship, immigrant status",COMPAS and the non-expert evaluators were similarly accurate in correctly predicting recidivism. The two systems were similarly unfair regarding race when predicting false positives and false negatives.,yes,yes,yes,True,yes,,2016,,,False,no,no,,,US,North America,,, 6.1-6.3 - This date refers to the publication of the initial ProPublical report rather than the use of COMPAS.,"Black people, Northpointe, ProPublica, Broward County, Glenn Rodríguez, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), Equivant (formerly Northpointe), Julia Dressel, Hany Farid, Defendants assessed by COMPAS",0,0,False,,"The COMPAS system calculates a person's risk of recidivism based on 137 questionnaire responses about the situation and context of the crime and the person involved.

In the Dartmouth study, participants saw a short description of a defendant that included the defendant’s sex, age, and previous criminal history, but not their race. They predicted whether this person would recidivate within 2 years of their most recent crime. The description was formatted as follows: ""The defendant is a [SEX] aged [AGE]. They have been charged with: [CRIME CHARGE]. This crime is classified as a [CRIMINAL DEGREE]. They have been convicted of [NON-JUVENILE PRIOR COUNT] prior crimes. They have [JUVENILE- FELONY COUNT] juvenile felony charges and [JUVENILE-MISDEMEANOR COUNT] juvenile misdemeanor charges on their record.""

They also used an algorithmic assessment to determine that COMPAS' predictive accuracy can be achieved with only two features in a linear classifier, and that more sophisticated classifiers do not improve prediction accuracy or fairness.","personal data, Crime Records, Police reports, Text, sex, age, 137 questionnaire responses, previous criminal history, criminal degree","law enforcement, public administration",yes,Autonomy3,,No. Not intentionally designed to perform harm,,"predict recidivism, prediction","machine learning, logistic regression, non-linear SVM, signal detection theory",
CSETv1,104,True,104,,4. Peer review complete,001,False,no,no,no,maybe,no,no,no,no,maybe,"no tangible harm, near-miss, or issue",no,yes,True,none," It is unclear if the vaccine distribution algorithm involved AI. Additionally, at the time of the report, the system had not yet been deployed. ",yes,maybe,no,no,yes,"geography, financial means",,yes,no,yes,True,no,,2021,,,True,no,no, ,CA,US,North America,healthcare and public health,,,"California's Department of Public Health , California low-income neighborhoods, Blue Shield of California, California residents",0,0,False,,,zip code,"public administration, human health and social work activities",yes,Autonomy2,,No. Not intentionally designed to perform harm,,vaccine allocation,,
CSETv1,106,True,106,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,yes,"sexual orientation or gender identity, disability, race, sex",,yes,yes,yes,True,yes,,2021,01,,False,no,no, ,,KR,Asia,,,,"Scatter Lab, Lee Luda, Facebook Messenger, Korean Facebook Messenger users",0,0,False,,Lee Luda was a South Korean chatbot trained on messages from around 10 billion real-life conversations between young couples taken from KakaoTalk for Scatter Lab's Science of Lab app. Lee Luda was meant to emulate a 20-year-old female college student.,text,"information and communication, Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,chat bot,mesh autoencoders,
CSETv1,108,True,108,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,yes,no,yes,race, Facial recognition systems misidentify black people at a rate 5-10 times more than white people.,yes,yes,yes,True,yes,,2021,07, ,False,no,no,Livonia,MI,US,North America,,,,"Riverside Arena Skating Rink, Lamya Robinson, Unnamed facial recognition system",0,0,False,,facial recognition software,facial images,"Arts, entertainment and recreation",no,Autonomy3,"Annotator 2: 

 The system provides administrators with matches so that human workers can ban profile matches from entering the premises. The facial recognition systems alone cannot prevent humans from entering an establishment.",No. Not intentionally designed to perform harm,,facial recognition,,
CSETv1,109,True,109,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,False,unclear, The article notes the potential for nefarious actors like stalkers to use the PimEyes tool to do tangible harm.,no,maybe,no,no,no,none,,maybe,yes,yes,False,maybe,"Annotator 2: 

 It is unclear whether PimEyes violates peoples' privacy and whether or not that constitutes a rights violation.",2021, ,,False,no,no,,,,Global,,," 2021 represents the date when the article was posted, not necessarily PimEyes' date of development.","PimEyes, PimEyes facial recognition software, women, Lukasz Kowalczyk and Denis Tatina",0,0,True,,"PimEyes is a face-searching tool that ""can scan through more than 900 million images from across the Internet and find matches with startling accuracy."" It uses bots known as ""spiders"" to crawl the Web, ""scanning for photos of faces and then recording those images as numerical code. If the search tool is later shown a photo that resembles one of those images, it will return a direct link to where the image can be found.""",images,"information and communication, Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,facial recognition,,
CSETv1,112,True,112,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event," Many police departments that deployed ShotSpotter terminated expensive contracts because it was ineffective. The prominent harm in this incident is that ShotSpotter is ineffective: it wastes officers' time by prompting a response to shots fired calls that cannot be verified or are not gunfire. Often, it doesn’t result in more arrests or convictions. It increases policing in historically overpoliced neighborhoods. Microphones can record the conversations of people in these neighborhoods. However, these harms are not necessarily tangible.


 Several tangible harms have been reported in connection with the technology. 
1. Michael Williams spent a year in jail on murder charges based on evidence from ShotSpotter, before having his charges dismissed when prosecutors admitted they had insufficient evidence against him.
2. 13-year-old Adam Toledo was shot and killed by Chicago police after they were dispatched to his neighborhood in the early-morning hours of March 29 following a ShotSpotter alert. The boy was unarmed. 
3. The high false positive rate of the alerts leads to a significantly higher number of police deployments, and because officers expect to enter a dangerous situation with active shooters during these deployments, there is a reasonable probability that residents in affected neighborhoods experience harm to their health and safety. These residents are potentially identifiable because they have formed community activist groups fighting the use of these systems in their cities. 
4. Some police departments have determined that they sustained a financial lose because the expense of installing and using the system did not result in improved outcomes, while the systems did not come near to meeting the performance metrics promised by the sales and marketing department.",yes,maybe,no,no,yes,"race, financial means, geography","Investigations conducted by organizations like the ACLU have found that ShotSpotter false alarms send police on numerous trips into communities for no reason and on high alert.

Microphones are disproportionately installed in minority and low-income neighborhoods, increasing police engagement in those areas. ",yes,yes,yes,True,yes,,2008,,,True,no,maybe,,,US,North America,,," The ShotSpotter technology has been deployed in more than 130 cities across the US, as of March 2022. The earliest use mentioned in the reports goes back to 2008 by the San Francisco PD. 

It is unclear if there is AI in the sensors that are installed in neighborhoods (i.e. 'embedded AI') or if the AI processing occurs on software installed outside of the microphones--or both options are occurring.","ShotSpotter, SST, Troy Police Patrol, Charlotte Mecklenburg Police Department, Residents in neighborhoods with ShotSpotter microphones, Various US Police Departments, ShotSpotter Inc., City of Chicago Inspector General, Surveillance Technology Oversight Project, MacArthur Justice Center, Brighton Park Neighborhood Council, Lucy Parsons Lab, Organized Communities Against Deportation, Low-income and minority neighborhoods, Residents in affected neighborhoods, ShotSpotter analysts, Michael Williams, Action Center on Race and the Economy, Adam Toledo",1,0,True,"It is possible that more lives lost could be attributed to the ShotSpotter's performance.  Currently, the reports only mention one loss.","The ShotSpotter system records all loud noises, using at least three microphones to locate gunshots within a 25-meter radius. Then, SST staff review each report to make sure the computer flags only gunshots. The technology's accuracy depends on everything from ""topography, temperature, humidity and wind speed, as well as the trained ears of employees.""","microphone inputs, audio",law enforcement,yes,Autonomy3,"ShotSpotter detects loud noises and sends the information to human reviewers employed by ShotSpotter. Reviewers have the option to change the detection results from the system.  After review, the gunshot detections are sent to police departments.",No. Not intentionally designed to perform harm,microphones,"detect gunshots, locate gunshots, identify weapon calibre, predict shooter movement",,
CSETv1,113,True,113,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,yes,race,,yes,yes,yes,True,yes,,2021,08, ,True,no,no,,,,Global,,,date based upon publication date of the report,"Facebook, Facebook Watch, Black people, The Daily Mail, Facebook users, Content classifier",0,0,False,,"AI tool that is part of Facebook's content recommender system. It classifies and labels the content of Facebook posts, in this case a video. ","video, images",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"video suggestion, video classification",,
CSETv1,114,True,114,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,race,,yes,yes,yes,True,yes,The ACLU's test demonstrated Rekognition's disproportionate inaccuracy on the faces of people of color. ,2018,07,,False,no,no, ,,US,North America,,,ACLU's report was published in July 2018. ,"Amazon, Members of Congress, Black people, Rekognition, ACLU",0,0,False,,,"arrest photos, images, facial images","information and communication, law enforcement",yes,Autonomy1,"According to the report, Rekognition is used by the Sheriff's department in Oregon. ",No. Not intentionally designed to perform harm,,facial recognition,,
CSETv1,115,True,115,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"sex, sexual orientation or gender identity",,no,yes,yes,True,yes,There is no evidence or indication that the system led to any special interest intangible harms through its use or deployment. ,2020,07, ,False,no,no,,,US,North America,,,date based upon publication date of report,"Genderify, Arevik Gasparayan, Women and non-binary people",0,0,False,,"Genderify was designed to ""identify a person's gender by analyzing their name, username or email address"" by using ""data based on sources such as governmental and social network information.""","names, usernames, email addresses",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,gender classification,machine learning,
CSETv1,116,True,116,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"The implementation of the AI-powered camera system has led to financial loss for drivers, who get penalized for alleged unsafe driving behavior. However, the cameras regularly punish drivers for so-called ""events"" that are beyond their control or don't constitute unsafe driving, such as looking at a side mirror or fiddling with the radio, stopping ahead of a stop sign at a blind intersection, or getting cut off by another car in dense traffic.

 Delivery drivers' weekly bonuses and rewards are determined in part by the number of ""events"" they are found to be a part of by the Netradyne cameras. This is a tangible financial harm caused by AI.",no,no,no,no,no,none,,no,yes,yes,True,no,,2021, ,,False,no,yes,,,US,North America,transportation,,,"Amazon van surveillance cameras, Netradyne, Safety cameras, Amazon, Amazon delivery drivers",0,0,False,,"Netradyne cameras have ""four lenses that record drivers when they detect ""events"" such as following another vehicle too closely, stop sign and street light violations, and distracted driving. When the camera detects an ""event,"" it uploads the footage to a Netradyne interface accessible to Amazon and its delivery companies, and in some instances, a robotic voice speaks out to the driver: ""distracted driving"" or ""maintain safe distance."" Each time the camera registers an event, footage is uploaded into a system, recorded, and affects a score drivers receive at the end of the week for safe driving. For many Amazon drivers, these performance scores determine whether they receive weekly bonuses, prizes, and extra pay.""","camera input, video input, biometrics, video","wholesale and retail trade, transportation and storage",no,Autonomy1,,No. Not intentionally designed to perform harm,camera,"driver surveillance, safe driving detection, event detection, eye tracking, object recognition, event prediction","computer vision, Sensor Data Processing",
CSETv1,65,True,65,,4. Peer review complete,002,False,no,yes,no,no,yes,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,no,none,,no,yes,yes,False,no,,2016,12,21,True,no,no,,,US,Global,,,6.1-6.3 - The date refers to the publication of the blog post from OpenAI about its experiments with reinforcement learning algorithms on rewards in games.,"CoastRunners, Reinforcement learning agent, OpenAI, OpenAI's Universe",0,0,False,,"A reinforcement learning AI system that controls a player in the game CoastRunners.  The game operates in OpenAI's test environment, Universe.",,"Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,none,optimize movement to win a computer game,reinforcement learning,
CSETv1,82,True,82,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,"This incident is around the flagging of Facebook posts about the killing and injuring of protesters.  While the protestors did experience tangible harm, Facebook and the AI it uses are not directly linked to the tangible harm.  However, they are linked to the special interest intangible harm of detrimental content.",no,no,no,yes,no,none,"Facebook, which uses a hybrid AI/human system to flag false news, erroneously labeled postings, about the killing and injuring of protesters, as false information.  This incorrect labeling is false information and could serve as propaganda that makes it appear like physical harm did not occur.",yes,yes,yes,True,yes,,2020,10,20,False,no,no,,,,Global,,,,"Third-party fact-checking organizations  (ex. Africa Check Nigeria, AFP Nigeria, Dubawa), Facebook and Instagram users interested in the Lekki Massacre incident, Meta (former: Facebook), Facebook content moderation AI, Instagram, Facebook",0,0,False,"While tangible deaths and injuries did occur, those physical harm events are not directly linked to the AI",a system that identifies false information in Facebook posts,"text, Facebook posts, social media posts",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"content moderation, identification or detection",,
CSETv1,83,True,83,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"nation of origin, citizenship, immigrant status, geography","4.4 and 4.6- Spam filters (specifically Microsoft Outlook's and SpamAssassin's) often mistakenly identify emails containing words like ""Nigeria"" and ""Ivory Coast"" as spam even if they are valid emails. Emails containing words referring to other nationalities are not identified as spam at the same rates.",yes,yes,yes,True,yes,,2020,,,True,no,no,,,,Global,,,,"AlgorithmWatch, Gmail, Yahoo Mail, Google, Yahoo, GMX Mail, GMX, Outlook, Microsoft, LaPoste, SpamAssassin, Apache Software Foundation, email users of Gmail, Yahoo Mail, GMX Mail, Outlook, or LaPoste",0,0,False,,Spam filter,"text, emails",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"spam filter, classification",,
CSETv1,87,True,87,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,maybe,maybe,no,no,yes,"race, sex","The report focused on differential treatment to black women.  However, it also showed differential treatment based individually on gender and lightness of skin tone.",yes,yes,yes,True,yes,,2020,10,08,True,no,no,,,GB,Europe,,,,"UK Home Office, female UK passport applicants, dark-skinned UK passport applicants, Elaine Owusu, BBC, UK Passport Photo Checker",0,0,False,,"People applying to get a passport must pass an automated check to ""detect poor quality photos which do not meet Home Office rules"" including ""having a neutral expression, a closed mouth, and looking straight at the camera.""","images, photos",public administration,yes,Autonomy2,Passport photo submitters could contest the AI system's assessment and get it over-ridden,No. Not intentionally designed to perform harm,,"passport photo quality check, classification","facial recognition, machine learning",
CSETv1,88,True,88,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,yes,religion,,yes,yes,yes,True,yes,"The incident is a result of malicious actors exploiting a functionality/vulnerability in the Google Search algorithm to display harmful content as search results. While the AI system is not the primary cause for the harm, its susceptibility to manipulation is a vulnerability that is directly linked to the AI. ",2017,09,,True,no,no,,,,Global,,,"6.4 - Incidences of images of portable ovens being associated with the term ""Jewish baby stroller"" have been found on sites like 4chan as early as 2017. However, the Google image search result for this incident drew attention in September of 2020.","Google, Anti-Semitic extremists, Jewish people, Google search, Network Contagion Research Institute",0,0,False,,Search engine,"search terms, keywords, Text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"search optimization, search engine",,
CSETv1,91,True,91,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,"An algorithm for prioritizing COVID vaccine distribution was developed by ethicists and experts.  However, the algorithm failed when there was no data entered from the location into the algorithm.  This resulted in few medical residents getting the vaccine although they were high-risk.  Therefore there was tangible harm to health. However, there is no evidence that the algorithm was AI and not just an expert-based formula.",yes,no,no,no,no,age,"Access to health services is a critical public service.  This poorly executed algorithm affected access to the COVID vaccine, a health service. Residents may have received a lower priority assignment because their ages did not fall below 25 or above 65, ranges determined to be at higher risk for COVID.
",no,no,yes,True,no,,2020,12,,False,no,no,Palo Alto,CA,US,North America,,,,"Stanford Medicine vaccine allocation algorithm, Stanford Medical residents, Stanford Medical residents, Stanford Medicine",0,0,False,,COVID-19 vaccine allocation algorithm,"CDPH guidelines, age, job role, department, medical personnel data",human health and social work activities,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,not AI
CSETv1,92,True,92,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,There was a gender bias in the rates and credit limits offered by the Apple card.  This results in financial harm based on gender.,no,no,no,no,yes,sex,,yes,yes,yes,True,yes,,2019,11,,False,no,no,,NY,United States,North America,,,,"Goldman Sachs, New York State Department of Financial Services, Apple, Apple Card, Apple Card female users, Apple Card female users",0,0,False,,Financial algorithm determining users' credit limits. ,"personal debt, personal data, credit report, income",financial and insurance activities,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,"According to Goldman Sachs and the investigation by the NYDFS the algorithm's output is explainable, which points to a simple machine-learning algorithm rather than deep learning. "
CSETv1,125,True,125,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,maybe,no,no,no,none,"Potential violation of Economic, social and cultural rights in the International Bill of Human Rights mandates the right to 'safe and healthy working conditions' as well as the right to 'rest, leisure and the reasonable limitation of working hours'. ",maybe,yes,yes,True,maybe,"Potential violation of Economic, social and cultural rights in the International Bill of Human Rights mandates the right to 'safe and healthy working conditions' as well as the right to 'rest, leisure and the reasonable limitation of working hours'. ",2014,,,False,yes,yes,,,US,North America,,,"Although it is unclear if the warehouse robots and the worker monitoring system interact at a system level, their simultaneous use in Amazon's fulfillment center means that workers interact with and navigate an environment shaped by both.","Amazon, Amazon warehouse robots, Reveal from The Center for Investigative Reporting, Barcode scanner, Warehouse workers, OSHA, Amazon fulfillment centers",0,55000,True,"Extrapolated: 14,000 serious injuries reported in 2019, equivalent to an injury rate of 7.7/100 employees. Similar injury rate reported for 2018, following a ~10% annual increase in 2016 and 2017. No earlier data available. The number of non-serious injuries is likely substantially higher, but no hard data is available in the reports. There is also evidence of underreporting of injuries. 8.2 should be considered a most conservative estimate. ","Two AI systems of note.
1) Worker monitoring system integrated in workers' handheld RFID scanners. Tracks time in between activity (scans) which is used to assess worker performance, assigns performance targets and automatically penalizes workers if targets are not met.  The device also assigns workers products to collect from the warehouse based on their real-time location. 
2) Warehouse robots that move shelves towards workers in the warehouse. ","Product Info, Warehouse data, Employee Info, GPS, video, Time off Task (TOT), RFID","wholesale and retail trade, transportation and storage",no,unclear,,No. Not intentionally designed to perform harm,"Handheld barcode scanner; flat, round mobile robots;","delivery , navigation, move shelves, worker monitoring, performance tracking",,
CSETv1,126,True,126,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,,no,no,no,no,no,none,,no,no,yes,True,no,,2021,07,16,False,no,yes,London, ,GB,Europe,food and agriculture,,"The technology is not AI-powered, but it is still embedded in physical systems. More than 2300 individual robots interact in the fulfillment center. ","Ocado, Ocado fulfillment center in Erith , Ocado customers",0,0,False,,Not an AI system. Fulfillment center robots controlled by a wireless protocol that collect customer items (groceries). ,"geospatial data, sensor","wholesale and retail trade, transportation and storage",no,Autonomy1,,No. Not intentionally designed to perform harm,fulfillment robot,,,Not AI.
CSETv1,132,True,132,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,maybe,yes,no,none,,yes,yes,yes,True,yes,,2020,12,,False,no,no, ,,,Global,,,TikTok banned content labeled with hashtags promoting eating disorder habits on its platform in the summer of 2020. But in December 2020 the Guardian published an investigative report demonstrating that this type of content was still easily searchable on TikTok.  ,"TikTok accounts posting content promoting unhealthy eating habits, TikTok, ByteDance, TikTok's content moderation algorithm, TikTok users",0,0,False,,"Content moderation system that should identify and remove pro eating disorder content from the app, and identify related search terms to direct users to help pages.","TikTok posts, video, text, audio","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"content suggestion, content moderation",,
CSETv1,133,True,133,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,"3.1 By TikTok deleting her account, Rose lost access to the money accrued in her TikTok Creator Fund. 
3.2 Content on TikTok is automatically removed when enough users have flagged it as violating community guidelines. This appears to be a rules-based automation rather than AI-powered. ",no,no,no,maybe,yes,"disability, race, sexual orientation or gender identity","4.4-4.6 - TikTok accounts run by creators who are BIPOC, LGBTQ+, and disabled are wrongfully reported by discriminatory viewers for detrimental content, which negatively affects their ability to stay on the platform.

Trolls produced detrimental comments and content about Montoya's TikTok postings.",yes,no,yes,True,no,"Rose experienced discrimination and hate speech in the comments responding to her content. The reporting of her videos as violating TikTok's content standards is also driven by disciminatory users. 

Montoya did experience intangible harm.  However, that harm cannot be directly linked to an AI system.  It can be linked to an automated algorithm and the behavior of other TikTok users.",2020,12,14,False,no,no,,,,Global,,,,"Rosalynne Montoya, Rosalynne Montoya, TikTok, ByteDance, Internet trolls, Minority content creators",0,0,False,,"Unclear if an AI system is involved. Automated content review, removal and appeal system in response to user reports of detrimental content. ","TikTok posts, TikTok accounts, user reports","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"content moderation, enforcement of community guidelines",unclear,It is unclear that this technology is AI rather than an automated system.
CSETv1,134,True,134,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,maybe,yes,True,unclear,Unclear if there was AI in the robot.,no,no,no,no,no,none,,no,maybe,yes,True,no,,2020,12,25,False,no,maybe,Fuzhou,Fujian,CN,Asia,commercial facilities,,,"Fuzhou Zhongfang Marlboro Mall, Shopping guide robot, Unidentified shoppers",0,2,False,,It is unclear what the shopping guide robot's tasks are or how it was designed.,Unclear,wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,4ft tall vaguely humanoid droid on wheels,,,
CSETv1,122,True,122,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,no,no,maybe,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"An entity experienced tangible harm. Facebook experienced a tangible financial loss in a class-action lawsuit.

Facebook paid a fine for implementing a facial recognition tool that stored users' biometric data without consent. 
",no,yes,no,no,no,none,"Facebook's guilty verdict indicates that the Illinois citizens' privacy rights had been violated.

",yes,yes,no,True,yes,"
",2015, ,,True,no,no,,IL,US,North America,,,2020 refers to the date during which Facebook agreed to pay the class-action lawsuit settlement.,"Facebook users in Illinois, Facebook, Tag suggestion tool",0,0,False,,The Tag Suggestion tool uses facial recognition to identify individuals on users' photos and offers suggestions for tagging their Facebook profiles to the image.  ,"photos, facial images, biometric data","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,facial recognition,"computer vision, classiification",
CSETv1,123,True,123,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,yes,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"Epic's sepsis prediction algorithm missed two-thirds of sepsis cases, rarely found cases medical staff did not notice and frequently issued false alarms. The study did not elaborate on the consequences for patient's health outcomes, but there is a potential that due to the system's failure to alert to a sepsis case, patients received less or delayed care. ",no,no,no,no,no,none,,no,yes,yes,True,no,,2017, ,,True,no,no,,,US,North America,healthcare and public health,,"Annotator 1: 

 The date refers to the study/research and does not encapsulate the length of time of deployment of the Epic Sepsis Model

Annotator 2: 

 First implementation of the model in 2017. ","Epic Systems, Epic Sepsis Model, US hospitals, Researchers from the University of Michigan Medical School, University of Michigan hospital patients",0,0,True,"The model failed to alert to sepsis for 1,709 (67%) out of the 2,552 patients that developed sepsis (out of the 38,455 patients under study). It is unclear if these patients had poorer health outcomes as a result. ",The Epic Sepsis Model calculates and indicates the probability of a likelihood of sepsis to help clinicians identify hard-to-spot cases.,"electronic health records, patient data",human health and social work activities,yes,Autonomy3,"Annotator 1: 

 9.4 - some of the hospitals that deployed SEM were likely public hospitals.
9.5 - The tool is able to summarize and look for indications of sepsis, but the final decisions on care and patient treatment are made by doctors. They may take the tool's recommendations into consideration.",No. Not intentionally designed to perform harm,,"identify sepsis, diagnose sepsis, sepsis prediction",large language model,
CSETv1,93,True,93,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,yes,no,no,yes,"age, disability, religion, race, geography, sex, familial status (e.g., having or not having children) or pregnancy, nation of origin, citizenship, immigrant status","HUD alleged that Facebook restricted who saw ads based on users' age, gender, zip code, religion, citizenship, and more.",yes,yes,yes,True,yes,,2019,03,,False,no,no,,,US,North America,,,,"U.S. Department of Housing and Urban Development, Facebook, Real estate advertisers on Facebook, Facebook users, Facebook users, Meta, Facebook advertising algorithms",0,0,False,,Ad-serving algorithm,user data,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,
CSETv1,25,True,25,,4. Peer review complete,001,False,yes,no,no,yes,no,yes,no,no,yes,"no tangible harm, near-miss, or issue",yes,maybe,False,none,"The two autonomous cars were able to avoid each other successfully. Thus, there is no adverse outcome for the technology to link to. Since there was no harm, there is no potentially identifiable specific entity that experienced harm. This incident may represent the success of autonomous driving systems to detect and avoid accidents, even with each other.",no,no,no,no,no,none,,no,yes,yes,False,no,,2014,06, ,False,yes,yes,Palo Alto,CA,US,North America,transportation,,,"Google, Delphi Technologies, Google autonomous Lexus RX400h, Delphi autonomous Audi Q5 Crossover, John Absmeier, AI for self-driving vechiles",0,0,False,,Autonomous driving software,"road data, video, lidar, traffic, camera input, radar, spatial data",transportation and storage,no,Autonomy2,"At least in the Delphi car, a human was present as a passenger with the ability to observe and override the system's decisions in real-time.",No. Not intentionally designed to perform harm,"Vehicles (Audi Q5 Crossover, Lexus RX400h Crossover)","autonomous driving, self driving, autonomous navigation",,
CSETv1,71,True,71,,4. Peer review complete,001,False,yes,no,no,yes,no,yes,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,A Google driverless car collided with a bus ,no,no,no,no,no,none,,no,yes,yes,True,no,,2014,10, ,False,no,yes,Silicon Valley,CA,US,North America,transportation,Driverless car,"This incident describes a variant with several events.

The first event occurred in October 2014, when a Delphi autonomous vehicle was broadsided by another car while waiting to make a left turn.

On February 14, 2016, in Mountain View CA, a self-driving Lexus SUV developed by Google detected a pile of sandbags surrounding a storm drain in its path and moved to the center lane to avoid the hazard. Three seconds later, it collided with the right side of a city bus.

On September 23, 2016, in Mountain View, CA, a commercial van running a red light struck one of Google's autonomous Lexus SUVs as it crossed an intersection.

On May 14, 2018, in Phoenix, AZ, an autonomous car being tested by Google-owned Waymo swerved to avoid another car and left the human operator with minor injuries.","Delphi Technologies, Delphi Audi SQ5, Unnamed car, Google, Google Lexus SUV, Mountain View city bus, Lexus test driver, Google Lexus RX450 SUV, Commercial van, Waymo, Waymo Chrysler Pacifica, Waymo self-driving car human operator, Honda sedan and driver",0,1,False,,"Self-driving vehicles are designed to autonomously navigate roads and respond to obstacles and situations in real time. They are likely equipped with lasers, radar, cameras, and computer software designed to enable the vehicle to drive itself, with a person at the wheel as backup.","camera input, laser input, radar, spatial data",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,"Vehicles (Lexus, Audi, Chrysler Pacifica)","autonomous navigation, self-driving","computer vision, object detection",
CSETv1,89,True,89,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,no,True,none,"There was tangible harm because people died.  However, the harm was indirectly, not directly linked to the AI algorithm in YouTube.  Many people watch extreme content on YouTube but do not commit violence. Thus, there is not a direct link from the AI to the tangible harm.",no,no,no,yes,yes,"religion, race",The terrorist watched and shared detrimental white supremacist and islamophobic content content on Youtube and other web forums and social media sites.,yes,yes,yes,True,yes,"The “step-ladder of amplification” is, in part, a byproduct of the business model for YouTube. This amplification process resulted in a terrorist being exposed to large amounts of extremist content associated with differential treatment based on race and religion. Thus, the YouTube AI is directly linked to detrimental content.",2019,03,15,False,no,no,Christchurch,,NZ,Oceania,,operationally representative,,"Christchurch shooter, YouTube, YouTube recommender, YouTube users exposed to far-right propaganda, Muslims, Victims of Christchurch terrorist attack",51,40,False,The numbers refer to those killed and injured during the terrorist attack on two mosques. They are not considered to be directly linked to AI. ,Content recommender system,youtube history,"information and communication, Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,recommender,,
CSETv1,94,True,94,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"There are two instances of harm in this incident. First, the unfavorable court ruling against Deliveroo required them to pay  €50,000 to the suing parties, which is a tangible harm. Second, the finding that the algorithm to determine rider reliability was discriminatory against valid reasons not to work such as illness. This presents a non-imminent risk of tangible harm because riders would have their status demoted and potentially suffer financial loss. However, the incident does not list any real-life instances of this occurring, and the case was mostly hypothetical. The most severe level of harm is tangible harm, which is what is indicated in this question.",no,yes,no,no,no,other," 4.2 - The ruling indicated that Deliveroo violated riders' right to not work because of injury. The court also found that the algorithm also differentially treated workers with a reasonable reason to cancel work on short notice, e.g., illness.  This is a differential treatment, but one that is not associated with a protected characteristic.",yes,yes,yes,True,yes,"An Italian court found that it violated workers' right to strike.  The right to strike is part of the freedom of association, a UN-identified human right.  

The court also found that the algorithm also differentially treated workers with a reasonable reason to cancel work on short notice, e.g., illness.  This is a differential treatment, but one that is not associated with a protected characteristic.",2021,01,,False,no,no,,,IT,Europe,,,,"Deliveroo workers, Deliveroo workers, Deliveroo, Bologna court, Deliveroo workers with legitimate reasons for cancelling shifts, Delivery reputation-ranking algorithm",0,0,False,,"Algorithmic scheduling system that allows high-performing workers to choose shifts first. Performance includes workers' reliability ranking, which is reduced if they cancel a shift less than 24 hours ahead. ","reliability index, shift data, worker performance data",accommodation and food service activities,no,Autonomy1,,No. Not intentionally designed to perform harm,,"shift assignment, rank the reliability of workers",machine learning,
CSETv1,96,True,96,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,no,yes,True,none,3.5 - the value-added measurement/modeling is not AI - it is a statistical model,no,yes,no,no,no,none,"""A proprietary system that measures teacher performance based on student test scores may violate teachers’ civil rights because they can’t verify the results are accurate, a federal judge ruled, advancing a lawsuit against Texas’ largest school district.""",yes,no,yes,True,no,,2012,,,False,no,no,Houston,TX,US,North America,Other,," The lawsuit was first filed in 2014 after the system was implemented in 2012 and used until 2016. U.S. Magistrate Judge Stephen Smith sided with teachers in a decision on May 4, 2017.","SAS Institute, Educational Value-Added Assessment System (EVAAS), Houston Independent School District teachers, Houston Independent School District teachers, Houston Independent School District",0,0,False,,EVAAS is a teacher performance assessment system that tracks teachers’ impact with a proprietary algorithm that compares their students’ test results to the statewide average for students in that grade or course.,"student data, student test results, standardized test results, statewide average test scores, worker performance data",Education,yes,Autonomy3,The value-added system provides data about teachers which is used by the school district to make firing decisions.,No. Not intentionally designed to perform harm,none,prediction,response model,not AI
CSETv1,102,True,102,,4. Peer review complete,002,False,no,no,no,yes,yes,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,yes,"race, geography",Voice recognition software's performance varied depending on the speaker's regional accents and race. Speech recognition performed worse for African American vernacular. ,yes,yes,yes,True,yes,,2020,03,23,True,no,no,,CA,US,North America,,,"Date and location information is based on the publication of the study, not a specific occurrence of ASR systems misunderstanding African-American voices.","Speakers with regional accents, Black Americans, Microsoft automated speech recognition, Microsoft, Apple automated speech recognition, Apple, IBM automated speech recognition, IBM, Google automated speech recognition, Google, Amazon automated speech recognition, Amazon, Allison Koenecke, Andrew Nam, Emily Lake, Sharad Goel",0,0,False,,"Automated speech recognition (ASR) systems that are used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing","speech, voice","administrative and support service activities, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"speech-to-text, automated speech recognition",machine learning,
CSETv1,110,True,110,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,"The report states that a patient in Idaho had to be hospitalized following the cut in at-home care hours assigned by the algorithm. However, the incident talks about an 'algorithm' and does not mention AI, Machine Learning, or models. It is likely an expert based on statistics serviced algorithm and therefore does not meet the CSET definition for AI.",yes,yes,no,no,no,none,"Access to health care is a UN-defined human right. Therefore, this is a human rights violation--even if it does not meet the definition of CSET AI harm because CSET's definition for AI was not met.",yes,no,yes,True,no,"The first incident just talks about an 'algorithm' and does not mention AI, Machine Learning, or models.  It is likely an expert based on statistics serviced algorithm and therefore, does not meet the CSET definition for AI. Therefore, it does not meet the CSET definition of AI intangible harm.",2011,,,False,no,no,,AR; ID,US,North America,healthcare and public health,,Idaho developed and implemented their algorithm to determine at-home care needs in 2011. Arkansas deployed their algorithm in 2016,"InterRAI, Idaho welfare recipients, Bradley Ledgerwood, Tammy Dobbs, Arkansas Medicaid waiver program beneficiaries, Arkansas Department of Human Services, Arkansas healthcare workers, Unnamed  beneficiary of the Medicaid waiver program, Legal Aid ",0,0,True,It is unclear how many injuries or hospitalizations were caused because of the hour cuts mandated by the algorithm,The algorithm allocates medicate services based on recipient data.,"symptoms, health data",human health and social work activities,yes,Autonomy1,,No. Not intentionally designed to perform harm,,,,
CSETv1,118,True,118,,4. Peer review complete,002,False,no,no,no,yes,no,yes,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,yes,religion,,yes,yes,yes,True,yes,,2021,01,18,True,no,no,,,US,North America,,,The date reflects the publication date of the study.,"Researchers from Stanford and McMaster universities, GPT-3, Muslims, Jewish people, OpenAI",0,0,False,,GPT-3 is a large language model that generates text,text,"information and communication, professional, scientific and technical activities",no,Autonomy1,,No. Not intentionally designed to perform harm,,text generation,"natural language processing, transformer",
CSETv1,119,True,119,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,Big data analytics was used for the harm. AI is not equivalent to big data analytics. ,no,no,no,no,no,none,,no,no,yes,True,no,,2021,07,31,False,no,no,,,RU,Europe,,,,"Xsolla, Xsolla employees",0,0,False,,Big data analytics used to assess employees' work activity.  ,"employee activity, employee performance data",administrative and support service activities,no,Autonomy3,,No. Not intentionally designed to perform harm,,,,Not AI. 
CSETv1,120,True,120,,4. Peer review complete,002,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,no,no,none,"4.4 - ""Many of the bot's responses were harmless and amusing...but others promoted conspiracy theories and discussed extremely sensitive topics."" The discussion of conspiracy theories and sensitive topics does not constitute detrimental content, even if the behavior of the bot was questionable.",no,yes,yes,True,no,,2020,09,,True,no,no,,,US,Global,,,"Bot was discovered on October 4th, had been active for over a week. ","Murat Ayfer, GPT-3, Reddit, OpenAI, thegentlemetre, Philosopher AI",0,0,False,,The bot thegentlemetre was a text generator powered by the app Philosopher AI which is built on GPT-3,text,"Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,text generation,"GPT-3, large language model, natural language",
CSETv1,121,True,121,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"It is unclear that the autonomous drone killed or injured anyone. However, it is certain that the drone was used to ""hunt[ed] down and remotely engage[d]"" retreating [Haftar-affiliated forces] which indicates an imminent risk of tangible harm, especially considering it was not being supervised by a human.",no,no,no,no,no,none,,no,yes,yes,True,no,,2020,03, ,False,no,yes,Tripoli,,LY,Africa,defense-industrial base,,,"STM (Savunma Teknolojileri Mühendislik ve Ticaret A.Ş.), Soldiers loyal to the Libyan General Khalifa Haftar, Forces backed by the government based in Tripoli, Kargu-2 drone",0,0,True,It is unclear if and how many people were wounded or killed during the encounter. ,"The Kargu is a lethal, autonomous, ""loitering"" drone that can use machine learning-based object classification to select and engage targets.","geospatial data, sensor data, video",defense,no,Autonomy1,,Yes. Intentionally designed to perform harm and did create intended harm,drone,object classification,"computer vision, machine learning",
CSETv1,128,True,128,,6. Complete and final,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,no,True,no,,2017,,,False,no,yes,Redmond,WA,US,North America,,,,"Tesla, Tesla Autopilot, Eric Horvitz, Tesla sedan",0,0,False,,Tesla's Autopilot is designed to autonomously navigate roads and respond to real time situations on the road.,"sensor data, video",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,car,autonomous driving,computer vision,
CSETv1,145,True,145,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,yes,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,Stopping incorrectly could cause accidents on the road.,no,no,no,no,no,none,,no,yes,yes,True,no,,2021,07,22,False,no,yes, ,,US,North America,transportation,,,"Tesla, Tesla vehicle, Tesla self-driving function, unnamed Tesla driver",0,0,False,,Tesla's full self-driving technology is meant to autonomously navigate and respond to ongoing situations on the road. This includes reacting to traffic lights and stop signs.,"camera input, spatial data",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,"Tesla vehicle (Model 3, Model S, Model X)",autonomous navigation,,
CSETv1,146,True,146,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,yes,"race, sex, sexual orientation or gender identity, nation of origin, citizenship, immigrant status, religion, geography","Especially in the beginning, Delphi applied moral judgments to neutral identifiers based on qualities like race. For example, it said ""being a white man"" is more morally acceptable than ""being a black woman."" Since then, Ask Delphi has been updated with fixes to better detect offensive speech and guard against statements implying racism and sexism.",yes,yes,yes,True,yes,"The system produced biased content, reproduced stereotypes, and declared genocide as morally acceptable.",2021,10,14,False,no,no, ,,,Global,,,"Ask Delphi was launched on October 14th, 2021. It was developed at a research institute in the United States but the website can be accessed outside of the United States as well.","Ask Delphi users, Ask Delphi users, Mechanical Turk reviewers, Allen Institute for AI, Ask Delphi",0,0,False,,Ask Delphi is a neural network (mathematical system loosely modeled on the web of neurons in the brain) that analyzed more than 1.7 million ethical judgments by human reviewers from Mechanical Turk from everyday scenarios from websites and other sources. ,"ethical judgments, ethical scenarios",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"ethical decision making, reply to questions","natural language processing, neural network",The researchers developed a model to make moral judgments - which by definition will be offensive to some - and released it knowing of its inconsistencies and biases. 
CSETv1,152,True,152,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"Customers who purchased Pepper received a subpar product that did not meet their expectations or contractual terms. They, therefore incurred financial loss. ",no,no,no,no,no,none,,no,yes,yes,True,no,,2014,,,True,no,yes,  ,,JP,Asia,,,,"Softbank Group Corp., Ittokai nursing-home operator, Tsutsumu Ishikawa, Pepper, Aldebaran, Nissei Eco Co.",0,0,False,,"Pepper was given a perky demeanor and programmed to grasp human emotions and engage in basic conversations. Different customers intended to use it for different purposes. For example, reciting scripture or leading elderly people in singing and exercises at a nursing home.","sensor data, audio inputs","administrative and support service activities, other service activities, financial and insurance activities, Arts, entertainment and recreation, human health and social work activities",no,Autonomy1,,No. Not intentionally designed to perform harm,Humanoid robot,"chant scripture, entertain guests, household administration, cheerleading, exercise demonstration, concierge","computer vision, natural language processing",
CSETv1,153,True,153,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"3.3 - Criminal charging documents do not mention Autopilot. However, the National Highway Traffic Safety Administration confirmed that Autopilot was in use in the Tesla at the time of the crash.",no,no,no,no,no,none,,no,yes,yes,True,no,,2019,12,29,False,no,yes,Los Angeles,CA,US,North America,transportation,,,"Tesla, Tesla Model S vehicle, Tesla Autopilot, Gilberto Alcazar Lopez, Maria Guadalupe Nieves-Lopez, Kevin George Aziz Riad, Victim's Honda Civic, Unnamed female Tesla passenger",2,2,False,,"Autopilot can control steering, speed, and braking. ","sensor data, video input, video, sensor",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,2016 Tesla Model S,"semi-autonomous navigation, object detection, object recognition, autonomous driving",,
CSETv1,154,True,154,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"The AI's prediction of recidivism risk directly influences inmates' ability to participate in programs under the First Step Act. Participation in these programs can facilitate early release for inmates. Inaccurate predictions can, therefore extend an inmate's prison time, which is a harm to physical freedom and autonomy and falls under the 'other intangible harm' category. 

CSET annotators read the documentation for the program, verifying that the algorithm was AI.",yes,yes,no,no,yes,race,,yes,yes,yes,True,yes,,2021,12,,True,no,no, ,,US,North America,government facilities,,,"United States Justice Department, Black people in federal prison, Hispanic people in federal prison, Asian people in federal prison, Black people in federal prison, Hispanic people in federal prison, Asian people in federal prison, Pattern",0,0,False,"14,000 inmates were allocated to a wrong risk category. ",Recidivism risk prediction algorithm used to determine if inmates qualify for participation in resocialization programs under the First Step Act. ,"previous criminal history, personal information",law enforcement,yes,Autonomy3,,No. Not intentionally designed to perform harm,,"risk assessment, predict recidivism, predict recidivism risk",prediction,
CSETv1,127,True,127,,4. Peer review complete,002,False,no,yes,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,no,True,none,"This incident has reports that cover two distinct types of AI-related content.
#1 MSN journalists are fired and replaced by AI.  There was financial harm to the journalists, loss of income.  However, the firing is not directly linked to an output from an AI.  The discussed AI's function is to write news articles, not an output about firing or hiring.  If the AI discussed produced recommendations associated with HR, employee firing, or other similar decisions, then the AI could potentially be directly linked to the employee experienced harm.  However, this is not the case.

#2
This incident's second type of AI-related report is around the MSN AI journalist misidentifying a music celebrity.  This is potentially a special interest intangible harm.",no,no,no,no,yes,race,,yes,yes,yes,True,yes,,2020,06,09,False,no,no,,,,Global,,,The AI mixed up and misidentified images as Jade Thirlwall and Leigh-Anne Pinnock. This likely occurred because the AI was not good at verifying image identities and is more likely to incorrectly identify mixed race individuals,"Microsoft, Microsoft News, MSN.com, AI news editor, new editors, Jade Thirlwall, Leigh-Anne Pinnock",0,0,False,,an AI editor for news articles,"news articles, text, images","Arts, entertainment and recreation, information and communication",no,unclear,,No. Not intentionally designed to perform harm,,generation,"Natural Language Processesing, classification",
CSETv1,129,True,129,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,no,unclear,"The report describes the content as 'hate speech'.  Hate speech usually involves a protected characteristic.  However, the report does not give information on which protected characteristics were involved.",yes,yes,yes,True,yes,,2021,03,18,False,no,no,,,,Global,,,,"Facebook users, Facebook, Facebook content moderation AI",0,0,False,,"Facebook's automated moderation tools, powered by artificial intelligence, are intended to flag and remove posts containing hate speech and other detrimental content.","Facebook posts, text, images",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"content moderation, classification",natural language processing,
CSETv1,135,True,135,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,none,"The articles do not indicate actual events where the system has led to biased candidate assessments. However, the descriptions of the way the system is trained and the kind of information it evaluates from candidates' applications resulted in the university being concerned about the AI and the school halted usage.
",no,yes,no,True,no,,2013, ,,False,no,no,Austin,TX,US,North America,,,"6.1 - ""The system was used to organize graduate admissions in the Department of Computer Science between the 2013 and 2019 academic years.""","Austin Waters and Risto Miikkulainen, University of Texas at Austin Computer Science Department PhD applicants, Graduate admissions evaluator (GRADE), University of Texas at Austin Department of Computer Science",0,0,False,,Applicant screening tool for PhD applicants to the computer sciences department using logistic regression classification. ,"GPA, university previously attended, letters of recommendation, area of research interest, faculty advisor , university name, text, letter of recommendation, statement of interest",Education,yes,Autonomy3,"Annotator 1: 

 9.4 - The University of Texas at Austin is a public university.

Annotator 2: 

 Every application was assessed by one human reviewer in addition to the software tool at every stage of the selection process.",No. Not intentionally designed to perform harm,,assess applicants,"logistic regression model, logistic regression",Not AI. 
CSETv1,136,True,136,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,,no,no,no,no,no,none,,no,no,yes,True,no,,2020, ,,True,no,no,,,,Global,information technology,,,"Websites with ad spaces, Brand Safety Technology, Legitimate online media, Advertisers",0,0,False,,"Content classification system that determines whether the content of a website is 'safe' for brands to place their advertisements. Not AI, based on keyword lists. ","keywords, article text, webpage content, text","Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,brand safety detection,text scraping,Not AI. 
CSETv1,142,True,142,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"Ads placed on Instagram and Facebook are screened for policy violations before being placed visibly on the platform. Ads for adaptive clothing and accessories are repeatedly erroneously identified as medical products and rejected. While companies can have the decision reviewed, this often takes many days, which means there is a reasonable probability that they are losing traffic to their site and profit as a result. ",no,no,no,no,yes,disability,,yes,yes,yes,True,yes,,2021, ,,True,no,no,,,,Global,,,,"Mighty Well, Facebook and Instagram marketing algorithms, Yarrow, Small adaptive clothing companies, Mighty Well, Yarrow, Small adaptive clothing companies, Facebook, Meta, Ad-screening algorithm, Instagram",0,0,False,,Ad screening algorithm,"advertisements, images","wholesale and retail trade, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,"advertisement screening, detect policy violations",,
CSETv1,143,True,143,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,maybe,"geography, other","Twitter and Facebook users who speak Bosnian, Serbian, Montenegrin or Macedonian encounter more detrimental content (especially hate speech and violent content) because content moderation tools do not work effectively in their language. While users speaking these languages are not explicitly treated differently, harm is unevenly distributed based on geography and language. ",yes,yes,yes,True,yes,,2021, ,,False,no,no,,,RS,Europe,information technology,,,"Facebook, Twitter, Inc., Meta, Twitter, Content moderation algorithm, Balkan Investigative Reporting Network (BIRN), Twitter users of small language groups, Facebook users of small language groups",0,0,False,,Content moderation systems used on Twitter (now X) and Facebook to detect content that violates the platform's Community Standards,"Facebook posts, Twitter posts, text, images",information and communication,no,Autonomy1,"Complicated cases might be subject to human review, but according to a statement, Facebook now relies primarily on AI. ",No. Not intentionally designed to perform harm,,content moderation,,
CSETv1,144,True,144,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,yes,unclear,yes,yes,True,unclear,"It is possible that Antonio Radic may have lost some revenue from having his YouTube channel suspended for 24 hours, given its popularity. However, this is not explicitly mentioned in the reports. ",no,no,no,no,no,none,,no,yes,yes,False,no,,2020,06,28,False,no,no,,, ,Global,information technology,,,"Youtube content moderation algorithms, Agadmator (Antonio Radic), YouTube, Google, CMU researchers",0,0,False,,YouTube's content moderation algorithm,"Youtube videos, Youtube content, audio, video","Arts, entertainment and recreation, information and communication",no,Autonomy1,"Annotator 1: 

 It is unclear how much of the reviewing process goes through humans as opposed to AI.",No. Not intentionally designed to perform harm,,"content moderation, identify hate speech",,
CSETv1,131,True,131,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,yes,no,no,no,,"The reports discuss how exam monitoring software had many false detections of cheating. Because this can have a direct impact on test takers' ability to get a (permanent or provisional) law license, it directly affects access to employment. Test takers also did not receive access to the recording of their test or a specific accusation of wrongdoing, undermining their right to due process. Thus, this is a human rights violation. ",yes,yes,yes,True,yes,,2020,10,,False,no,no,,CA,US,North America,unclear,,,"California Committee of Bar Examiners, California bar exam proctoring software, ExamSoft, California bar exam takers",0,0,False,,monitoring tool for online exams,"video, audio","Education, professional, scientific and technical activities",yes,Autonomy3,"This should have operated at autonomy level 3.  However, it is unclear if any oversight or review of the AI's output was done. If that is the case, then it effectively operated at level 1.",No. Not intentionally designed to perform harm,,classification,"facial recognition, computer vision",
CSETv1,137,True,137,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,unclear,no,yes,True,unclear,"3.1 - The adverse outcome of this incident is a lack of transparency/legal dispute over whether the Israeli Tax Authority should be obligated to disclose the software/algorithm used to make tax decisions. It is unclear that any individual or business assessed by the algorithm was financially harmed or was forced to pay more taxes than they would have if assessed by a human. 
3.2 - It is unlikely that there was any AI involved in this incident. The algorithm likely does not use AI in complex decision making, but rather relies on a formula or set of equations to determine the tax.",no,maybe,no,no,no,none,"The report discusses an event where a black box algorithm was used to calculate property taxes in Israel. When asked to explain how the taxes were calculated--what formulas and processes were used, the government refused to provide the information.  This could be a denial of due process and, in the US, could be a civil rights violation. It is unclear if this would be a civil rights violation in Israel. ",maybe,no,yes,True,no,"Based on the report, it is unclear that an AI was involved. The event was reported in 2021 but occurred in 2014.  Thus, the algorithm is more likely to be a formula or rule-based calculation, not AI.",2014,,,False,no,no,,,IL,Asia,,,6.10 - Israel is a country in the Middle East that is geographically located on the continent of Asia.,"Moshe Har Shemesh, Israeli Tax Authority, Israel District Court, Israel Supreme Court",0,0,False,,Algorithm for determining property tax. ,"property data, income statements","real estate activities, public administration",yes,Autonomy1,It is unclear whether human reviewers were involved in the process in between the algorithm's decisions and the final result. ,No. Not intentionally designed to perform harm,,,,not AI
CSETv1,155,True,155,001,3. In peer review,001,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"This incident can be classified as an imminent risk of tangible harm (near miss) because harm would have occurred if not for atypical intervention. Users were directed toward ""precarious and dangerous situations with unplowed roads/dirt roads"" while trying to avoid highway closures.
 

 GPS navigation services GoogleMaps and Waze directed people to closed-off highways, mountain passes and lakeside roads to get around road closures during a severe snowstorm. ",no,no,no,no,no,none,,no,yes,yes,True,no,,2021,12,,False,no,no, ,CA; NV,US,North America,transportation,"inclement weather - snow, inclement weather and snow, seasonal road closures",Users in both Nevada and California were impacted by Google Maps failure to direct them effectively during the snowstorm.,"Google, Google Maps, Google Maps users driving during inclement weather, Waze, Wendy Becktold, Caltrans, Washoe County Sheriff’s Office",0,0,False,,Google Maps uses machine learning to predict traffic patterns based on GPS data and other users' reports in order to optimize routes for the primary user.,"traffic patterns, user traffic reports","transportation and storage, information and communication",no,Autonomy3,,No. Not intentionally designed to perform harm,,"navigation, route optimization","shortest-path algorithm, Dijkstra Algorithm",
CSETv1,156,True,156,,3. In peer review,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,no,True,none,"Annotator 1: 

 3.3 and 3.5 - The adverse outcome in this event, the suicides involving online purchases of the preservative chemical compound, cannot be directly linked to the AI. Although the e-commerce sites did include the products on their platforms and did suggest ""other products that customers frequently bought along with it"", this inclusion was not the direct reason for the adverse outcomes. However, the special interest intangible harm of detrimental content can be directly linked to the AI system since it recommended other suicide-related products to purchase along with the poison that customers bought.

Annotator 2: 

 Several people committed suicide using the products sold on Amazon, meaning tangible harm definitely happened. This led to Amazon's recommender system suggesting other products related to self-harm as 'Frequently bought together'. While this may have made it easier or quicker for vulnerable customers to find the products they were looking for, it does not seem likely that they would not have attempted self-harm had it not been for the recommendation. Therefore, the AI system cannot be directly linked to the tangible harm. ",no,no,maybe,yes,no,none,"4.3 - Some of the buyers who had killed themselves using the chemical compound included minors. However, they were not disproportionally treated or specifically targeted/affected.
4.4 - By recommending products also used in suicide, Amazon may have been promoting detrimental content that encouraged self-harm. In that case, the AI would be directly linked.
",yes,yes,yes,True,yes,"4.3 - Some of the buyers who had killed themselves using the chemical compound included minors. However, they were not disproportionally treated or specifically targeted/affected.
4.4 - By recommending products also used in suicide, Amazon may have been promoting detrimental content that encouraged self-harm. In that case, the AI would be directly linked.
",2019,,,True,no,no, ,,,Global,,,"Annotator 2: 

 The NYT investigation discovered 10 suicides linked to the chemical compound bought on Amazon since 2019. ","Amazon, Product recommender system, Vulnerable Amazon customers considering self-harm, Suicide victims, US House of Representatives",18,0,True,"Annotator 1: 

 8.1 - The New York Times identified 10 people who had killed themselves using the chemical compound after buying it through Amazon and 8 suicides involving eBay sales of the poison. 
8.3 - There are likely more people who used the chemical compound for suicide after purchasing it from e-commerce vendors.
",Product recommender system on large online retail platform. ,purchase history,wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,,recommendation,,
CSETv1,162,True,162,,3. In peer review,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event," 3.1 Test takers were wrongfully detained and deported after the voice recognition system flagged their tests as invalid. 
3.3 The harm would not have occurred without the use of voice recognition to detect cheating and is, therefore, directly linked to the AI system. However, it is also the result of massive wrongdoing on the side of both ETS and the UK Home Office, which are equally, if not more, responsible for the harm. 

ETS knew that cheating by proxy and remote testing was a problem. ETS decided to use voice recognition to identify instances of fraudulent test takers.  
To perform the fraud and allow test-takers who could not speak English sufficiently to pass, people running the test agencies would replace ALL of the voice recordings by testers, even those who could pass the test and were not paying to pass the test fraudulently.  Thus, even people who could pass the English test and were genuinely trying to legitimately pass the test, unknowingly had their test replaced by a recording of a different person.

ETS used voice recognition software to identify cheaters. Conceptually, this works to detect proxy test-takers but not remote testing. Moreover, because remote testing fraud is likely done by overwriting *all* test-takers' files with the same fake one, voice recognition cannot distinguish valid tests from fake ones. Being aware of this, ETS should never have deployed voice recognition in the first place or passed the results on to the government. Poor data management and record-keeping by ETS prevented both prosecution and appeal of the outcome of the voice recognition system's output. Finally, being equally aware of this, the UK Home Office should have never used the AI system's output as a base for its policy. ",no,yes,no,no,maybe,"nation of origin, citizenship, immigrant status","While there was no discrimination based on citizenship, the incident only affected immigrants to the UK. ",yes,yes,yes,True,yes,"Test takers had their visas wrongfully canceled after the voice recognition system flagged their tests as invalid. In addition, for several years after deportations had begun, test takers were denied the right to appeal the government's decision and the right to due process (because they did not receive access to the recordings used to identify them as cheaters). 

The harm would not have occurred without the use of voice recognition to detect cheating and is, therefore, directly linked to the AI system. However, it is also the result of massive wrongdoing on the side of both ETS and the UK Home Office, which are equally, if not more, responsible for the harm. 

ETS knew that cheating by proxy and remote testing was a problem. ETS decided to use voice recognition to identify instances of fraudulent test takers.  
To perform the fraud and allow test-takers who could not speak English sufficiently to pass, people running the test agencies would replace ALL of the voice recordings by testers, even those who could pass the test and were not paying to pass the test fraudulently.  Thus, even people who could pass the English test and were genuinely trying to legitimately pass the test, unknowingly had their test replaced by a recording of a different person.

ETS used voice recognition software to identify cheaters. Conceptually, this works to detect proxy test-takers but not remote testing. Moreover, because remote testing fraud is likely done by overwriting *all* test-takers' files with the same fake one, voice recognition cannot distinguish valid tests from fake ones. Being aware of this, ETS should never have deployed voice recognition in the first place or passed the results on to the government. Poor data management and record-keeping by ETS prevented both prosecution and appeal of the outcome of the voice recognition system's output. Finally, being equally aware of this, the UK Home Office should have never used the AI system's output as a base for its policy. ",2014,06,,True,no,no,,,GB,Europe,Other,,"A BBC Panorama investigation in 2014 revealed two London test centres were running fraudulent exams so people could falsely obtain a pass they could use to apply for a visa. This prompted the government to investigate and crack down on fraud, with results that were ineffective and caused the unjust deportation and investigation of many testers into 2022.

The date corresponds to the first reported detention of a test-taker, Nomi Raja, after the government was given the list of accused cheaters. ","UK Home Office, ETS, BBC Newsnight, Toeic test takers, Toeic test takers, Toeic test takers, Wahidur Rahman, Nomi Raja, Nomi Raja, Shakil Rathore",0,0,False,,Voice recognition system ,"speech, audio recordings","Education, public administration",yes,Autonomy3,"There were multiple 'human-in-the-loop' failures: The voice recognition software was used to determine if the same voice turned up on multiple test recordings. If a test was flagged, two ETS staff had to agree for it to be classified as ""invalid"".  Despite the incredibly high share of tests (97%) being flagged as suspicious or invalid, the list was passed on to the government who then (again, despite the unreasonably high number) decided to rely on it for its visa policy. ",No. Not intentionally designed to perform harm,,voice recognition,,
CSETv1,139,True,139,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,,yes,yes,yes,True,yes,,2021,01,21,True,no,no,,,,North America,,,Data refers to the publication of the study. ,"Amazon, Amazon search and recommendation algorithms, University of Washington researchers, Amazon users",0,0,False,,recommender and search algorithm for Amazon ,"Amazon account information, Amazon products, text",wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,,"recommender, search engine",search engine optimization,
CSETv1,140,True,140,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,yes,no,no,yes,race,"This differential treatment affects people's public education, which is a civil rights/liberty violation.",yes,yes,yes,True,yes,,2020,12,,True,no,no,Montreal,QC,CA,North America,,,The incident occurred for the first time in December 2020 at the University of Concordia in Montreal.,"ProctorU, University of Toronto, University of Toronto BIPOC students, Chelsea Okankwu, Concordia University, ProctorU, Examity, University of Toronto BIPOC students, Maame Adjoa",0,0,False,,exam monitoring system,video,Education,yes,Autonomy3,,No. Not intentionally designed to perform harm,,"exam proctoring, remote proctoring, identification or detection",computer vision,
CSETv1,164,True,164,,3. In peer review,001,False,no,maybe,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,yes,no,none,"The new algorithm’s heavy weighting of reshared material in its News Feed resulted in more harmful content on users' News Feeds overall, because misinformation, toxicity, and violent content are inordinately prevalent among reshares. ",yes,yes,yes,True,yes,,2018,01,,True,no,no,,,,Global,,,,"Facebook users, The Wall Street Journal, Facebook, Meta, News Feed algorithm, BuzzFeed, Meta's Integrity Team, Breitbart News, ABC News, Topix, Facebook users from Poland, Spain, India or Taiwan",0,0,False,,Algorithm determining the content shown on a Facebook user's News Feed ,"Facebook posts, Facebook reactions, Facebook likes, Facebook comments, Facebook reshares, text, images, user engagement, user comments",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"content recommendation, prediction, engagement optimization",,
CSETv1,157,True,157,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,maybe,True,unclear,The driver of a vehicle hit by an Amazon delivery van suffered life-threatening injuries that were expensive to treat. The driver is suing Amazon for compensation based on the argument that Amazon's performance-tracking AI causes its drivers to drive less safely. Not clear if this is a direct link between AI and harm.,no,no,no,no,no,none,,no,yes,no,True,no,,2021,03,,False,no,maybe,Atlanta,GA,US,North America,transportation,,,"Amazon, Tesla Model S, Amazon delivery van, Toyota Corolla, Harper Logistics LLC, Bryan Williams, Amazon delivery drivers, driver hit by Amazon delivery van, AI for tracking Amazon delivery driver performance",0,1,False,,An AI that monitors delivery driver performance,"speed, breaking, acceleration, delivery times, seatbelt usage, phone calls, texting, in-van camera input, driver performance metrics","transportation and storage, wholesale and retail trade",no,Autonomy1,,No. Not intentionally designed to perform harm,in-van cameras and sensors,,,
CSETv1,160,True,160,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"ourcommunitynow.com published a report about an unsafe 'penny challenge' going around TikTok.  The Alexa app, extracted the unsafe challenge from the website and suggested it to a 10-year-old as a physical challenge to attempt.  The child did not execute the challenge.",no,no,yes,yes,no,none,The suggestion from Alexa promoted an unsafe activity that could lead to self-harm. Hence the AI provided is detrimental content.,yes,yes,yes,True,yes,The suggestion from Alexa promoted an unsafe activity that could lead to self-harm. Hence the AI provided is detrimental content.,2021,12,26,False,no,yes,,,US,North America,,,,"Amazon, Alexa, Kristin Livdahl's daughter, ourcommunitynow.com, Echo Dot, Alexa users",0,0,False,,"Alexa is a virtual assistant technology that is deployed to help households complete basic tasks like creating shopping lists, searching the web, and more.","voice, websites",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,Amazon Echo Dot,"virtual assistant technology, suggestion, recommendation","ranking, Natural Language Processesing",
CSETv1,79,True,79,,3. In peer review,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,no,yes,True,none,There is no AI. The harm comes from a formula that uses race as a factor.,yes,yes,no,no,yes,race,4.1 - Black patients overlooked by the calculation because of built-in points had their access to critical public healthcare reduced.,yes,no,yes,True,no,"5.3 - Though there was no AI, the technology involved can be linked to the adverse outcomes in the incident.
5.5 - Because there is no AI, this incident does not qualify for CSET's definition of AI special interest intangible harm.",2009,,,True,no,no,,,US,North America,healthcare and public health,,"According to the incident report, ""Researchers who created the formula in 2009 added the “race correction” to smooth out statistical differences between the small number of Black patients and others in their data."" ","Black patients with chronic kidney disease, Black patients with chronic kidney disease, CKD-EPI eGFR calculation, physicians, National Kidney Foundation, American Society of Nephrology, Paloma Orozco Scott, medical institutions, Black patients with chronic kidney disease",0,0,True,"In June 2019, it was estimated how many Black Americans, at that point in time, were negatively affected by the algorithm using race. The estimate just looked at a small portion of the patients in the US, those at Mass General Brigham health system. The research estimated that 64 additional Black Americans would have qualified to be referred, evaluated, or waitlisted for a kidney transplant if the race factor was removed from the equation. Additional 743, would have been classified at a more severe stage if the race factor was removed. Since this equation has been used for about 30 years throughout health institutions in the US, 10s of thousands of Black Americans were likely affected.",There is no AI. The harm comes from a formula that was developed in the 1990s and uses race as a factor.,"age, sex, race, creatinine levels, medical data",human health and social work activities,yes,Autonomy3,,No. Not intentionally designed to perform harm,,,,not AI
CSETv1,147,True,147,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2020,,,False,no,no,Hong Kong,,CN,Asia,financial services,,,"Martin Zelner, Centennial Bank, Dubai Public Prosecution Office, Bank fraudsters, Audio Deepfake, Hong Kong bank, U.A.E. Executive Office of Anti-Money Laundering and Counter Terrorism Financing,",0,0,False,"at least $400,000 was lost ","The fraudsters used ""deep voice"" technology to clone a director's speech to convince a branch manager to authorize money transfers","speech, Audio , voice, text","financial and insurance activities, other",no,Autonomy2,The harm was in the financial sectors but the system was deployed in the criminal sector,Yes. Intentionally designed to perform harm and did create intended harm,,"deepfake audio generation, audio deepfake","Text to Speach, Deep Learning, Natural language processing",
CSETv1,148,True,148,,4. Peer review complete,002,True,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,3.2 - Overlay vendors offer products like on-page assistive technologies but also claim that they are using artificial intelligence to render websites complaint with laws and standards having to do with accessibility,maybe,yes,no,no,yes,disability,"Overlays as solutions to accessibility are not effective at accommodating webpages for people with disabilities. Not only are they misleading and misadvertized, but they are also ineffective.",yes,yes,yes,True,yes,,2021,,,True,no,no,,,US,North America,information technology,,,"Overlay vendors, Web users with disabilities, Accessibility Guidelines Working Group of the Worldwide Web Consortium, overlay widgets, consumer website using overlay widgets, Web users with disabilities",0,0,False,,"Overly products purport to deliver automated compliance with ADA and WCAG standards. Early versions included small user interface controls that read the page's content aloud. Similarly-positioned products added ""widgets"" intended to function as on-page assistive technologies that do things like increase font size, change the contrast of the colors on the page, and change the appearance of certain types of content on the page. Most recently, vendors of these products claim that their product can repair underlying code quality problems in the sites on which they're deployed using artificial intelligence.",website code,information and communication,maybe,unclear,,No. Not intentionally designed to perform harm,,"accessibility compliance, ADA website compliance",website code,
CSETv1,149,True,149,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2021,11,,True,no,no,,,US,North America,financial services,COVID-19 pandemic,"6.1 Zillow Offers was launched in 2018, but the financial loss occurred in 2021. 
6.12 The COVID-19 pandemic led to an increase in the volatility of house prices. While prices fell rapidly at the beginning of 2020, they rose strongly by late 2021. Then, inflation and rising Central Bank rates lowered housing demand again, reducing the price increase. ","Zillow, Zillow Offers employees, Zillow Offers employees, Zestimate",0,0,False,Zillow lost $540M.,"Zestimate is a machine learning-assisted estimate of a home's market value that is calculated by taking into account data about the property gathered from sources including tax and property records, homeowner-submitted details such as the addition of a bathroom or bedroom, and pictures of the house.","tax records, property records, homeowner-submitted details, home valuations, real estate data, geographical information, house pictures",real estate activities,no,unclear,"While the algorithm produces housing price predictions autonomously, the harm occurs with the decision to purchase the real estate. In this decision, there has to be a human on-the-loop at least, although the exact level (autonomy 2 or 3) is unclear based on the descriptions in the incident reports. ",No. Not intentionally designed to perform harm,,"price determination, home valuation, estimating house prices, real estate market forecasting","estimation, prediction",
CSETv1,151,True,151,,4. Peer review complete,002,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2021,10,28,False,no,yes,Fremont,CA,US,North America,transportation,,,"Pony.ai Hyundai Kona autonomous vehicle, California Department of Motor Vehicles, Pony.ai",0,0,False,,Pony.ai's autonomous vehicle was designed to navigate and operate around obstacles on roads driverlessly.,"spatial data, sensor data, video",transportation and storage,no,Autonomy1,The vehicle was completely driverless. There were no safety operators in the car at the time of the crash.,No. Not intentionally designed to perform harm,Electric vehicle,autonomous driving,"computer vision, object detection",
CSETv1,58,True,58,,4. Peer review complete,002,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,no,yes,yes,"sexual orientation or gender identity, ideology, race","4.4 and 4.5 - Yandex's chatbot Alice expressed views supporting shooting Russia's ""enemies of the people,"" self-harm, opposing gay marriage, opposing diversity, and suggesting people should put up with domestic violence. These sentiments are classified as detrimental content and indicate Alice's bias against protected groups.",yes,yes,yes,True,yes,,2017,10,10,False,no,no,,,RU,Europe,,,,"Users of Alice, Misha Bilenko, Yandex, Alice, SpeechKit, Users of Alice",0,0,False,,Russian-language A chatbot called Alice ,"speech, audio inputs, Text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"voice recognition, speech interpretation, virtual assistant technology, chatbot","speech recognition, natural language processing",
CSETv1,210,True,210,,6. Complete and final,001,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,,,,False,,,,,,,,,,,,,False,,,,,,,,,,,,
CSETv1,360,True,360,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,maybe,no,no,no,none,"Annotator 1: 

 4.2 - McDonald's was sued by Shannon Carpenter, who claimed that McDonald's voice-recognition drive-through technology violated  Illinois’ Biometric Information Privacy Act (BIPA). However, Carpenter and McDonald's later jointly filed a stipulation in Illinois federal court to have the case dismissed.",no,yes,yes,True,no,Privacy infringement is an intangible harm not covered under CSET's special interest intangible harms.,2021,06, ,False,no,yes,Chicago,IL,US,North America,food and agriculture,,,"McDonald's, McD Tech Labs, McDonald's voice-recognition drive-through software, McDonald's customers in Chicago, IL",0,0,False,,McDonald's implemented voice recognition technology to automate the drive-through ordering process with AI.,"voice input, audio",accommodation and food service activities,no,Autonomy2,"Annotator 1: 

 9.5 - Restaurant workers are able to jump in to help the AI.",No. Not intentionally designed to perform harm,,voice recognition,voice recognition,
CSETv1,273,True,273,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,sexual orientation or gender identity,,yes,yes,yes,False,no,"The outcome of this incident is that FaceApp identifies a person with thinner eyebrows as a woman. The same picture of them with thicker eyebrows identifies them as men. FaceApp likely considers thicker eyebrows a more masculine feature, but this likely applies to all users.

Being misgendered doesn't automatically mean that a harm issue occurred.  However, the affected user in the incident specifically mentioned dysphoria and dysmorphia.",2020,12,24,False,no,no,, ,US,North America,communications,,,"FaceApp, Erin Reed",0,0,False,,FaceApp is a photo editing app that generates realistic transformations of human faces based on user uploaded photos.,"facial images, selfies, uploaded images","information and communication, Arts, entertainment and recreation",no,Autonomy1,,unclear,,human facial image transformation,,"It is unclear if there is a harm in this incident, and if so, what the harm is. FaceApp was designed and intended to produce realistic transformations of human facial images."
CSETv1,414,True,414,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,,yes,yes,yes,False,yes,,2020,01,18,False,no,no, ,,,Global,communications,,,"Facebook, Suu Kyi's official Facebook page, Facebook's Burmese to English translation algorithm, Facebook users, Xi Jinping",0,0,False,,"Facebook includes a built in translation tool between languages. However, it has faced numerous problems with translation from Burmese in the past.","text, Burmese text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,"translation, Burmese to English translation",,
CSETv1,452,True,452,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,"Although ChatGPT did not directly cause the harm of users submitting auto-generated bug reports to Immunefi, those users did create extra work for Immunefi by submitting poor-quality and erroneous bug reports.",no,no,no,maybe,no,none,Chat-GPT-regenerated bug reports are not helpful and require Immunefi to ban users who submit them. Immunefi is open to allow LLM-generate submissions if their quality improves.,no,yes,yes,False,no,,2023,01, ,False,no,no,,,,Global,information technology,,,"Immunefi, OpenAI, ChatGPT, Immunefi submitters who use ChatGPT to generate reports",0,0,False,,ChatGPT is a tool that uses a large-language model called GPT-3 to converse naturally with humans.,,information and communication,no,Autonomy3,,No. Not intentionally designed to perform harm,,text generation,"deep learning, large-language models","10.1 - ChatGPT was designed with the intention of answering user queries and generating texts, but it was not designed with the purpose of hindering the normal functionings of platforms like Immunefi."
CSETv1,454,True,454,,4. Peer review complete,001,False,no,no,no,yes,no,no,maybe,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,race,,yes,yes,yes,True,yes,,2018,12, ,False,no,no,,,US,North America,,,"Annotator 1: 

 Date refers to the date of publication of the study that this incident is about, titled ""Racial Influence on Automated Perceptions of Emotions.""","Face++, Microsoft, Microsoft Face API, Lauren Rhue, Black people, Megvii",0,0,False,,Facial recognition software is used to assign and analyze emotions based on facial images.,facial images,"information and communication, unclear",no,Autonomy1,These emotion detection systems can be used in a variety of sectors,No. Not intentionally designed to perform harm,,emotion detection,"facial recognition, emotion detection",
CSETv1,435,True,435,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,maybe,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,Coupang was fined 3.29 billion won ($2.81 million) after it manipulated search algorithms to prioritize its own products over those of suppliers.,no,no,no,no,no,none,,no,yes,yes,True,no,,2017, ,,True,no,no,,,KR,Asia,,, 6.1 - The Korea Fair Trade Commission (KFTC) said the e-commerce giant was found to have forced hundreds of sellers from early 2017 to September 2020 to comply with its unlawful sales and marketing policies to maintain its competitive edge over other online retailers amid the fierce battle for market dominance.,"Coupang, Fair Trade Commission of South Korea, LG Household & Health Care, Yuhan Kimberly, Maeil Dairies, Namyang Dairy Products, Cuchen",0,0,False,,"According to Korea's Fair Trade Commission's announcement, Coupang made changes in search algorithms to prioritize the appearance of products and services related to Coupang. It also engaged in unfair business practices with suppliers.",text,wholesale and retail trade,no,Autonomy3,9.5 - The choice to push Coupang-affiliated products and services to the top of Coupangsearch results was made by Coupang itself. The AI was a tool Coupang used to achieve its goal of capturing more market share. ,Yes. Intentionally designed to perform harm and did create intended harm,,"product promotion, search result ranking",,10.1 - Coupang successfully used the search AI to promote its own products and services
CSETv1,281,True,281,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,False,none," 3.1 - There is a non-imminent risk that watchers could be influenced by the self-harm detrimental content that Youtube recommends which could cause tangible harm. However, in this case, the tangible harm would not be directly linked to the AI. ",no,no,yes,yes,no,none,,yes,yes,yes,True,yes,,2019,, ,False,no,no,,,,Global,information technology,,,"Youtube, Youtube search algorithm, Minors watching Youtube",0,0,False,,Youtube has failed to remove inappropriate suicide-themed content before and has offered worrying search term recommendations related to self-harm.,"text, video","information and communication, Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,"video recommender, search suggestion",,
CSETv1,349,True,349,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"3.1 - The rate of false positives caused by Evolv AI's misidentifications causes schools to lower sensitivity settings. This could contribute to lower accuracy in the future. False positives also require manual searches on ""almost every child as they walked through,"" monopolizing the attention of safety officers who would otherwise be monitoring the halls and other entrances.

Schools operating at a lower setting means that the rate of false negatives is increased. This could result in fewer guns being detected, which creates a potential for tangible harm.

Additionally, no evidence exists that the purchased gun detection systems are effective. Thus, there is a potential for school systems to have wasted a large amount of funds on ineffective systems.",yes,maybe,yes,no,no,none,"Because these systems are deployed in public schools, there could be an impact on public services and civil rights.",maybe,yes,yes,True,maybe,"Because these systems are deployed in public schools, there could be an impact on public services and civil rights.",2022,03, ,False,no,yes,Charlotte,NC,US,North America,,,,"Evolv Technology, Mallard Creek High School, Evolv AI-based weapons screening system, Students in the Charlotte Mecklenburg School District (CMS), Schools in the Charlotte Mecklenburg School District",0,0,False,,"Evolv systems scan the bodies of students for weapons like guns as they walk through. The scanning is meant to flag weapons like guns, but not common school items like phones, laptops, and binders. However, there is little statistical evidence that Evolv's scanners make schools safer or effectively detect guns in the operational conditions of school settings. During the 2021-22 school year, 30 guns were found on the school's campuses by means other than Evolv scanners. A decline in guns found on campus began months before Evolv scanners were implemented","body scans, images, video","Education, Arts, entertainment and recreation",yes,unclear,"9.3 - The incidents focus on the deployment of these AI systems in schools, but they are also used in entertainment arenas and venues. 

9.5 - The scanners do not require human oversight, interaction, or intervention to flag a student for what it determines to be a weapon. However, the scanners do not act on this information. It is up to security officers to investigate the flagged students.",No. Not intentionally designed to perform harm,body scanner,weapon detection,,"10.1 - The Evolv system was designed to detect weapons like guns, but turned out to be less effective than suggested."
CSETv1,490,True,490,,4. Peer review complete,001,False,no,yes,maybe,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,maybe,True,none,3.3 - The existence of chatbot AIs like ChatGPT is not directly linked to the adverse outcome of this incident. Human users actively choose to use AI to try and generate submissions that they can get paid for.,no,no,no,no,no,none,,no,yes,maybe,True,no,,2023,02, ,False,no,no,,,,Global,,,February 2023 refers to the date on which Clarkesworld editor Neil Clarke closed story submissions.,"Clarkesworld Magazine, ChatGPT, Clarkesworld Magazine story submitters , OpenAI, plagiarized entities",0,0,False,,ChatGPT is a chatbot intended to generate text and answer user prompts. It is based on a large language model.,text,"information and communication, Arts, entertainment and recreation",no,Autonomy3,,No. Not intentionally designed to perform harm,,text generation,large language model,"10.1 - ChatGPT was designed to generate text. However, it was not intended to be passed for original work and submitted to publications that do not allow AI submissions."
CSETv1,520,True,520,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm issue,"Because customers cannot pay for uncharged grocery items after the fact, it would be possible for shoppers to steal from Amazon Fresh stores either accidentally or intentionally, causing a financial loss to Amazon.",no,no,no,no,no,none,,no,yes,yes,True,no,,2022, ,,False,no,no,,,US,North America,,,,"Amazon, Amazon Fresh stores, Amazon Fresh cashierless checkout algorithm, Amazon Fresh customers",0,0,False,,"Amazon Fresh's computer vision-based checkout enables shoppers to pick up items and leave the store without having a discrete checkout phase to the visit. The shopping experience begins and ends with entry and exit gates requiring you to scan a QR code or credit card. Cameras track shoppers' movements and purchases, recognizing the products that shoppers pick up. A receipt is billed to the shoppers after they leave the store.","video input, camera footage",wholesale and retail trade,no,Autonomy2,"9.5 - There are likely humans on the loop if different cameras have different analyses of footage. According to Sean McGregors hypothesis, it appears Amazon is first checking for machine agreement, then having a human provide definitive labels only when the models are more likely to have failed.",No. Not intentionally designed to perform harm,store security camera,product recognition,computer vision,
CSETv1,476,True,476,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,maybe,tangible harm definitively occurred,yes,maybe,True,unclear,"When this incident was annotated (Jan 2024) there was an existing legal case about the liability of Youtube for the terrorist attack.  Because the liability is unresolved, we annotate AI Tangible Harm as 'unclear.' After the eventual court decision, the annotation should change to either yes or no AI tangible harm.",no,no,no,yes,no,none,,yes,yes,yes,True,yes,,2015,11,13,False,no,no,Paris, ,FR,Europe,information technology,,,"Nohemi Gonzalez , Paris 2015 terrorist attack victims, Google, Youtube, Youtube content recommender algorithm, Youtube users, ISIS",130,416,False,It is unclear if these deaths and injuries are clearly and directly linked to the AI. This matter is currently in the US court system.,Youtube's content recommender algorithm promotes content that users have shown interest in in the past. Someone who shows some interest in terrorist recruitment videos will be shown more recruitment videos based on past viewing habits. This feature may promote misinformation and encourage people to join harmful terrorist organizations.,Youtube videos,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,content recommendation,,"10.1 - The recommender system was designed to promote content users showed interest in. However, this does not include content related to terrorism or videos being used to radicalize and recruit extremists."
CSETv1,574,True,574,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,maybe,no,none,,no,yes,yes,True,no,,2023,07, ,False,no,no,,,,Global,,,,"G/O Media, Gizmodo, Gizmodo writers, Google, Bard, OpenAI, ChatGPT",0,0,False,,ChatGPT and Bard are text-generation bots.,"text, user prompts","information and communication, Arts, entertainment and recreation",no,Autonomy3,9.5 - Bard and ChatGPT do not operate unless they are user prompted. The chatbot-written articles would not have been published without the active decision of human executives at G/O Media.,Yes. Intentionally designed to perform harm and did create intended harm,,"text generation, content generation",,10.1 - Bard and ChatGPT were designed to generate text. 
CSETv1,461,True,461,,4. Peer review complete,001,False,no,no,maybe,yes,maybe,no,no,no,yes,"no tangible harm, near-miss, or issue",no,yes,True,none,,no,no,no,no,yes,"race, financial means",,yes,no,yes,True,no,,2023,01, ,False,no,no,,,US,North America,,,"6.1 and 6.2 - Date refers to the publication of the paper ""Measuring and Mitigating Racial Disparities in Tax Audits""","Black taxpayers, EITC claimants, Internal Revenue Service, Audit selection algorithms, RegLab researchers",0,0,False,,"The IRS's audit selection algorithms automate selecting returns for audit. The algorithms disproportionately flag tax returns with potential errors in the claiming of certain tax credits, like the earned-income tax credit, which supplements low-income workers' incomes to alleviate poverty. Research suggests that the IRS has focused on audits that are easier to conduct. However, Black Americans claiming the EITC only explained a small part of the audit differences. More than three-quarters of the disparity stems from how much more often Black taxpayers who claim the credit are audited compared with EITC claimants who are not Black.


There was both tangible and intangible harm. However, the report did not provide evidence that the IRS used AI (as opposed to a non-AI algorithm) when selecting who to audit. Thus, the definition of CSET AI harm was not met.","taxpayer profile, 148 million tax returns , 780000 audits, taxpayer names, census demographics","public administration, financial and insurance activities",yes,Autonomy3,,No. Not intentionally designed to perform harm,,audit selection,,
CSETv1,410,True,410,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,yes,"none, religion",,yes,yes,yes,True,yes,,2022,11,09,False,no,no, ,,DE,Europe,,,,"KFC, KFC app, KFC app automated messaging system, German KFC app users, Jewish German KFC app users",0,0,False,,KFC's automated messaging system is designed to detect holidays and other days of significance and write relevant marketing messages. Messages created by the system are supposed to be checked by a human before being sent to users.,"dates of significance, marketing material","accommodation and food service activities, information and communication",no,Autonomy3,9.5 - Messages created by the system are supposed to be checked by a human before being sent to users. This message slipped through the cracks. ,No. Not intentionally designed to perform harm,,marketing material generation,,
CSETv1,244,True,244,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"3.3 - The license plate reader software incorrectly matched the vehicle to a stolen vehicle with the same plate information but from a different state. The stolen vehicle in question was a motorcycle, while the one involved in the traffic stop was an automobile. It is important to note that while the AI system may have correctly matched the license plate number of the SUV to the same license number of a stolen motorcycle, question, the human police officers definitely should have exercised less extreme policing practices and should have been more careful about examining the results of the system's match--they should have noticed the extreme difference in vehicle types. However, if the license plate matcher hadn't returned the result of a match (which would be correct, considering the differing vehicle types and states of registration), then this harm would not have happened.

This appears to be a human in the loop failure",no,maybe,yes,no,yes,race,"4.2 and 4.3 - Four of the people apprehended by the police were children between the ages of 6 and 17. All of the people in the vehicle were black.  Everyone in the vehicle apprehended by the police was subject to extreme and unfair treatment by the police. The report cited other cases where that particular police department had extreme behavior towards black people.  Thus, the police appear to disproportionally treat black people differently. However, this disproportional behavior is not directly linked to the AI in this instance.",yes,yes,no,True,yes,"All of the people in the vehicle were black.  Everyone in the vehicle apprehended by the police was subject to extreme and unfair treatment by the police. The report cited other cases where that particular police department had extreme behavior towards black people.  Thus, the police appear to disproportionally treat black people differently. However, this disproportional behavior is not directly linked to the AI in this instance.",2020,08,02,False,no,no,Aurora,CO,US,North America,,,,"Aurora Police Department, Aurora Police Department license plate reader system, Family incorrectly apprehended by the police in Aurora, Family incorrectly apprehended by the police in Aurora",0,0,True,,The license plate matching system apparently matched the license plate of the family's minivan to the license plat of a motorcycle from Montana that had been reported as stolen.,license plate images,law enforcement,yes,Autonomy3,,No. Not intentionally designed to perform harm, ,"license plate recognition, Matching","Natural Language Processesing, computer vision",
CSETv1,239,True,239,,4. Peer review complete,001,False,no,no,yes,yes,no,no,no,no,yes,tangible harm definitively occurred,maybe,maybe,True,unclear,"3.1 - Some teachers were fired because of a negative assessment by the Gates Foundation's Intensive Partnerships for Effective Teaching value-added model. However, it is unclear that they would not have otherwise been fired for poor performance. Thus, there is an imminent risk that at least some of those teachers were unfairly fired because of their assessment by the algorithm.


Harm definitively occurred, but based on the information provided in the article, it is not clear that an AI is linked to the harm and not a non-AI (as defined by CSET) algorithm. ",no,no,no,no,no,none,,no,maybe,maybe,True,no,,2018,, ,True,no,no,, ,US,North America,,,,"Bill & Melinda Gates Foundation, Intensive Partnerships for Effective Teaching, Rand Corporation, Teachers assessed by the Intensive Partnerships for Effective Teaching program",0,0,False,,"The Intensive Partnerships for Effective Teaching program sought to ""improve education for low-income minority students... by gathering data and using an algorithm to assess teacher performance. It focused on measures such as test scores, the observations of school principals, and evaluations from students and parents to determine whether teachers were adding value.


Harm definitively occurred, but based on the information provided in the article, it is not clear that an AI is linked to the harm and not a non-AI (as defined by CSET) algorithm. ","test scores, school principal assessments, student evaluations",Education,yes,Autonomy3,,unclear,,,,no AI 
CSETv1,594,True,594,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,People could misuse the AI to get harmful recipes--but they would have to intentionally put inedible ingredients into the AI to get these harmful recipes.  This is an example of AI misuse.,no,no,no,yes,no,none,People could misuse the AI to get harmful content (ie recipes),yes,yes,yes,True,yes,,2023,08, ,False,no,no,,,NZ,Oceania,food and agriculture,,,"Pak 'n' Save, Savey Meal-bot, Savey Meal-bot users",0,0,False,,The Savey Meal-bot auto-generates meal plans from user-inputted ingredients.,ingredients,"accommodation and food service activities, wholesale and retail trade",no,Autonomy3,,unclear,,"recipe generation, text generation",,"10.1 - The Meal-bot was intended to create recipes from ingredients that users input. However, when customers began experimenting with entering a wider range of household shopping list items into the app, the Meal-bot continued to create recipes with them even if toxic or deadly."
CSETv1,545,True,545,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,,yes,yes,yes,True,yes,"5.2: Tessa was originally a closed, rules-based, non-AI system (based on decision trees). Cass AI (Tessa's developer) later implemented an AI component without the knowledge or consent of the co-developers.",2023,06,,True,no,no,,, ,Global,healthcare and public health,,"Date: The chatbot was deployed in February 2022, but reports of harmful conversations emerged in June 2023. 

Location: The chatbot is deployed online. While designed and deployed by a US association, it is not clear if there are regional restrictions to accessing it. ","Tessa, Cass (formerly X2AI), National Eating Disorders Association, Chatbot users with, or at risk of developing, an eating disorder, Prof. Ellen Fitzsimmons-Craft",0,0,False,"Re: the scale of harm: According to the company that operates the chatbot, .1% of interactions contained harmful content. 

However, there are calls from about 70,000 people each year, or about 200 calls per day. Each call should involve more than 1 interaction. If each call has 10 interactions, then there are 20 interactions with harmful content each day. So maybe 2 to 20 people receive harmful content each day (or 700 to 1400 each year) ",Chatbot for eating disorder prevention,text,human health and social work activities,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,
CSETv1,457,True,457,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,no,none,,no,yes,yes,True,no,,2022,11, ,True,no,no,,,,Global,communications,,,"CNET, CNET readers, Red Ventures, \""CNET Money Staff\, Forbes, The Balance, Investopedia, Bankrate, CNET readers",0,0,False,,Generative language model,text,information and communication,no,Autonomy3,"9.5: According to The Verge: ""AI editors can choose domains and domain-level sections from which to pull data and generate stories; editors can also use a combination of AI-generated text and their own writing or reporting.""",No. Not intentionally designed to perform harm,,text generation,,
CSETv1,493,True,493,,3. In peer review,001,False,no,maybe,no,yes,no,no,no,no,maybe,non-imminent risk of tangible harm (an issue) occurred,maybe,yes,True,unclear,"Annotator 1: 

 3.1: The user drew an audience to his profile by impersonating Andrew Tate (without claiming explicitly that they were him), where they were selling an 'Online Wealth Creation Course', similar to Andrew Tate's 'Hustler's University'. They intentionally misled other users who may have purchased the class thinking it was held by Andrew Tate. 
3.2: It is unclear if the voice alteration technology used by @drreality.life is powered by AI. ",no,no,no,maybe,no,none,,maybe,maybe,yes,True,maybe,"Content produced by Andrew Tate contained disinformation and hate speech and was banned from TikTok. The article is vague about whether the impersonator also produced this type of content. 

Additionally, while it is possible that the impersonator used AI to fake Tate's voice, it is not known if he did.",2023,02,14,False,no,no, ,,,Global,communications,,,"Andrew Tate, Dr.Reality, TikTok, TikTok users, Andrew Tate followers on TikTok",0,0,False,,,,information and communication,no,unclear,"9.5: The system itself likely operates independently (autonomy = 1), but the corresponding adverse effects (impersonation, misleading other users) happen only after the publication of content on the platform, which is decided by a human (@drreality.life)

Additionally, it is unclear if AI was used to impersonate Tate's voice.",unclear,,voice alteration,,
CSETv1,396,False,0,,,,False,,,,,,,,,,,,,False,,,,,,,,,,,,,False,,,, ,,False,,,,,,--,,,,,0,0,False,,,,,,,,,,,,
CSETv1,519,True,519,005,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,,no,no,no,no,no,none,,no,yes,yes,True,no,"
  ",2022,04,03,False,no,yes,Los Angeles,CA,US,North America,transportation,,,"University of California Los Angeles, Starship Technologies, Starship delivery robot, ",0,0,False,,"The Starship delivery robot is designed to operate fully autonomously on more than 99 percent of deliveries. In the remaining 1 percent of the time, a persona connects remotely to the unit from one of the three countries with operator teams. In this case, the Starship vandalism detectors were triggered by the interference of humans to try and help the robot when it veered off the path. As soon as the unit registered potential vandalism, a human operator connected to the cameras and controls. The task for the human operator is to return the unit to the GPS track so it can continue on its path without human supervision.","video input, sensor data","accommodation and food service activities, transportation and storage",maybe,Autonomy2,,No. Not intentionally designed to perform harm,Starship delivery robot,autonomous food delivery,,
CSETv1,501,True,501,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,yes,maybe,no,no,yes,"age, disability, financial means",This may be a civil rights violation because there maybe unequal access to Medicare benefits (a government-provided service) with older people being at a disadvantage.,yes,yes,yes,True,yes,,2023, ,,True,no,no,,,US,North America,healthcare and public health,,6.1 - Year refers to the date of publication of the investigation published by STAT. It is unclear how long algorithms have been in use for determining how soon patients can be discharged or whether or not patients meet coverage requirements. The source identifies June 2019 as when Frances Walter was wrongfully denied payment for her care.,"Frances Walter, Medicare and Medicaid patients, Medicare and Medicaid patients, Medicare Advantage insurers and healthcare insurance companies, NaviHealth, nH Predict, Frances Walter, Medicare and Medicaid patients",0,0,True,"There are likely injuries related to this, but we cannot estimate the amount.","nH Predict uses details such as a person's diagnosis, age, living situation, and physical function to find similar individuals in a database of 6 million patients it compiled over years of working with providers. It then generates an assessment of the patient's mobility and cognitive capacity, along with a down-to-the-minute prediction of their medical needs, estimated length of stay, and target discharge date.","diagnosis, age, living situation, physical function",human health and social work activities,yes,Autonomy3,"While deployers and users of the systems claim Autonomy level 3 or 2 functionality, it appears that the system is regularly used without review or oversight, which is autonomy level 1",No. Not intentionally designed to perform harm,,predict necessary amount of medical coverage,,
CSETv1,290,True,290,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,"By allowing contaminated beaches to stay open because of measurements taken by AI water quality monitoring systems, the Toronto government put beach-goers directly at risk of contracting E. coli and other bacteria.",no,no,no,no,no,none,,no,yes,yes,True,no,,2022, , ,False,no,yes,Toronto,,CA,North America,,,,"Toronto Public Health (city government), Toronto beachgoers, Cann Forecast, Sunnyside and Marie Curtis beaches, Water quality testing artificial intelligence predictive modeling tool",0,0,False,,"Waters that tested high for E. coli using traditional means were marked safe by the new AIPM system dozens of times. ""Cann Forecast describes its beach water monitoring system as a high-tech ""artificial intelligence algorithm"" that uses ""machine learning"" to provide ""real-time water quality advisories that are 90% accurate on average.""""",water samples,"human health and social work activities, public administration",yes,Autonomy1,,No. Not intentionally designed to perform harm,,water quality testing,machine learning,
CSETv1,329,True,329,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,maybe,"no tangible harm, near-miss, or issue",yes,no,False,none,"There are no instances of users being influenced by Amazon's ""frequently bought together"" recommendations to build bombs. Even if there were, the harm would not be directly linked to the AI because the AI would not have directly caused the person to make the bomb.",no,no,no,yes,no,none,4.4 - Exposing users to suggestions that could help them make bombs could be considered detrimental content.,yes,yes,yes,True,yes,"5.3 - AI is directly linked to the intangible harm of detrimental content, or exposing people to suggestions that might help them make bombs.",2017,, ,True,no,no,,,,Global,,,,"Amazon, Amazon purchasers of black powder, thermite, ball bearings, and other potential bomb ingredients, Amazon recommender algorithm",0,0,False,,Amazon's recommender algorithm provides buyers of a product with suggestions of bundles of other items that other users frequently bought along with it.,"Amazon items, Amazon purchases",wholesale and retail trade,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,
CSETv1,245,True,245,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"Annotator 1: 

 3.3 - The automated license plate reader did make a mistake in identifying Denise Green's license plate number, 5SOW350, as plate number 5SOW750 which belonged to a stolen vehicle.  However, police officers involved had several chances to verify the plate number or caution the others involved to do so. Sergeant Kim, who was the one that pulled Green over, did not visually identify all seven numbers on Green's license plate. Thus, even though verification protocols may not have been in place at the time of the incident, there were several points at which humans should have reasonably decided not to act on the automated system's misidentification.",yes,maybe,no,no,no,none,Unclear if this was a civil rights violation. ,maybe,yes,yes,True,maybe,,2009,03,30,False,no,yes,San Francisco,CA,US,North America,,,,"Denise Green, Automated license plate reader software, San Francisco Police Department, Denise Green",0,0,False,"Annotator 1: 

  ",Automated license plate readers parse passing vehicles and search for matches with license plate numbers in a database of vehicles that are stolen or wanted.,license plate images,law enforcement,yes,Autonomy3,"The ALPR operated by SFPD officers Alberto Esparza and Robert Pedersen misidentified Denise Green's plate as belonging to a stolen vehicle. Esparza reported it over the radio to Sergeant Kim, who apprehended Green. 

Note the systems, which should be operated at autonomy level 3, seems to be used at autonomy level 1.  Users and deployers are not providing oversight and verifying results. ",No. Not intentionally designed to perform harm,,license plate matching,,
CSETv1,489,True,489,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,maybe,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,no,yes,"age, disability, race",,yes,yes,yes,True,yes,,2023,02, ,False,no,no,,,US,North America,,,Date refers to when the lawsuit against Workday was launched.,"Workday, Derek Mobley, Black people applying to jobs through Workday, Disabled people applying to jobs through Workday, Older people applying to jobs through Workday",0,0,False,,"Workday is used as a recruitment screening tool. Allegedly, ""the selection tools marketed by Workday to its customers allows these customers to manipulate and configure them in a discriminatory manner to recruit, hire, and onboard employees. Workday's products process and interpret an applicant's qualifications and recommend whether the applicant should be accepted or rejected.""",job application,"professional, scientific and technical activities, information and communication",no,unclear,"Annotator 1: 

 9.5 - It is unclear whether the Workday recruitment screening tools completely eliminate certain candidates from the applicant pool before a human ever sees them, or if the tools only provide recommendations to human reviewers about whether or not to hire the candidate.",No. Not intentionally designed to perform harm,,"recruitment screening, job screening",,
CSETv1,412,True,412,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,yes,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,yes,yes,no,no,none,,yes,yes,yes,True,yes,,2021,04,08,False,no,no, ,,,Global,,,"Date refers to the discovery of the incident by Finland's National Bureau of Investigation. However, the tool was presented to Finnish authorities as early as 2019.
Buzzfeed discovered that 88 international government-affiliated and taxpayer-funded agencies and organizations around the globe used Clearview AI.","Clearview AI, Clearview AI facial recognition software, Law enforcement agencies and government organizations, Subjects of images scraped from the web by Clearview AI",0,0,False,,ClearView AI distributes a facial recognition tool that law enforcement officers can use to receive matches on photos of suspects or people of interest. It claims its software is more accurate than other facial recognition technologies because it is trained on a database of more than 3 billion images scraped from websites and social media platforms.,"facial images, images from websites, scraped images, images from social media","law enforcement, information and communication",yes,Autonomy3,,No. Not intentionally designed to perform harm,,facial recognition,"image match, facial recognition",
CSETv1,509,True,509,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,maybe,no,none,,maybe,yes,yes,True,no,,2023,03,  ,False,no,no,,,VN,Asia,,,,"Deepfake technology, Deepfake scam victims in Vietnam, Deepfake scammers, Deepfaked Facebook users in Vietnam",0,0,False,,"Scammers using deepfake technology began by collecting visual data on victims from Facebook, Zalo, etc. Then, they would collect personal data such as the victims' phone numbers, email addresses, and family relationships. Next, they would make a phone or video call using deepfaked audio and video of the victims in order to convince their family members and acquaintances to transfer money to their compromised accounts.","Facebook accounts, facial images, personal information, family relationships",information and communication,no,Autonomy3,,No. Not intentionally designed to perform harm,,"deepfake video generation, deepfake audio generation",,
CSETv1,474,True,474,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,"In January of , many Replika users noticed that the interactions with their AI companions were veering increasingly toward the vulgar - even if they didn't initiate it. Many found this sudden change ""unwelcome and disturbing.""

Interactions changed because the Italian Data Protection Authority required that Replika stop processing Italians' data because of the app's risk to children.",yes,yes,yes,True,yes,,2023,02, ,False,no,no,,,,Global,information technology,,"6.4 - Users noticed unwelcome vulgar responses from the Replika AI in January of 2023. In February, users started noticing that their Replika companions were no longer engaging in erotic roleplay (ERP), a service initially advertised as part of the product.","Luka, Replika, Replika users, Replika users",0,0,False,,Replika's AI chatbot provided sexually-charged conversations as part of a $70-per-year paid tier.  It advertised that users could find sexual fulfillment or emotional support through use of the chatbot.,text,"Arts, entertainment and recreation, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,chat bot,,
CSETv1,510,True,510,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,maybe,no,none,,yes,yes,yes,True,yes,,2023,03, ,False,no,no,,,,Global,,,,"Twitter user skyferrori, Midjourney Inc., Midjourney v5, Pope Francis, Twitter users",0,0,False,,Midjourney v5 is an image synthesis service that creates artificially-generated photos based on user prompts.,"user prompts, user queries","information and communication, Arts, entertainment and recreation",no,Autonomy3,,No. Not intentionally designed to perform harm,,image generation,,
CSETv1,339,True,339,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,yes,no,no,none,,no,yes,yes,True,no,,2022,,  ,True,no,no,,,,Global,,,,"Sudowrite, OpenAI, ChatGPT, Teachers, Non-cheating students, Cheating students",0,0,False,,"Sudowrite, ChatGPT, and other tools are used in some instances to facilitate cheating in academic settings. These tools accept user prompts/queries and specifications about output in order to answer essay questions or provide unauthorized assistance with assignments for students.","user queries, user prompts, text","Education, information and communication",no,Autonomy3,,No. Not intentionally designed to perform harm,,text generation,large language models,
CSETv1,554,True,554,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,"3.2 - There are two AI systems potentially involved in this incident - first, the AI that was used to generate the Vermeer ""Girl with a Pear Earring"" replica. Second, Google's search algorithm placed the AI-generated image in the featured card of the Google search result.",no,no,no,yes,no,none,,yes,yes,yes,True,yes,,2023,06, ,False,no,no,,,,Global,,,,"Google , Google's search engine algorithm, Google users interested in art, Julian van Dieken, Midjourney, Midjourney Inc.",0,0,False,,"While it is unclear which AI image generation software van Dieken used to generate the replica image, he likely inputted details about the original painting to prompt the AI to generate the image that it did. ","user queries, user prompts","Arts, entertainment and recreation",no,unclear,9.5 - The image-generation AI was operating under Autonomy3 and would not have produced an image without the prompts of van Dieken. Google's search engine algorithm likely operates under Autonomy1.,No. Not intentionally designed to perform harm,,"image generation, search optimization, search engine optimization",,
CSETv1,341,True,341,005,6. Complete and final,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"Nissan said it knew of ""at least 30 allegations of 'minor collisions' stemming from automatic emergency braking incidents, and 10 reports alleging 'minor injuries.'""",no,no,no,no,no,none,,no,yes,yes,True,no,,2017,,,True,no,yes,, ,US,North America,,,"The allegations come from Rogue and Rogue Sport vehicles produced in the 2017 to 2020 model years, and Sentras made in the 2018 and 2019 model years. Other vehicles that may be equipped with faulty Automatic Emergency Braking (AEB) systems include the Nissan Pathfinder, Murano, Altima, Maxima, Armada, Leaf, Sentra, Kicks, and Versa.","Nissan, Nissan Rogue, Rogue Sport, and Sentra vehicles, Nissan drivers, Automatic Emergency Braking systems",0,10,True,,"Nissan vehicles' Automatic Emergency Braking systems are there to ""alert drivers of a possible imminent frontal collision, braking if the driver doesn't respond to the warning by putting on the brakes themselves. Nissan's AEB systems use radar to determine pedestrians, other vehicles and other potential obstacles ahead of a car."" However, the systems are accused of detecting non-existent obstacles, providing false alarms, or triggering the brakes despite no obstacles being present. This is especially dangerous because the emergency braking systems have falsely engaged while in intersections or on bridges, highways and railroad tracks. The activation of the brakes allegedly makes it difficult for drivers to move out of the way of danger, putting them at an increased risk of side-on or rear-end collisions.",radar input,transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,"Nissan vehicles (Rogue, Rogue Hybrid, Rogue Sport, Sentra)",automatic emergency braking,,
CSETv1,387,True,387,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",no,yes,False,none,,no,maybe,no,no,no,,"The substance of the class action  litigation ""hinges on allegations that Oracle collects vast amounts of data from unwitting Internet users, i.e. without their consent, and uses this surveillance intelligence to profile individuals, further enriching profiles via its data marketplace and threatening people's privacy on a vast scale.""

Note 'privacy' is not one of CSET's special interest intangible harms",maybe,no,yes,True,no,"The legal case concerns Oracle gathering detailed dossiers on about 5 billion people, potentially violating privacy laws.  There is no evidence of AI in this incident.",2022,08, ,False,no,no,,,US,North America,,,,"Oracle, Internet users surveilled by Oracle, Dr. Johnny Ryan, Michael Katz-Lacabe, Dr. Jennifer Golbeck, Lieff Cabraser, Oracle data collection tools",0,0,False,,Oracle may collect data from Internet users without their consent in order to create more comprehensive user profiles to sell on the marketplace for data.,Internet usage,information and communication,no,unclear,,,,,,no AI
CSETv1,367,True,367,,4. Peer review complete,001,False,no,no,maybe,yes,no,yes,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,yes,sex,,yes,yes,yes,True,yes,,2021,01,,True,no,no, ,,,Global,,,,"Ryan Steed and Aylin Caliskan, OpenAI, iGPT, Google, SimCLR, Women",0,0,False,,"Steed and Caliskan examined embeddings within image-generation algorithms, which separate pixels based on how often they co-occur within training images. Those pixel embeddings can then be used to compare how close or far two images are in mathematical space.  AI algorithms like iGPT and SimCLR are used to auto-complete input images.",images,information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,,"unsupervised learning, natural language processing, pixel embeddings",
CSETv1,313,True,313,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,yes,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,unclear,,yes,yes,yes,True,yes,,2022,08,24,False,no,no, ,,,Global,,,,"Marietje Schaake, BlenderBot 3 users, BlenderBot 3, Meta",0,0,False,,"Meta's BlenderBot 3 was a ""state-of-the-art conversational agent"" developed as a research project that generated text responses to user prompts.","user prompts, user queries, text",information and communication,no,Autonomy1,,No. Not intentionally designed to perform harm,,text generation,large language models,
CSETv1,576,True,576,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,False,none,,no,no,maybe,yes,no,none,"4.3 and 4.4 - PicSo emphasizes ""girls."" Although the AI images do not specify what age the people in the images are, the AI has harmful applications. ",yes,yes,yes,True,yes,,2023,11, ,False,no,no,,,,Global,,,"Annotator 1: 

 Date refers to post made by Patrick Hall on Linkedin, who drew attention to the existence of PicSo AI's ads on Instagram.","PicSo AI, Instagram, PicSo AI users",0,0,False,,PicSo AI is an app that uses generative AI to create images and perform image generation services based on user uploaded images and requests.,images,information and communication,no,Autonomy1,,Yes. Intentionally designed to perform harm and did create intended harm,,image generation,,10.1 - PicSo AI was developed to generate images and could very well be used to generate detrimental and inappropriate content.
CSETv1,254,True,254,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,no,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,maybe,no,no,no,none,"4.2 - Google's agreed to pay $100 million to settle a class-action lawsuit regarding its facial grouping tool, which automatically identifies your face in photos and videos uploaded to Photos. Plaintiffs in Illinois sued for the violation of their privacy rights as outlined by the Illinois Biometric Information Privacy Act.

Note, while a privacy violation is an intangible harm, it is not one of the CSET special interest intangible harms",no,yes,yes,True,no,,2022,, ,True,no,no,,IL,US,North America,,,"Annotator 1: 

 Anyone who appeared in a photo on Google Photos between May 1, 2015, and April 25, 2022, while they were an Illinois resident was eligible to submit a claim for their part of the settlement.","Google, Google Photos, Google Photos users in Illinois",0,0,False,,Google Photo's facial grouping tool recognizes and sorts faces on Google Photos by similarity.,"photos in Google photos, photos, facial images",information and communication,no,Autonomy1,,Yes. Intentionally designed to perform harm but created an unintended harm (a different harm may have occurred),,"facial recognition, facial grouping",,"10.1 - Google Photos' facial grouping tool was intended to recognize and sort faces. However, it was not intended to violate any users' rights."
CSETv1,220,True,220,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,maybe,financial means,Primarily small to medium-sized business owners were affected.,maybe,yes,yes,True,maybe,,2020,11, ,False,no,no,,,,Global,,,"Facebook switched to relying more heavily on artificial intelligence to police its platform after the COVID-19 pandemic began in March 2020. However, users mainly started reporting their ads being unjustly taken down in November 2020.","Facebook (now Meta), Facebook automated spam filters, Small and mediaum businesses on Facebook, Advertisers on Facebook",0,0,False,,Facebook deploys automated spam filters to moderate content and take down ads/posts that violate its content policies. ,"Facebook ads, Facebook posts","wholesale and retail trade, information and communication",no,Autonomy1,,No. Not intentionally designed to perform harm,,content moderation,,
CSETv1,294,True,294,,3. In peer review,001,False,yes,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2018,05,,False,no,yes,Thessaloniki, ,GR,Europe,transportation,,,"Tesla, Tesla Model 3, You You Xue, Tesla Autopilot",0,0,False,,Tesla's Autopilot is designed to assist drivers by semi-autonomously navigating road obstacles and responding to real-time traffic conditions.,"geospatial data, sensor data",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,Tesla Model 3,semi-autonomous navigation,,
CSETv1,456,True,456,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,yes,maybe,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,yes,yes,no,none,,yes,yes,yes,True,yes,,2021,,,True,no,no, ,,,Global,,, 6.1 - There are instances of Replika AI being overly sexual since as early as 2021.,"Luka, Replika, Replika users, Eugenia Kuyda",0,0,False,,Replika AI is a chatbot that uses Luka's own GPT-3 model and scripted dialogue content in order to customize conversations with users.,text,"Arts, entertainment and recreation, information and communication",no,Autonomy1,,unclear,,chat bot,natural language processing,"10.1 - Replika AI was designed to initiate and reciprocate sexual conversations. However, this incident includes instances in which it was overly violent or sent sexual messages to users who didn't request it."
CSETv1,354,True,354,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,maybe,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"3.1 - Uber lost in an appeals court in the Netherlands and was ordered to pay a fine of €10,000 every day it continued to violate data protection laws. Uber was withholding details about its batch matching and upfront pricing systems, as well as ""information about the existence of automated decision-making... including assigning rides, calculating prices, rating drivers, calculating 'fraud probability scores', deactivating drivers' accounts,"" etc. This, especially wrongful termination of accounts, could cause drivers financial loss. In this instance, the company Uber experienced tangible AI harm while drivers experienced a non-imminent risk of financial loss.",no,yes,no,no,no,none,"4.2 - an appeals court in the Netherlands ""found largely in favor of platform workers litigating against ride-hailing giants... judging the platforms violated the drivers' rights in a number of instances, including when algorithms were involved in terminating driver accounts.""",yes,yes,yes,True,yes,,2019, ,,True,no,no,,,,Europe,,,6.1 - it is unclear how long Uber has been using algorithmic decision-making.,"Uber, European Uber drivers, European Uber drivers, Uber app",0,0,False,,"Uber automates decision-making for management functions such as driver termination, account deactivation, upfront pricing, batch matching, fraud probability scores, profiling, assigning riders, calculating pricing, rating drivers, and more","Uber accounts, driver behavior data, driver rating data",transportation and storage,no,Autonomy1,,No. Not intentionally designed to perform harm,,,,
CSETv1,214,True,214,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,,no,yes,yes,no,yes,race,,yes,yes,yes,True,yes,,2020,01, ,False,no,yes,New York,NY,US,North America,,,,"Lockport City School District, SN Technologies, AEGIS face and weapons detection system, Lockport City School District students, id3 Technologies, Dark Skinned Lockport City School District Students, Lockport City School District Students",0,0,False,,SN Technologies' weapons and facial recognition software uses facial recognition to surveil students and identify objects within the school to see if they are guns or not. It was found that these systems reported a lot of false alarms.,"video input, camera footage","Education, law enforcement, public administration",yes,unclear,"“The police have said if they get a notification, they’re going to treat it as a live shooter system, and you have a system that’s predisposed to make mistakes and misidentify people,” This treatment could be seen as effectively making the autonomy level 1, but in other cases, it acts with less autonomy.",No. Not intentionally designed to perform harm,,"facial recognition, object detection, weapons detection",facial recognition,
CSETv1,547,True,547,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,no,no,maybe,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,,yes,yes,yes,True,yes,,2023,06,05,False,no,no, ,,US,North America,,,,"Ron DeSantis's campaign, Donald Trump, People who viewed the DeSantis campaign's AI-generated images",0,0,False,,"It is unclear which tool was used. However, DeSantis's campaign used some technology to generate fake images of Donald Trump hugging Anthony Fauci.",images,other,no,Autonomy3,9.3 - political advertising,No. Not intentionally designed to perform harm,,image generation,,
CSETv1,526,True,526,,4. Peer review complete,001,False,no,yes,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,maybe,no,no,no,none,"Annotator 1: 

 4.2 - there are no established legal standards on whether or not deepfake AI of artist's voices violates copyright laws",no,yes,yes,True,no,,2023,04, ,False,no,no,,,,Global,,,,"Drake and The Weeknd, @ghostwriter, Universal Music Group",0,0,False,,"It is unclear which deepfake AI technology TikTok user @ghostwriter used to create the viral song ""Heart On My Sleeve."" However, it was likely trained with catalogs of Drake's and The Weeknd's music. Then, @ghostwriter likely inputted lyrics and prompted the AI to output a likeness of Drake and The Weekend singing them.","songs, Drake songs, Drake voice, The Weeknd songs, The Weeknd voice","Arts, entertainment and recreation",no,Autonomy3,,unclear,,deepfake audio generation,,"10.1 - The technology used was designed to deepfake voices. However, it was likely not intended to infringe on copyrights, as UMG alleges it does."
CSETv1,619,True,619,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,no,yes,non-imminent risk of tangible harm (an issue) occurred,yes,yes,True,AI tangible harm issue,"3.1 - People flagged as shoplifters by Rite Aid's facial recognition technology could be approached, asked to leave without finishing their purchases, searched, humiliated, or reported to the police. ",no,no,no,no,yes,race,,yes,yes,yes,True,yes,,2012, ,,True,no,yes,,,US,North America,,,6.1 - The FTC alleged that Rite Aid deployed facial recognition technology in hundreds of retail pharmacies and stores between 2012 and 2020.,"Federal Trade Commission, Rite Aid, Rite Aid customers, Rite Aid customers, DeepCam, FaceFirst, Rite Aid pharmacies and stores",0,0,False,,"Rite Aid supervised the creation of a ""watchlist database"" of images of people the company claimed had engaged in actual or attempted criminal activity at one of its stores. These entries included ""first and last names, years of birth, and a description of behavior Rite Aid claimed the person in the photo had engaged in... According to the complaint, Rite Aid directed store security to ""push for as many enrollments as possible."" If someone who entered the store matched an image in the database, employees received an alert and could choose to follow, apprehend, report, or otherwise engage with the customer. However, Rite Aid's technology was found to disproportionately misidentify black, Asian, Latino, and female consumers as matching an image in the database.","store camera footage, camera footage, surveillance footage",wholesale and retail trade,no,Autonomy3,,No. Not intentionally designed to perform harm,,facial recognition,facial recognition,
CSETv1,564,True,564,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,imminent risk of tangible harm (near miss) did occur,yes,yes,True,AI tangible harm near-miss,,no,no,no,no,no,none,,no,yes,yes,False,no,,2023, ,,False,no,no,,,,Global,,,,"Clive Kabatznik, Unknown deepfake technology developers, Unnamed deepfake scammers, Unknown deepfake voice technology",0,0,False,,"Although it is unclear who the scammers were, what technology they used, and who developed that technology, it is clear that there was fraud involved with Kabatznik's account. The scammer likely used publicly available recordings of Kabatznik's voice to train a deepfake voice AI, and then inputted text when calling the Bank of America representative to attempt to get Kabatznik's money transferred without his consent.","audio inputs, voice recordings",financial and insurance activities,no,Autonomy3,,No. Not intentionally designed to perform harm,,deepfake audio generation,,"10.1 - deepfake voice technology was designed to output audio based on recording/voice input. However, it was not designed with the goal of implementing financial scams in mind."
CSETv1,323,True,323,,4. Peer review complete,001,False,yes,no,no,yes,no,no,no,maybe,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,no,no,no,no,no,none,,no,yes,yes,True,no,,2018,05,29,False,no,yes,Laguna Beach,CA,US,North America,transportation,,,"Tesla sedan, Tesla driver, Tesla, Laguna Beach Police Department vehicle, Tesla Autopilot",0,1,False,,Tesla Autopilot is designed to assist drivers in semi-autonomously navigating road obstacles and reacting to real-time traffic conditions.,"geospatial data, sensor input, camera input",transportation and storage,no,Autonomy2,,No. Not intentionally designed to perform harm,Tesla sedan,semi-autonomous navigation,,
CSETv1,409,True,409,,4. Peer review complete,001,False,no,yes,yes,maybe,no,no,no,no,no,"no tangible harm, near-miss, or issue",maybe,no,True,none,,no,no,maybe,no,yes,sexual orientation or gender identity,,yes,maybe,no,True,no,,2013, ,,True,no,no,,,,Global,,,"6.1 - Ricanek was researching as early as 2013. However, the investigation of the dataset he compiled didn't happen until 2017. He collected the data without first going through the ethical review process at his university. Additionally, he kept the data in an unsecured dropbox. ","Karl Ricanek, Trans people who uploaded videos of their hormone therapies on Youtube",0,0,False,,"Ricaneck built a biometric dataset of 10,000 images of 38 trans people, scraped from their YouTube videos documenting their hormone therapies in order to improve the accuracy of facial recognition systems in identifying people pre and post hormone therapy. This ""HRT Transgender Dataset"" was still available as a Dropbox URL as late as April 2021 and also included the videos.","Youtube videos, facial images","professional, scientific and technical activities",no,Autonomy3,"9.5 - Ricanek initiated and facilitated the scraping of the YouTube videos. The dataset would not have been built without his involvement. However, there is no technology or AI explicitly involved in this incident.",,,,,no AI
CSETv1,614,True,614,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,maybe,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,no,no,yes,no,none,,yes,yes,yes,True,yes,,2023,10, ,False,no,no,,,AU,Oceania,,,,"Australian academics urging parliamentary inquiry into Big 4 Accounting firms, Google, BardAI, KPMG, Deloitte, Readers of parliamentary inquiry report",0,0,False,,"Google's Bard AI, now known as Gemini, is a generative artificial intelligence chatbot that responds to user prompts by generating text to answer their questions.","text, user queries, user prompts","law enforcement, information and communication",no,Autonomy3,"Annotator 1: 

 9.4 - Bard AI was used in a report submitted by academics to the Australian Parliament.",No. Not intentionally designed to perform harm,,"chatbot, content generation",large language models,
CSETv1,393,True,393,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,yes,no,no,"no tangible harm, near-miss, or issue",yes,yes,True,none,"3.2 - It is unclear whether Facebook's failure to reject the ads containing harmful content was attributable to human content moderators or content moderation algorithms. Facebook does use ""proactive detection technology.""",no,no,no,yes,yes,"nation of origin, citizenship, immigrant status, race","4.4 - In Kenya, Global Witness submitted test ads that ""spoke of beheadings, rape and bloodshed. They compared people to donkeys and goats. Some also included profanity and grammatical errors.""
4.6 - Test ads submitted in Ethiopia used dehumanizing hate speech to call for the murder of people belonging to each of Ethiopia's three main ethnic groups - the Amhara, the Oromo and the Tigrayans.",yes,yes,yes,True,yes,,2022,07, ,False,no,no,,,,Africa,,,"Annotator 1: 

 Global Witness submitted ads in Kenya, Ethiopia, and Myanmar.","Facebook, Meta, Foxglove, Global Witness, Facebook users, Facebook content moderation algorithms",0,0,False,,"Facebook utilizes content moderation algorithms that are developed to review ad submissions and reject those that contain content that violates its guidelines. These algorithms work in tandem with human content moderators. Some of the English ads submitted by Global Witness were rejected at first, but only because they contained profanities and mistakes in addition to hate speech. Once the profanities were removed and grammar errors fixed, however, the ads - still calling for killings and containing obvious hate speech - went through without a hitch.

Note, while the harmful ads were approved for use, they we never actually used.","Facebook ads, Facebook posts","Arts, entertainment and recreation",no,Autonomy1,,No. Not intentionally designed to perform harm,,content moderation,,
CSETv1,583,True,583,,4. Peer review complete,001,False,no,maybe,yes,yes,no,no,no,no,yes,"no tangible harm, near-miss, or issue",yes,yes,True,none,,no,yes,yes,yes,no,age,4.2 - Child sexual abuse content is illegal,yes,yes,yes,True,yes,,2023,06, ,True,no,no,,, ,Global,information technology,,,"Instagram, Meta, Instagram recommender algorithms, The Wall Street Journal, Researchers at Stanford University's Internet Observatory Cyber Policy Center and the University of Massachusetts Amherst, Instagram users, Children",0,0,False,,"""Instagram's recommendation algorithms have been connecting and promoting accounts that facilitate and sell child sexual abuse content...Meta's photo-sharing service stands out from other social media platforms and 'appears to have a particularly severe problem' with accounts showing self-generated child sexual abuse material.""","Instagram accounts, Instagram posts","Arts, entertainment and recreation",no,Autonomy1,"9.5 - ""Due to the widespread use of hashtags, relatively long life of seller accounts and, especially, the effective recommendation algorithm, Instagram serves as the key discovery mechanism for this specific community of buyers and sellers... researchers discovered Instagram's recommendation algorithms also promoted them 'to users viewing an account in the network, allowing for account discovery without keyword searches.'""",No. Not intentionally designed to perform harm,,content recommendation,,
CSETv1,205,True,205,,4. Peer review complete,001,False,no,maybe,no,yes,no,no,no,no,no,"no tangible harm, near-miss, or issue",no,yes,False,none,,no,no,no,yes,yes,"nation of origin, citizenship, immigrant status, geography",,yes,no,yes,True,no, 5.5 - no AI was involved. Hackers gained access to Facebook and Instagram accounts through phishing. ,2022,02,  ,False,no,no,,,,Europe,,,6.10 - Russia and Ukraine,"Facebook, Instagram, Meta, Ghostwriter, Hacked Facebook and Instagram users, Facebook, Instagram, Twitter, Youtube, Telegram, Odnoklassniki, and VK users",0,0,False,,"Hacking group Ghostwriter used Facebook to target public figures in Ukraine. Hackers successfully gained access to targets' social media accounts and attempted to post YouTube videos from them portraying Ukrainian troops as weakened. In a separate influence campaign, individuals used a number of fictitious personas to run websites masquerading as independent news outlets to publish claims about the West betraying Ukraine and Ukraine being a failed state.","social media accounts, Facebook accounts, Instagram accounts","Arts, entertainment and recreation, information and communication",no,Autonomy3,,unclear,,,,"no AI - AI was only used by the hackers to generate fake profile pictures, which is not the main harm in this incident"
CSETv1,603,True,603,,4. Peer review complete,001,False,maybe,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,,yes,yes,no,no,yes,"disability, financial means, age",4.2 - The algorithm that cut Seiler's care in 2008 was declared unconstitutional by the court in 2016.,yes,yes,yes,True,yes,,2008, , ,True,no,no,,,US,North America,,,"6.1 - Idaho created an algorithm to apportion home care assistance for people with disabilities as early as 2008. These algorithms continued to be used in different states after 2008. These include Pennsylvania, Iowa, New York, Maryland, New Jersey, Arkansas, Missouri, Washington, D.C., and more. ","Larkin Seiler, People receiving state-determined home care, People receiving state-determined home care, Tammy Dobbs, State governments, Home-care allocation algorithms, Brant Fries, Larkin Seiler",0,0,True,"It is unclear how many injuries, hospitalizations, or deaths were caused because of the hour cuts mandated by the algorithm.","The algorithm included a computerized assessment with 286 questions covering everything from mental health to how much help patients need with daily activities like eating or doing their personal finances. Then, an algorithmic tool sorted patients into various levels of need. Each level was assigned a standard numbers of hours of care.","286 questions, health measures, patient information",human health and social work activities,yes,Autonomy3,,No. Not intentionally designed to perform harm,,"welfare determination, benefits allocation",,
CSETv1,355,True,355,,4. Peer review complete,001,False,no,no,no,yes,no,no,no,no,yes,tangible harm definitively occurred,yes,yes,True,AI tangible harm event,"3.1 - Drivers were falsely accused of ""fraudulent activity"" by Uber's automated systems and were subsequently let go without appeal. This would very likely cause tangible harm to the drivers' financial statuses and ability to earn money. Uber also used its algorithms to allocate jobs from the available pool, impacting drivers' earnings potential.",no,yes,no,no,no,none,"4.2 - An appeals court in the Netherlands judged that ride-hailing giants Uber and Ola violated driver rights in a number of instances, including when algorithms were involved in terminating driver accounts. The companies violated the European Union's General Data Protection Regulation which provides individuals with rights to data held on them and information about algorithmic decision making applied to them.",yes,yes,yes,True,yes,,2020,07,,False,no,no, ,,,Europe,,,6.1 - The legal challenges were originally lodged in July and September of 2020 in the United Kingdom.,"Ola, Uber, Ola and Uber drivers, Ola and Uber drivers, App Drivers and Couriers Union, International Alliance of App-based Transport, Worker Info Exchange, Uber algorithmic management systems, Amsterdam courts",0,0,False,,"Uber uses algorithms to detect fraud, such as strategically logging out to await higher surge pricing or declining work offered, in driver accounts. These algorithms have the ability to dismiss drivers without the right of appeal. Uber also uses algorithms to determine the earnings potential of a driver by assigning or withholding jobs from the available pool.",Uber driver account information,transportation and storage,no,Autonomy1,9.5 - There was no evidence of humans in the loop.,No. Not intentionally designed to perform harm,,worker management,,
